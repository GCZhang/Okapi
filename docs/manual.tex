\documentclass[10pt]{article}
\usepackage[letterpaper]{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage{setspace}
\usepackage{ragged2e}
\usepackage{algorithm2e,etoolbox}
\AtBeginEnvironment{algorithm}{\let\textnormal\ttfamily\DontPrintSemicolon}
\usepackage{color, colortbl}
\usepackage{titlesec}
\usepackage{graphicx}
\usepackage{float}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage[font=small,labelfont=bf,labelsep=period]{caption}
\usepackage[english]{babel}
\usepackage{indentfirst}
\usepackage{array}
\usepackage{makecell}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{arydshln}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xfrac}
\usepackage{etoolbox}
\usepackage{cite}
\usepackage{url}
\usepackage{dcolumn}
\usepackage{hyperref}
\usepackage{courier}
\usepackage{url}
\usepackage{esvect}
\usepackage{commath}
\usepackage{verbatim} % for block comments
\usepackage{enumitem}
\usepackage{hyperref} % for clickable table of contents
\usepackage{braket}
\usepackage{titlesec}
\usepackage{booktabs}
\usepackage{gensymb}
\usepackage{longtable}
\usepackage{glossaries}
\usepackage{tcolorbox} % for colored boxes
\tcbuselibrary{breakable} % to allow colored boxed to extend over multiple pages
\usepackage[mathscr]{euscript} % for script letters
\usepackage{wasysym}  % for checkboxes
\usepackage{bm} % for bolding math symbols
\usepackage{listings} % for including code in boxes
\setcounter{MaxMatrixCols}{20} % sets max matrix length as 20 columns

\lstdefinestyle{mystyle}{ 
    commentstyle=\color{gray},
    keywordstyle=\color{magenta},
    stringstyle=\color{orange},
    basicstyle=\footnotesize,
    language=C++,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,          
    basicstyle=\small\ttfamily,        
    tabsize=2
}

\lstset{style=mystyle}

% change spacing around sections
\titlespacing*{\section}
{0pt}{10ex}{1ex}
\titlespacing*{\subsection}
{0pt}{7ex}{1ex}

% new commands for shorter writing of equations
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\beqa}{\begin{equation}\begin{aligned}}
\newcommand{\eeqa}{\end{aligned}\end{equation}}

% Acronyms used
\setacronymstyle{long-short}
\newacronym{anl}{ANL}{Argonne National Laboratory}
\newacronym{cfd}{CFD}{Computational Fluid Dynamics}
\newacronym{fe}{FE}{Finite Element}
\newacronym{fem}{FEM}{Finite Element Method}
\newacronym{inl}{INL}{Idaho National Laboratory}
\newacronym{io}{I/O}{Input/Output}
\newacronym{moon}{MOON}{MOOSE and Nek}
\newacronym{moose}{MOOSE}{the Multiphysics Object-Oriented Simulation Environment}

\titleclass{\subsubsubsection}{straight}[\subsection]

% define new command for triple sub sections
\newcounter{subsubsubsection}[subsubsection]
\renewcommand\thesubsubsubsection{\thesubsubsection.\arabic{subsubsubsection}}
\renewcommand\theparagraph{\thesubsubsubsection.\arabic{paragraph}} % optional; useful if paragraphs are to be numbered

\titleformat{\subsubsubsection}
  {\normalfont\normalsize\bfseries}{\thesubsubsubsection}{1em}{}
\titlespacing*{\subsubsubsection}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{5}{\z@}%
  {3.25ex \@plus1ex \@minus.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries}}
\renewcommand\subparagraph{\@startsection{subparagraph}{6}{\parindent}%
  {3.25ex \@plus1ex \@minus .2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries}}
\def\toclevel@subsubsubsection{4}
\def\toclevel@paragraph{5}
\def\toclevel@paragraph{6}
\def\l@subsubsubsection{\@dottedtocline{4}{7em}{4em}}
\def\l@paragraph{\@dottedtocline{5}{10em}{5em}}
\def\l@subparagraph{\@dottedtocline{6}{14em}{6em}}
\makeatother

\newcommand{\volume}{\mathop{\ooalign{\hfil$V$\hfil\cr\kern0.08em--\hfil\cr}}\nolimits}

\numberwithin{equation}{section} % for equation numbering

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}
\makeglossaries
\begin{document}

\title{OpenMC, BISON, and Nek Coupling}
\author{Argonne National Laboratory}
\maketitle

\tableofcontents
%\printglossary[type=\acronymtype,title=Abbreviations]

\clearpage

\section{Introduction}

This document is intended to describe the coupling methodology between Nek, a \gls{cfd} code developed by \gls{anl}, OpenMC, a Monte Carlo code developed by \gls{anl}, and BISON, a nuclear fuel performance code developed by \gls{inl}. Previous work by Matthew Ellis resulted in the coupling of OpenMC to MOOSE, and work by a joint effort of \gls{anl} and \gls{inl} resulted in the coupling of Nek to MOOSE. So, the coupling of all three codes together will utilize as much as possible regarding this previous work. The purpose of this report is to document the coupling process, justify decisions made, and illustrate how to use these coupled codes.

This report is organized beginning with a description of the overall coupling methodology, followed by a discussion of the nuts-and-bolts requirements for coupling two codes in terms of the data transfer between the codes and executing non-MOOSE codes from within the MOOSE framework.

\section{Data Transfer}

This section discusses the requirements needed to transfer information between two physics codes. When coupling two codes, there are two general ways in which the data can be transferred:

\begin{enumerate}
\item Force both codes to solve on the same mesh, in which case the data transfer can be accomplished by determining a functional form for the transfer variable (such as temperature) over a single computational element, and then passing this information to the second code. Assuming that the second code can utilize this information exactly, this is the easiest way to transfer information, because it requires little effort to map cells in one mesh to cells in another. However, this method is very restrictive, since it is often the case that the two physics do not require the same mesh, and forcing both physics to run on the finest mesh is unnecessarily expensive. For instance, boundary layers must be resolved for fluids computations with very fine elements that would be overkill for a Monte Carlo simulation.
\item Run both codes on different meshes.

	\begin{enumerate}
	\item Use arbitrary polynomials to express a transfer variable, and then pass the expansion coefficients to the second code. For instance, suppose Nek is to transfer temperature on a boundary to MOOSE, which would use this information to set a boundary condition. Because this information is not being passed on an element-to-element basis, its spatial distribution must be described using some type of function - delta functions if an elementwise-constant value from Nek is to be transferred to an element (not necessarily of the same size) in MOOSE, or really any arbitrary polynomial. To transfer a variable \(u(r, \theta, z)\) to MOOSE, Nek would compute the expansion coefficients \(C_{jkl}\) that capture the behavior according to expansion functions \(R(r), \Theta(\theta), Z(z)\):
	
	\beq
	\label{eq:GenericExpansion}
	u(r, \theta, z)\approx\sum_{j=0}^J\sum_{k=0}^K\sum_{l=0}^L C_{jkl}R_j(r)\Theta_k(\theta)Z_l(z)
	\eeq
	
Note that an approximate equals sign \(\approx\) is used, since for an arbitrary selection of functions, the (potentially) infinite degrees of freedom of the value of \(u\) computed by Nek cannot be fully captured. Once Nek passes the coefficients \(C_{jkl}\) to MOOSE, MOOSE would reconstruct the functional form of the transfer variable using the same equation above, and then proceed with its simulation for that step. The issue with using arbitrary polynomials is that there is no one way to determine the expansion coefficients \(C_{jk}\). For instance, assume we use the following expansion functions:

	\beqa
	R_j(r)=&r^j\\
	\Theta_k(\theta)=&\cos(k\theta)\\
	Z_l(z)=&1+z^l\\
	\eeqa

To determine the expansion coefficients for this choice requires that we construct a Vandermonde matrix, which requires that we arbitrarily select points over the Nek element at which to sample the Nek transfer variable. For \(J=1\), \(K=0\), \(L=0\), this matrix system would take the form:

\beqa
u(r,\theta,z)=\begin{bmatrix}
r_0^0\cos{(0)}(1+z_0^0) & r_0^1\cos{(\theta_0)}(1+z_0^0)\\
r_1^0\cos{(0)}(1+z_0^0) & r_1^1\cos{(\theta_0)}(1+z_0^0)\\
\end{bmatrix}
\begin{bmatrix}
C_{000}\\C_{100}
\end{bmatrix}
\eeqa

where \(r_0\) and \(r_1\) are the two sampled \(r\)-coordinates in the element that must be selected in order to determine the expansion coefficients for a first-order \(r\)-expansion, and \(\theta_0\) and \(z_0\) the sampling points to determine the expansion coefficients for a zeroth-order \(\theta\)- and \(z\)-expansion. This can cause problems such as oscillations in the interpolation if the choices of functions are poor, and can completely miss parts of the transfer variable solution if we happen to not select points near important features.

	\item Use orthogonal functions to express a transfer variable, and then pass the expansion coefficients to the second code. Using orthogonal functions is essentially the same formulation as that shown in Eq. \eqref{eq:GenericExpansion}, except that the expansion functions are chosen to be orthogonal over the domain of transfer. Then, orthogonality conditions lead to exact formulas for \(C_{jkl}\) that are {\it integral} in nature, which means that all features of the solution are captured, and no creation and inversion of a Vandermonde matrix is required. 
	\end{enumerate}
	
\end{enumerate}

For the reasons discussed above, the data transfer between two physics codes can be performed in a fairly simple manner when using orthogonal polynomial expansions. This is the approach that is taken in the coupling of Nek, OpenMC, and BISON, and therefore warrants more discussion. For any code to implement data transfers using polynomials, four capabilities are required:

\begin{enumerate}
\item Functions to compute the orthogonal polynomials given a coordinate point and expansion order.
\item Routines to integrate a continuous variable over an orthogonal domain to compute the expansion coefficients. This process will be referred to in this document as ``deconstruction,'' since a continuous field is decomposed into a finite set of expansion coefficients that are to be passed to another physics code.
\item Routines to multiply a set of expansion coefficients by orthogonal polynomials to form a continuous field form a set of finite coefficients. This process will be referred to in this document as ``construction,'' since a continuous field is formed by multiplying discrete expansion coefficients by continuous functions.
\item Use the constructed field in some internal capacity that defines the coupling, such as in the specification of a boundary condition or heat source.
\end{enumerate}

For the three physics codes to be discussed, each of these four capabilities will be described in detail.

\subsection{Choices of Orthogonal Polynomials}
\label{sec:Polynomials}
This section describes several choices of orthogonal polynomials that can be used to construct and deconstruct continuous fields.

\subsubsection{Legendre Polynomials}

Legendre polynomials are orthogonal over \( -1 \leq z \leq 1\), and hence are suitable for expanding solutions along the axes of fuel pins, along one direction in a slab, or in any Cartesian dimension. The Legendre polynomials are given as:

\beq
\label{eq:Legendre}
P_l(z)=\frac{1}{2^ll!}\frac{d^l}{dx^l}\left\lbrack\left(z^2-1)^l\right)\right\rbrack
\eeq

which are orthogonal over \(-1\) to \(+1\):

\beq
\int_{-1}^{+1}P_l(z)P_{l'}(z)dz=\frac{2}{2l+1}\delta_{ll'}
\eeq

When calculating expansion coefficients, it will be useful to work with scaled Legendre polynomials such that the orthogonality condition gives {\it only} a delta function. These scaled Legendre polynomials are indicated with a tilde:

\beq
\label{eq:LegendreScaled}
\tilde{P}_l(z)=\sqrt{\frac{2l+1}{2}}P_l(z)
\eeq

such that the orthogonality condition becomes:

\beq
\label{eq:LegendreScaledOrthogonality}
\int_{-1}^{+1}\tilde{P}_l(z)\tilde{P}_{l'}(z)dz=\delta_{ll'}
\eeq

\subsubsection{Fourier Polynomials}

Fourier polynomials are orthogonal over \(-\pi\leq\theta\leq\pi\), and hence are suitable for expanding along the \(\theta\) direction of a cylindrical fuel pin. The Fourier functions are given as:

\beq
\label{eq:Fourier}
F_k(\theta)=\cos(k\theta)
\eeq

which are orthogonal over \(-\pi\) to \(+\pi\):

\beqa
\label{eq:FourierOrthogonal}
\begin{cases}
\int_{-\pi}^{+\pi}F_k(\theta)F_{k'}(\theta)d\theta=2\pi\delta_{kk'}& k=k'=0\\
\int_{-\pi}^{+\pi}F_k(\theta)F_{k'}(\theta)d\theta=\pi\delta_{kk'}& \textrm{else}\\
\end{cases}
\eeqa

and hence for \(n=0\), the return value is scaled by \(1/\sqrt{2\pi}\) instead of \(1/\sqrt{\pi}\). 

Similar to Legendre polynomials, it will be more convenient when calculating expansion coefficients for the Fourier polynomials to be scaled such that the orthogonality condition gives {\it only} a delta function. These scaled Fourier polynomials are indicated with a tilde:

\beq
\label{eq:FourierScaled}
\tilde{F}_k(\theta)=\frac{1}{\sqrt{\pi}}\cos{(k\theta)}
\eeq

such that the orthogonality condition becomes:

\beq
\int_{-\pi}^{+\pi}\tilde{F}_k(\theta)\tilde{F}_{k'}(\theta)d\theta=\delta_{kk'}
\eeq

\subsubsection{Zernike Polynomials}

Zernike polynomials are orthogonal over the unit circle, and hence are suitable for expanding in the plane of a fuel pin. The Zernike polynomials are given as:

\beq
\label{eq:ZernikeScaled}
Z_n^m(r,\theta)=
\begin{cases}
\sqrt{\frac{2(n+1)}{\pi\left(1+\delta_{m0}\right)}}R_n^{|m|}(r)\cos{(m\theta)} & m\geq 0\\
-\sqrt{\frac{2(n+1)}{\pi\left(1+\delta_{m0}\right)}}R_n^{|m|}(r)\sin{(m\theta)} & m < 0\\
\end{cases}
\eeq

where \(R_n^{|m|}\) is given as:

\beq
R_n^{|m|}(r)=\sum_{s=0}^{\frac{n-|m|}{2}}\frac{(-1)^s(n-s)!}{s!\left(\frac{n+|m|}{2}-s\right)!\left(\frac{n-|m|}{2}-s\right)!}r^{n-2s}
\eeq

Note that \(m\) can only obtain values \(-n, -n+2, -n+4, \cdots, n\). For this definition of the Zernike polynomials, which include the normalization factor that was similarly introduced for Fourier and Legendre polynomials, the orthogonality condition over the unit circle reads:

\beq
\label{eq:ZernikeOrthogonal}
\int_{0}^{1}rdr\int_{0}^{2\pi}d\theta Z_{n}^m(r,\theta)Z_{n'}^{m'}=\delta_{n,n'}\delta_{m,m'}
\eeq

\subsection{Deconstructing a Continuous Variable}
This section describes how a continuous field \(u(r,\theta,z)\) can be deconstructed into a set of expansion coefficients using the orthogonal polynomials discussed in the previous section. The procedure is the same regardless of the orthogonal domain (i.e. if performing a boundary or volume coupling), but will be illustrated for both a boundary coupling over the surface of a cylinder and for a volume coupling over a cylindrical domain. 

For a boundary coupling over the surface of a cylinder, a function \(u(\theta,z)\) can be expressed as:

\beq
\label{eq:OrthogonalExpansion}
u(\theta, z)\approx\sum_{k=0}^K\sum_{l=0}^LC_{kl}\tilde{F}_k(\theta)\tilde{P}_l(z)
\eeq

where \(\tilde{F}_k(\theta)\) is the \(k\)-th order Fourier function and \(\tilde{P}_l(z)\) is the \(l\)-th order Legendre polynomial. The expansion coefficients for \(u\) can be determined by multiplying both sides of Eq. \eqref{eq:OrthogonalExpansion} by orthogonal polynomials of different order, and applying the orthogonality conditions given in Section \ref{sec:Polynomials}:

\beq
\label{eq:ScaledExpansionCoeff}
C_{kl}=\int_{-1}^{+1}dz\int_{-\pi}^{+\pi}d\theta u(\theta, z)\tilde{F}_k(\theta)\tilde{P}_l(z)
\eeq

Likewise, for a volume coupling over a cylinder, a function \(u(r,\theta,z)\) can be expressed as:

\beq
\label{eq:ZLReconstruction}
u(r,\theta,z)=\sum_{l=0}^{L}\sum_{n=0}^N\sum_{m=-n}^{n}C_{l}^{nm}\tilde{P}_l(z)\tilde{Z}_n^m(r,\theta)
\eeq

Again, the coefficients can be determined by multiplying by orthogonal polynomials of different order, and applying orthogonality conditions:

\beq
C_l^{nm}=\int_0^1rdr\int_{0}^{2\pi}d\theta\int_{-1}^{1}dzu(r,\theta,z)P_l(z)Z_n^m(r,\theta)
\eeq

Hence, all that is required to deconstruct a continuous field into a set of expansion coefficients is to perform multiple integrations over the orthogonal domain.

\subsection{Constructing a Continuous Variable}

As opposed to deconstruction, constructing a continuous variable from a set of expansion coefficients is more straightforward, since all that is required is repeated evaluation of the orthogonal polynomials, multiplied by expansion coefficients.

\subsection{MOOSE Data Transfer}
This section describes the capabilities introduced into the MOOSE source code that allows evaluation of polynomials, and construction/deconstruction of continuous fields. 

\subsubsection{Evaluation of Polynomials}
This section discusses the generic functions added to MOOSE to compute orthogonal polynomials.

\subsubsubsection{{\tt LegendrePolynomial}}
This function computes the Legendre polynomial in Eq. \eqref{eq:LegendreScaled} for a given value of \(l\) and a specific position (a single value of \(z\)). This function is not particular to the direction in which the Legendre expansion is being made, since only a single number (not a point) is passed in. Because Legendre polynomials are only orthogonal on \([-1, +1]\), but the length of the domain that is being expressed using Legendre polynomials may not necessarily be 2, a scaling factor \(z\) is used to simply stretch or compress the physical domain to the orthogonal domain. The {\tt l\_geom\_norm} required parameter lists the minimum and maximum distances over which the Legendre polynomials will be forced to be orthogonal using a scaling factor. This function exists in both MOON and Okapi, but is cleaner in Okapi.

\subsubsubsection{{\tt FourierPolynomial}}
This function computes the Fourier function given in Eq. \eqref{eq:FourierScaled} for a given value of \(k\) and a specific position. The \(\theta\) value at which to evaluate this function is determined as the angle between the positive \(x\)-axis and a point \((x,y)\). This function is not specific to the direction in which the Legendre expansion is being made, i.e. two points are passed in, where the first is assumed to be the distance along the axis relative to which the angle \(\theta\) is determined. Because the fuel rod of interest for coupling exists on \([-\pi, +\pi]\), no additional scaling is needed like for the Legendre polynomials. This function exists in MOON.

\subsubsubsection{{\tt ZernikePolynomial}}
This function computes a Zernike polynomial given in Eq. \eqref{eq:ZernikeScaled} given orders \(n\) and \(m\). 

\subsubsection{Field Deconstruction}
This section describes how a continuous field is deconstructed into a set of expansion coefficients. 

\subsubsubsection{{\tt NekSideIntegralVariableUserObject}}
This user object determines an expansion coefficient given a field \(u(\theta, z)\) and the orders of the Fourier and Legendre expansions according to Eq. \eqref{eq:ScaledExpansionCoeff}. This user object only computes {\it one} expansion coefficient at a time, and hence for the example shown in the previous section, 50 of these user objects appear in a single input file. Note that this object does not actually compute the integral of \(u(\theta, z)\) - rather, it computes the integral of the heat flux, since this is the variable of interest that is to be expanded in polynomials for transfer to a boundary condition in Nek. This user object is available in MOON.

\begin{lstlisting}
  [./nek_f_4_l_9]
    type = NekSideIntegralVariableUserObject
    variable = temp
    boundary = wall
    legendre_function_name = 'legendre_function'
    fourier_function_name = 'fourier_function'
    l_direction = 2
    l_order = 9
    f_order = 4
    aux_scalar_name = heat_flux_scalar_f_4_l
    diffusion_coefficient_name = 'thermal_conductivity'
    surface_area_pp = 'surf_area'
  [../]
\end{lstlisting}

\subsubsubsection{{\tt ZernikeLegendreDeconstruction}}
This user object converts a continuous variable into the set of expansion coefficients that would be obtained by expanding the variable in Zernike and Legendre polynomials as shown in Eq. \eqref{eq:ZernikeLegendre}. Note that this user object only computes {\it one} \(C_l^{nm}\) at a time. The {\tt ZLDeconstruction} user object should be used instead, since it computes all of the Zernike coefficients given a single Legendre order. Computing each coefficient \(C_{l}^{nm}\) requires applying orthogonality as shown in Eqs. \eqref{eq:ZernikeOrthogonal} and \eqref{eq:LegendreScaledOrthogonality}. Applying both orthogonality conditions to the expansion gives:

\beq
C_l^{nm}=\int_0^1rdr\int_{0}^{2\pi}d\theta\int_{-1}^{1}dzu(r,\theta,z)P_l(z)Z_n^m(r,\theta)
\eeq

In order to determine the correct expansion coefficients, the above integral {\it must} be performed over the integral bounds given, i.e. those over which the functions are orthogonal. However, in the physical domain, the radius of the pin may not be unity, and the \(z\)-extent of the pin may not be from \([-1, 1]\) (though it is assumed that the \(\theta\) coordinate does exist over \([0,2\pi]\)). The integral will be computed in the physical domain. To correctly scale to the orthogonal domain, the integral should be multiplied as follows:

\beq
C_l^{nm}=\frac{1}{R^2}\frac{2}{H}\int_0^1rdr\int_{0}^{2\pi}d\theta\int_{-1}^{1}dzu(r,\theta,z)P_l(z)Z_n^m(r,\theta)
\eeq

where \(R\) is the radius and \(H\) the height in the physical domain. The factor of 2 appears because the height in the orthogonal domain is 2. To prevent the user from needing to enter a radius and height manually, these terms can be found using a postprocessor in MOOSE. By multiplying the factor above by \(\pi/\pi\), the denominator becomes the volume of the region. Hence, a postprocessor value provides the volume.

This user object inherits from the {\tt ElementIntegralUserObject}, which inherits from the {\tt ElementUserObject} class. This parent class inherits all of the parameters of the {\tt BlockRestrictable} interface. Hence, this object can be restricted to blocks using the {\tt blocks} parameter in the XML input file. If the block is not specified, then if a variable parameter is present, the blocks are set to all of the blocks that are associated with that variable (i.e. for most simulations, the entire domain). If no variable is specified, then by default the entire domain is used. 

For a given auxiliary SCALAR variable, the coefficients apply for a fixed Legendre order, and increase in the array beginning with the \(n\) index and then the \(m\) index of the Zernike expansion. The most rigorous way to verify that this deconstruction user object has been implemented correctly is to assume some set of coefficients and reconstruct a polynomial using the {\tt ZernikeLegendreReconstruction} function. Then, deconstruct this function using the {\tt ZernikeLegendreDeconstruction}, and verify that the coefficients have not changed. This must be performed over a cylindrical region to ensure that the orthogonality conditions are true. Note that the coefficients will not perfectly match those specified for the function due to errors associated with the quadrature rule used for numeric integration. This user object is available in Okapi.

\subsubsubsection{{\tt ZLDeconstruction}}
This user object computes {\it all} of the Zernike coefficients given a fixed Legendre order, and hence reduces the number of user objects required in the input file by a factor of the number of Zernike expansion coefficients. The contents of this user object are essentially identical to that of the {\tt ZernikeLegendreDeconstruction} user object, except that loops are used to update an array of values {\tt \_integral\_value}, instead of a singular value.

\subsubsection{Field Reconstruction}
This section discusses the functions developed in MOOSE to reconstruct a continuous field given expansion coefficients and specified orthogonal functions.

\subsubsubsection{{\tt FourierLegendreReconstruction}}
This function reconstructs a solution as a finite series of Fourier and Legendre polynomials given expansion coefficients, and evaluates it at a single point according to Eq. \eqref{eq:OrthogonalExpansion}. While {\tt LegendrePolynomial} and {\tt FourierPolynomial} are {\it not} specific to the orientation of the Legendre expansion relative to any of the Cartesian axes, this reconstruction assumes that given a point with three coordinates, that the third coordinate (the \(z\)-coordinate) serves as input to the {\tt LegendrePolynomial} function, while the \(x\) and \(y\) coordinates are input to the {\tt FourierPolynomial} function. This function is available in MOON.

\begin{comment}
To illustrate how solution reconstruction occurs in MOOSE, consider the following section of a MOON input file. Note that the Legendre and Fourier polynomials are simply instantiated to allow their member {\tt getPolynomialValue} to compute a scalar value given a point (or two for the case of the Fourier polynomial) and an order. The input specifications for {\tt FourierLegendreReconstruction} show that the surface temperature that will be passed from Nek to MOOSE in the form of expansion coefficients will have been expanded in a Legendre order of 10 and a Fourier order of 5. The input to the reconstruction function is a total of {\tt l\_order}\(\times\){\tt f\_order} coefficients. These coefficients are stored in {\tt SCALAR} auxiliary variables. A scalar auxvariable can hold any number of values, and this is implemented as an array of length {\tt ORDER}. For instance, the {\tt temp\_bc\_scalar\_f\_0\_l} auxvariable is declared as {\tt family=SCALAR} and {\tt order=TENTH}, so each of these auxvariables contains 10 scalar values. Each of these variables hence holds the \(C_{0l}, C_{1l}, C_{2l}, C_{3l}, C_{4l}\) values. This helps reduce the total number of variables that must be declared in this input file. For a total of 50 coefficients that are passed back and forth between Nek and MOOSE, this could have been implemented using 50 unique auxvariables, at the expense of a much larger input file.

\begin{lstlisting}
[Functions]
  [./legendre_function]
    type = LegendrePolynomial
    # z-domain exists from 0.0 to 1.0
    l_geom_norm = '0.0 1.0'
  [../]
  [./fourier_function]
    type = FourierPolynomial
  [../]
  [./fl_reconstruction]
    type = FourierLegendreReconstruction
    l_order = 10
    f_order = 5
    # Legendre expansion is for the z-coordinate (not x or y)
    l_direction = 2 
    legendre_function_name = 'legendre_function'
    fourier_function_name = 'fourier_function'
    poly_scalars = 'temp_bc_scalar_f_0_l temp_bc_scalar_f_1_l temp_bc_scalar_f_2_l temp_bc_scalar_f_3_l temp_bc_scalar_f_4_l'
  [../]
[]
\end{lstlisting}

The scalar value returned by the {\tt FourierLegendreReconstruction} function is used in the function Dirichlet boundary condition within MOOSE. 

\begin{lstlisting}
[BCs]
  [./wall]
    type = FunctionDirichletBC
    variable = temp
    boundary = 'wall'
    function = fl_reconstruction
  [../]
[]
\end{lstlisting}

So, this discussion on the functions in MOOSE concludes how MOOSE applies the temperature Dirichlet condition to the MOOSE solve within the fuel given the expansion coefficients from Nek. 
\end{comment}

\subsubsubsection{{\tt ZernikeLegendreReconstruction}}
This function reconstructs a solution as a finite series of Zernike and Legendre polynomials given expansion coefficients, and evaluates it at a single point according to Eq. \eqref{eq:ZLReconstruction}. This function is very similar to the Fourier-Legendre reconstruction function developed for MOON, except that it also includes the flexibility for the Legendre expansion direction to not be along the \(z\)-direction. This function is available in Okapi.

This function is set up so that the {\tt poly\_coeffs} coupled to this function {\it each} hold all of the expansion coefficients for a {\it fixed} Legendre order \(l\). For instance, for a Legendre expansion of 3 and a Zernike expansion of 2, these coefficients hold:

\begin{lstlisting}
l_0_coeffs = 'C000 C01n1 C011 C02n2 C020 C022'
l_1_coeffs = 'C100 C11n1 C111 C12n2 C120 C122'
l_2_coeffs = 'C200 C21n1 C211 C22n2 C2020 C222'
l_3_coeffs = 'C300 C31n1 C311 C32n2 C320 C322'
\end{lstlisting}

where the first index represents the \(l\) order, the second the \(n\) order, and the third the \(m\) order, where {\tt n} represents a negative number. Note that the choice in MOON was for each of these {\tt poly\_scalars} aux variables to hold all of the Legendre coefficients for a given Fourier order, which is the opposite of the choice here. 

\subsubsection{Using the Constructed Field}

\subsubsection{Performing the Data Transfer}
MOOSE controls all data transfers between the coupled external codes. This section describes the MOOSE {\tt Transfer} objects that were created to perform data transfer to/from OpenMC and to/from Nek.

\subsubsubsection{{\tt PolynomialOpenMC}}
This transfer object defines what routines are called to transfer data from MOOSE to OpenMC, and from OpenMC back to MOOSE.

\subsubsubsection{{\tt MultiAppPolynomialToNek}}
This transfer object defines what routines are called to transfer data from MOOSE to Nek, and from Nek back to MOOSE.

\subsection{OpenMC Data Transfer}
This section describes the capabilities introduced into the OpenMC source code that allows evaluation of polynomials, and construction/deconstruction of continuous fields. 

\subsubsection{Evaluation of Polynomials}

\subsubsection{Field Deconstruction}
OpenMC deconstructs a statistical distribution of fission events into expansion coefficients for the fission source. Tallies are used to compute these expansion coefficients, and hence an understanding of the tally system in OpenMC is required. The following provides the sequence of events followed in reading in tally information from the tallies XML input file.

\begin{enumerate}
\item Read the tallies XML file by calling the {\tt read \_tallies\_xml()} subroutine. The tallies XML file is optional. 
	\begin{enumerate}
	\item Allocate memory for the global {\tt tally\_derivs} array of type {\tt TallyDerivative}. This array is thread private. 
	\item The global variable {\tt n\_meshes} is set equal to the number of meshes the user specifies in the tallies XML file. Then, allocate the global {\tt meshes} array of type {\tt RegularMesh} to hold this mesh information. 
	\item The global variable {\tt n\_user\_filters} is read from the number of filters listed in the input file. The filters are allocated by calling {\tt add\_filters(n\_user\_filters)}, which allocates the global {\tt filters} array of type {\tt TallyFilterContainer} if it does not exist, or else extend the filters array. 
	\item The tallies specified in the tallies XML file are user tallies, and are counted as the global variable {\tt n\_user\_tallies}. 
	\item Allocate the global {\tt tallies} array by calling the {\tt add\_tallies("user", n\_user\_tallies)} subroutine. If the global variable {\tt n\_tallies} is zero, then allocate {\tt tallies(n)}, otherwise extend the {\tt tallies} array with the additional tallies. There are two types of "groups" that can be passed in - "user" and "cmfd". The index for the tally group is set as {\tt i\_user\_tallies = n\_tallies} (this index is the starting index, minus 1, in tallies for each tally group). Finally, because we moved indices in the global tallies array, we need to reassign pointers for each group. {\tt user\_tallies}, a point to tallies of type {\tt TallyObject}, is reset accordingly.
	\item Loop through the number of user meshes and read in XML data to the {\tt meshes(i)} structure. 
	\item Read data for derivatives. 
	\item Read data for filters. Loop through the {\tt n\_user\_filters}. A filter must be associated with an ID. The filter type is read into the temporary string {\tt temp\_str}, and then in a case select statement, we determine the number of bins. The valid types of filters are then searched over in a select-case statement. For each filter type, we allocate and declare the filter type (?) (for each type of filter, there is a corresponding type similar to {\tt CellFilter} for cell-type filters). For filters of compatible types, the {\tt filters(i) \% obj \% n\_bins} is set equal to the number of bins that appear in the XML file. Finally, the filter is added to the filter dictionary.
	\item Read data for tallies. Loop through the {\tt n\_user\_tallies}. The tally type {\tt tallies(i) \% type = TALLY\_VOLUME} is set by default. It's also better to use track length estimators for tallies, since more events will contribute (though we can't use this for tallies that require post-collision information). For now, set {\tt tallies(i) \% estimator = ESTIMATOR\_TRACKLENGTH}. 
		\begin{enumerate}
		\item Then, read data for filters for the tally - the number of filters for the tally is read into the local variable {\tt n\_filters}. Read any filters that are present, and store them in the temporary, local {\tt temp\_filter} array. For this tally, then loop through all of the filters for the tally. Set the filter index in the tally {\tt find\_filter} array (an array defined in the {\tt tally\_header} file that is initialized to zero, with a length equal to the constant {\tt N\_FILTER\_TYPES}. So, for each tally in {\tt tallies}, the {\tt find\_filter} array is all zeros except for any filters (type expressed as an integer) that are used for the tally (and then the number actually stored in each instance of {\tt find\_filter} corresponds to the filter index). It is here that the particular tallies that cannot use a tracklength estimator are set to {\tt ESTIMATOR\_ANALOG}. 
		\item Read data for nuclides that you'd like to tally for the current tally. You can specify all nuclides, in which case {\tt tallies \% nuclide\_bins} is allocated with a size of the total number of nuclides in the simulation, {\tt n\_nuclides\_total}, plus 1, and is populated with all of the integers corresponding to the nuclides. For this special case, {\tt tallies(i) \% all\_nuclides} is set to true. For other cases, where only a subset of the nuclides are specified, then the {\tt tallies(i) \% nuclide\_bins(n\_words)} is allocated with the appropriate length. Looping over the number of nuclides specified, then set the {\tt nuclide\_bins(j)} to the index of the nuclide in the nuclides array. If no nuclides are specified, then the {\tt total} material is used, and only one nuclide bin is added to {\tt tallies(i)} (note that specifying no nuclides is different from specifying all nuclides!). 
		\item Read data for scores. The number of scores listed is used to allocate the local array {\tt sarray(n\_words)} of type character to simply hold the names of the scores. Before allocating storage for the scores, we first need to determine the number of additional scores that would be required due to moment scores. This is only required if the score is of the form moment-p or moment-y. Now, allocate the global variable {\tt tallies(i) \% score\_bins(n\_scores)}, with the correct number of scores (for our case, this will just be the number of scores listed in the input file). Also allocate {\tt tallies(i) \% moment\_order(n\_scores)}. Then, loop over the number of scores. Additional checks are made here to ensure compatibility between the types of filters you specified above, and the types of scores. Then, enter a select-case statement that loops over all of the possible score types. For each type of score, set {\tt tallies(i) \% score\_bins(j) = SCORE\_FLUX}, or the appropriate type of scoring. Depending on the type of tally, other members of {\tt tallies(i)} such as {\tt moment\_order} may be set. Finally, at the end of the very large select-case statement for possible scores, we check to make sure that the tallies are compatible with MG mode. The number of scores specified in the input file may not necessarily be the actual number of scores that are used, since some tallies expand in spherical harmonics or Legendre polynomials, and return moments. Hence, {\tt tallies(i) \% n\_scores} contains the actual number of scores, while {|tt tallies(i) \% n\_words} contains the number of scores specified in the XML file. Finally, check that no duplicate scores exist by looping through the number of scores.
		\item Check for tally derivatives. 
		\item If {\tt trigger\_on} is true based on the settings XML file, then create tally triggers.
		\item Set tally estimator - checks if the user specified an estimator in the input file (previously it was set according to the type of tally as either was strictly required to be an analog estimator, or by default as a track-length estimator). Collision and track-length estimators cannot be used for tallies that need post-collision information. If you don't need post-collision information, then the type specified by the user in the tallies XML file will supersede any defaults set earlier. 
		\end{enumerate}
	\end{enumerate}
\end{enumerate}

To score a collision tally, the {\tt score\_collision\_tally(p)} routine is called after every collision. It is invalid for tallies that require post-collision information because it can score reactions that didn't actually happen, and without sampling, we don't know all of the outgoing information. 

\begin{enumerate}
\item Loop over all of the {\tt active\_collision\_tallies}. 
	\begin{enumerate}
	\item Find all valid bins for this filter. If there are no valid bins for this filter, then there is nothing to score, and you can move on to the next tally. Otherwise, loop over all of the filters until you've covered all of the valid bins on each of the filters. Loop over all nuclides, and determine if each nuclide is actually in the material. Finally, score for each bin by calling {\tt score\_general\_ce}.
		\begin{enumerate}
		\item The {\tt score\_general\_ce(p, t, start\_index, filter\_index, i\_nuclide, atom\_density, flux)} subroutine adds scores to the tally array for a given filter and nuclide, and it is called by all volume tallies. Begin by looping over all of the score bins ({\tt n\_user\_score\_bins}). Then, enter a select-case statement to determine the type of tally - it is here that the actual ``formulas'' for tallying different scores are held. The end result of this select-case statement is to determine a value for the {\tt score}. 
		\item After determining the correct value of the {\tt score}, expand the score if necessary and add it to the tally results by calling {\tt expand\_and\_score(p, t, score\_index, filter\_index, score\_bin, score, i)} subroutine. This routine takes a previously determined score value and adjusts it if needed, such as for functional expansion weighting, and then adds the resultant value to the tally results array. 
			\begin{enumerate}
			\item Immediately enter a case-select statement that determines the type of score (such as {\tt SCORE\_SCATTER\_N}). This will add to the value of {\tt t \% results} by increasing the value by the previously-calculated {\tt score}. 
			\item For tallies that expand in Legendre polynomials, {\tt calc\_pn} function computes the \(n\)-th order Legendre polynomial given an order and coordinate. 
			\end{enumerate}
		\end{enumerate}
	\end{enumerate}
\end{enumerate}

In order to tally the expansion coefficients for a Zernike-Legendre expansion, a new type of score was created - {\tt SCORE\_FISSION\_ZL}, which is implemented in a manner very similarly to the tallies that expand a reaction rate in Legendre polynomials or spherical harmonics. The following changes were made to add this new type of score:

\begin{itemize}
\item Register a new type of tally, {\tt SCORE\_FISSION\_ZL}. 
\item For the case statement in the {\tt score\_general\_ce}, we need to add the {\tt SCORE\_FISSION\_ZL} score type to the same case select statement for the {\tt SCORE\_FISSION} tally, since they both essentially tally the same thing, except that the new tally will compute multiple expansion coefficients.
\end{itemize}

\subsubsection{Field Reconstruction}

\subsubsection{Using the Constructed Field}
The data transferred to OpenMC are temperatures in the fuel (from BISON - it is assumed that the fuel density is negligible) and densities and temperatures of the fluid coolant (from Nek). In order to understand how to use these reconstructed fields, it is necessary to understand how temperatures and densities are treated in OpenMC.

The expansion coefficients received from MOOSE are stored each Picard iteration in the global {\tt expansion\_coeffs} array by calling the {\tt receive\_coeffs} subroutine from the {\tt coupling} module. It is assumed that the SCALAR variables used as the {\tt source\_variable} in the transfer lists the expansion coefficients, where they have been grouped by all the Zernike coefficients (organized by increasing \(n\), with increasing \(m\) for each \(n\)) in increasing Legendre order. For instance, the following would be the correct syntax in the input file for a transfer to OpenMC of coefficients associated with a Legendre order of 2 and a Zernike order of 1.

\begin{lstlisting}
[Transfers]
  [./to_openmc]
    type = PolynomialOpenMC
    direction = to_multiapp
    source_variable = 'l_0_coeffs l_1_coeffs l_2_coeffs'
    to_aux_scalar = 'foo'
  [../]
[]
\end{lstlisting}

where {\tt l\_0\_coeffs} then holds the following coefficients in the following order:

\beq
\textrm{{\tt l\_0\_coeffs}}= \begin{bmatrix}C_0^{n=0,m=0} & C_0^{n=1, m=-1} & C_0^{n=1,m=1} & C_0^{n=2,m=-2} & C_0^{n=2,m=0} & C_0^{n=2,m=2}\end{bmatrix}
\eeq

It is critical that the order of the source variables in the transfer be in order of increasing Legendre order (the Zernike order is implemented correctly automatically by nature of the {\tt ZLDeconstruction} user object). 

\subsubsubsection{Changing Temperatures}
This section describes how temperatures are set in OpenMC for a normal, non-coupled simulation in order to elicit what changes are needed to modify a temperature in OpenMC and obtain the correct result of changing the temperature at which cross sections are evaluated.

\begin{enumerate}
\item {\bf Read input data from geometry XML file}. In the {\tt read\_geometry\_xml()} subroutine, the global array {\tt cells(n\_cells)} is allocated, where {\tt n\_cells} is the number of cells that are specified in the input file ({\it not} each distributed cell). This is the subroutine in which the cell temperatures are read in, {\it if} they have been specified in the geometry XML file. If the temperature has not been specified, then set the temperature to {\tt ERROR\_REAL} to be changed later. 

\begin{algorithm}[H]
 \While{i $<$ n\_cells}{
  cells(i) \% instances = 0\;
  cells(i) \% distribcell\_index = NONE\;
  \;
  read in cell name and ID\;
  read in universe (0 default) and fill (NONE if simple cell)\;
  \;
  \eIf{n\_mats $>$ 0}{
   cells(i) \% material(j) = "material name"\;
   }{
   cells(i) \% material(j) = NONE\;
  }
  \;
   \eIf{number of temps specified $>$ 0}{
   allocate(cells(i) \% sqrtkT(n))\;
   }{
  allocate(cells(i) \% sqrtkT(1))\;
  cells(i) \% sqrtkT(1) = ERROR\_REAL\;
  }
 }
\end{algorithm}

\item {\bf Assign temperatures to cells that did not have temperatures specified in the geometry XML file}. The {\tt assign\_temperatures(material\_temps)} subroutine deallocates and reallocates {\tt cells(i) \% sqrtkT(:)}, where the length of the temperature vector no longer refers to the number of materials in the cell (each material may have a temperature specified in the materials XML file). If any of the materials in the cell don't have temperatures defined, then use the global {\tt temperature\_default} of 293 K.

\item {\bf Determine temperatures at which to read cross sections and \(S(a,b)\) tables}. This is done in the reading of the materials XML file stage, since this must follow the geometry XML reading in case the user specifies temperatures in the geometry XML file. The {\tt read\_materials} subroutine declares, but does not initialize, an array {\tt nuc\_temps(:)}, which is to hold the temperatures to read for each nuclide. From within the {\tt read\_materials} subroutine, a call to the {\tt get\_temperatures} subroutine then allocates memory for and returns a list {\tt nuc\_temps(:)} of temperatures that each nuclide appears at in the model. Each entry in {\tt nuc\_temps(:)} corresponds to one nuclide in the model. 

\begin{algorithm}[H]
\While{i $<$ size of cells}{
  \While{j $<$ size of cells(i) \% material}{
    \While{k $<$ size of materials(i\_material) \% names}{
      \If{cells(i) \% sqrtkT(j) not in nuc\_temps(i\_nuclide)}{
        add temperature to nuc\_temps;
      }
    }
  }
}
\end{algorithm}

The {\tt get\_temperatures} subroutine is called with the global variables {\tt cells}, {\tt materials}, and {\tt n\_nuclides\_total} (so the parameter list could have been made smaller, since this subroutine acts on {\it all} of the nuclides in the problem). Each nuclide pertains to an index in the {\tt nuc\_temps} array (do distributed materials correspond to unique entries in the nuc\_temps array even though they are technically the same?). 

\item {\bf Read cross sections}. The {\tt nuc\_temps(:)} array is passed into the {\tt read\_ce\_cross\_sections} subroutine. Memory is allocated for the global {\tt nuclides} (holds nuclide cross sections), {\tt sab\_tables}, and {\tt micro\_xs} (allocated in parallel using OpenMP) arrays. By looping over all of the materials, and then through all of the names in a material (are names equivalent to nuclides here?), then populate the fields in the {\tt nuclides} global array by reading nuclear data from the HDF5 files by passing in the component of the {\tt nuc\_temps} array for the current nuclide. 

\begin{algorithm}[H]
\While{i $<$ size(materials)}{
  \While{j $<$ materials(i) \% names}{
    i\_nuclide = key representation of nuclide\;
    call nuclides(i\_nuclide) \% from\_hdf5(..., nuc\_temps(i\_nuclide), ...)\;
  }
}
\end{algorithm}

Hence, the {\tt nuc\_temps(:)} array contains all of the temperatures that each nuclide appears at in the input files, while the {\tt Type(Nuclide) :: nuclides(:)} type contains much more aggregate information for the nuclides in the model. In the call to {\tt nuclide\_from\_hdf5}, an array of desired temperatures to read is passed in (referenced locally as {\tt temperature}, but {\tt nuc\_temps(i\_nuclide)} is actually passed in, so this subroutine looks up all of the cross sections for a nuclide given a list of temperatures). Loop over the number of temperatures available in the HDF5 file (11 for JEFF-3.2), and store these temperatures in the local {\tt temps\_available(:)} array. Then, loop over the list of input temperatures that you would like to read data for. For each desired temperature, loop through the available temperatures until you find the two available temperatures that bound the current temperature. If the local array {\tt temps\_to\_read} does not yet have these bounding temperatures, then add them to this array. Finally, the {\tt kTs(:)} array of the global {\tt nuclides} data structure is populated with all of the temperatures at which the data will be read. So, {\tt nuc\_temps} gets directly converted into the {\tt kTs(:)} field of the {\tt nuclides} data structure, except that additional fields may be present in the {\tt nuclides}' {\tt kTs(:)} array to extract the temperatures in the HDF5 nuclear data files that most closely bound the desired temperature.

\begin{algorithm}[H]
\While{i $<$ number of temps available in HDF5}{
  temps\_available(i) = read from HDF5\;
}
\;
\eIf{TEMPERATURE\_NEAREST}{
}{
  \While{i $<$ nuc\_temps(i\_nuclide) \% size()}{
     temp\_desired = nuc\_temps(i\_nuclide) \% data(i)\;
     \While{j $<$ size(temps\_available) - 1}{
       \If{temp\_desired bounded by values in temps\_available}{
         \If{temps\_to\_read does not have these 2 bounding temps}{
           Add bounding temperatures to temps\_to\_read\;
         }
       }
     }
  }
}
\;
allocate(this \% kTs(temps\_to\_read \% size()))\;
read entries into this \% kTs\;
\end{algorithm}

\item {\bf Transport the particles one-by-one}. A {\tt Particle} is passed into the {\tt transport} subroutine to transport one particle.

	\begin{enumerate}
	\item Perform initialization actions outside of the event loop. Initialize the number of events to zero, add the particle's starting weight to the {\tt total\_weight}, force calculation of cross sections by setting {\tt micro\_xs \% last\_E = ZERO}, and zero out any accumulated flux derivative on the particle. 
	\item {\bf Enter the event loop}, which is exited when the particle dies and there are no secondary particles to track.
		\begin{enumerate}
		\item {\bf Determine the particle's cell if it is not known} ({\tt NONE}) by calling the {\tt find\_cell} subroutine. An optional list of cells is passed into the {\tt find\_cell} routine, and those cells are looped over to determine which cell a particle is in (if this list is not present, then all of the cells in the universe are searched). This subroutine is called recursively, and will only break out once reaching the lowest universe. Once reaching the lowest universe (where the cell is filled by a homogeneous material (cell type is {\tt FILL\_MATERIAL}), then this is where the {\tt p \% last\_material} and {\tt p \% last\_sqrtkT} entries are saved. A distributed cell exists if {\it either} the material or temperature lists have more than one value. Then, the particle's new temperature and material are saved.
				
		\begin{algorithm}[H]
		\While{i $<$ size of search\_cells}{
		  index\_cell = search\_cells(i)\;
		  \;
	      \eIf{cell\_contains(cells(index\_cell, p))}{
			p \% coord(j) \% cell = index\_cell\;
			\;
		  \If{cells(index\_cell) == FILL\_MATERIAL}{
			p \% last\_material = p \% material\;
		    p \% last\_sqrtkT = p \% sqrtkT\;
			\;
		  \If{size(cells(index\_cell) \% sqrtkT) $>$ 1 or size(cells(index\_cell) \% material) $>$ 1}{
			find distributed offset\;}
				      \;
				      p \% material = cells(index\_cell) \% material(offset + 1)\;
				      p \% sqrtkT = cells(index\_cell) \% sqrtkT(offset + 1)\;
				    }
				  }{
				    repeat;
				  }
				}
				\end{algorithm}
				
		\item {\bf Calculate cross sections} by calling {\tt calculate\_xs(particle)}, but only if the {\tt p \% material} differs from {\tt p \% last\_material} (new nuclide cross section) or if {\tt p \% sqrtkT} differs from {\tt p \% last\_sqrtkT} (new energy). All of the material macroscopic cross sections {\tt material\_xs} are set to zero. {\tt material\_xs} is a cache of type {\tt MaterialMacroXS} that holds cross section values for the current material. The current material is indicated by the integer value of {\tt p \% material}. Loop through all of the nuclides in the material, and calculate microscopic cross sections for each nuclide (this is forced by setting {\tt micro\_xs \% last\_E = ZERO}, so microscopic cross sections are re-calculated {\it every} event. {\tt micro\_xs} is a cache to hold microscopic cross section data for each nuclide in a material. Call {\tt calculate\_nuclide\_xs} subroutine if the last energy or last temperature at which microscopic cross sections were looked up has changed. The {\tt calculate\_nuclide\_xs} subroutine populates {\tt micro\_xs(i\_nuclide)} for the specified particle energy. It is in this routine that the temperature method is actually implemented. 
		
				\begin{algorithm}[H]
				\;
				mat = materials(p \% material)\;
				\;
				\While{i $<$ mat \% n\_nuclides}{
				  \If{p \% E /= micro\_x(mat \% nuclide(i)) \% last\_E or p \% sqrtkT /= micro\_x(mat \% nuclide(i)) \% last\_sqrtkT}{
				    calculate\_nuclide\_xs(...)\;
				  }
				  \;
				  read atom density of nuclide\;
				  add contribution to macroscopic cross section\;
				}
				\end{algorithm}
		
		\item Find distance to nearest boundary by calling {\tt distance\_to\_boundary}.
		\item Sample a distance to collision and store in {\tt d\_collision}. 
		\item Select the smaller of the two distances as the distance to move the particle, and then update the {\tt p \% coord(j) \% xyz} particle coordinates by looping through the number of particle coordinates.
		\item Score track-length tallies by calling {\tt score\_tracklength\_tally(p, distance)}.
		\item Score track-length estimate of \(k\) by adding to the {\tt global\_tally\_tracklength} variable.
		\item If the particle crosses a surface, then determine if it crossed either a lattice boundary or a surface boundary, and update the {\tt p \% event} field. Otherwise, the particle has a collision, which is assumed for all following steps.
		\item Score to the collision estimate of \(k\) by adding to the {\tt global\_tally\_collision} global variable. Note that this occurs before sampling the type of collision and the particle's phase space following the collision.
		\item Score to surface current tallies by calling {\tt score\_surface\_current(p)} before the particle direction changes, since the direction is important for this tally. 
		\item Perform a collision by calling {\tt collision(p)}. 
		\item Score collision estimator tallies by calling {\tt score\_collision\_tally(p)}. This requires that {\tt active\_collision\_tallies \% size()} be greater than zero. This occurs after the collision has occurred, so why can't this be used for tallies that require outgoing information?
		\item Reset banked weight during collision by setting {\tt p \% n\_bank = 0, p \% wgt\_bank = ZERO, p \% n\_delayed\_bank(:) = 0}. 
		\item Save coordinates for tallying purposes by writing to {\tt p \% last\_xyz\_current}. 
		\item Set {\tt p \% last\_material = NONE} because cross sections will need to be re-evaluated. 
		\item Check for secondary particles if this particle is dead. If any secondary particles exist, then initialize them by calling {\tt initialize\_from\_source}. 
		\end{enumerate}
	\end{enumerate}
\end{enumerate}

The temperature at which a cross section is evaluated can be performed in one of two ways - {\tt TEMPERATURE\_NEAREST} simply finds the nearest temperature value in the cross section data, and uses that value, while {\tt TEMPERATURE\_INTERPOLATION} will randomly sample (linearly) between the two temperatures that bound the current temperature. The default method is to use the nearest cross section. To use the interpolation method, note that the default temperature of 293 K cannot be used, since no cross sections exist below this temperature (so there is nothing to bound on the lower end).

The following changes were made to the temperature treatment in OpenMC:

\begin{itemize}
\item The {\tt get\_temperatures} subroutine will cause cross sections to be read from HDF5 files for all temperatures that appear in the geometry and materials XML files. For coupling, these XML files will be read only once to establish an approximate initial condition on temperatures. Then, for each subsequent Picard step, updated temperatures will be passed from MOOSE into OpenMC. If these updated temperatures were not also used in the XML input files, then no nuclear data will have been read in. A change is needed in order to read in all of the necessary nuclear data during the initialization stage. For now, this is alleviated by defining small regions of the domain that contain the temperatures needed (a material must actually be used in the geometry in order for {\tt get\_temperatures} to read in the data).
\end{itemize}

In order to use expansion coefficients from MOOSE within OpenMC to change temperatures, several changes were made to OpenMC to permit storage of these coefficients. 

\subsubsubsection{Changing Densities}
This section discusses the changes made in OpenMC to allow fluid densities to be changed given information from Nek. 

\subsection{Nek Data Transfer}
This section describes the capabilities introduced into the Nek source code that allows evaluation of polynomials, and construction/deconstruction of continuous fields. 

\subsubsection{Evaluation of Polynomials}
\subsubsection{Field Deconstruction}
\subsubsection{Field Reconstruction}
\subsubsection{Using the Constructed Field}

\section{Overall Coupling Methodology}
This section discusses the code added to MOOSE to compute the expansion coefficients for the surface heat flux transferred to Nek, and reconstruct a surface temperature given coefficients from Nek.

Both codes were modified to include the capabilities to perform these two generic abilities, since the boundary coupling is a two-way coupling - Nek transfers a surface temperature to MOOSE, and MOOSE transfers a surface heat flux to Nek. 

\section{Executioners and Timesteppers}
This section describes how an external code can be run from within the MOOSE framework by developing custom MOOSE {\tt Executioner} and {\tt TimeStepper} classes that run the external code. For this three-way coupling, both OpenMC and Nek are run by MOOSE by wrapping calls to their routines that run a simulation from within MOOSE class structures. In order to understand how to run an external code from within the MOOSE framework, a discussion of the MOOSE executioner and time stepper classes is given. 

When MOOSE performs a simulation, the following major steps are performed:

\begin{enumerate}
\item Call {\tt MooseInit} to initialize MPI, the solvers, and MOOSE.
\item Register the application's MooseApp and any that it depends on, such as modules. 
\item Create an instance of the application.
\item Execute the application by calling the {\tt run()} method of the App. This will perform setup, read the input file, and then call the {\tt executeExecutioner()} method. This method calls the {\tt init()} method, followed by the {\tt execute()} method of the executioner specified in the input file.
\end{enumerate}

Hence, the brunt work of the solve is contained in the executioner, so in order to couple an external code, most of the required calls to external code routines can be placed in the executioner in the appropriate methods for routines that should be called once per coupled solve, for every Picard step, and at the end of the simulation to perform cleanup. So, in order to call an external code from within the MOOSE framework, the structure of the executioner class must be understood. Because all of the coupled simulations of interest to this project will be transient, emphasis will be placed on the {\tt Transient} executioner. In a MOOSE simulation, the methods of the transient executioner and called in the following order:

\begin{enumerate}
\item At the very start of the simulation (before any time stepping has occurred), call {\bf {\tt init()}}. This method selects a default timestepper ({\tt constantDT}) if none is specified.
	\begin{enumerate}
	\item Call the {\tt init()} method of the timestepper - defined by custom time stepper
	\end{enumerate}
\item Perform the time stepping within the {\tt execute()} method.
	\begin{enumerate}
	\item {\tt preExecute} - call the corresponding method of the time stepper, which deletes all previous sync times
	\item {\bf Main time stepping loop}. For each time step, perform the following:
		\begin{enumerate}
		\item For all but the first time step, the problem state and MultiApps are advanced by calling the {\tt incrementStepOrReject()} method, which checks to make sure that the last solve converged (MasterApp + any MultiApps) before advancing the state.
		\item Then, the {\tt keepGoing()} method is called to determine whether or not to break from the main time stepping loop. This method will break out of the main time stepping loop if the last solve converged (MasterApp + any MultiApps) and a) you've reached the desired number of time steps or the end simulation time or b) you've reached the steady state check, and no longer need to run a transient simulation. This method will break out of the main time stepping loop if last solve did not converge, and you can no longer halve the time step.
		\item {\tt preStep()}  - defined by custom time stepper
		\item {\tt computeDT()} - compute \(\Delta t\), which calls {\tt computeStep} by the time stepper (where the current \(\Delta t\) is actually selected). If the solve has not converged, then the {\tt computeFailedDT()}  method of the time stepper simply cuts the time step in half, provided you're not  already at the minimum time step size. If you're at the first time step, then call the {\tt computeInitialDT()} method of the time stepper. This sets the value of {\tt \_current\_dt}.
		\item {\tt takeStep()} - {\bf Perform a single complete time step}. Initialize the number of Picard iterations to zero and back up the MultiApps. The Picard loop is then controlled as a while loop. For each Picard iteration:
			\begin{enumerate}
			\item {\tt solveStep()}  - {\bf solve a single Picard step}. 
				\begin{itemize}
				\item First, transfers to MultiApps are performed, and those MultiApps that execute on {\tt timestep\_begin} are solved. If those MultiApps don't converge, then return.
				\item {\tt preSolve()} - call Executioner and time stepper methods of the same name, both undefined.
				\item {\tt \_time\_stepper->step()} solves the FEM solution of the Master App. 
				\item After this has been called, we need to check that both the MasterApp and all of the timestep\_begin MultiApps have converged. If all converged, then perform transfers to and execute MultiApps on timestep\_end. Check that those MultiApps also converged.
				\item {\tt postSolve()} - call Executioner and time stepper methods of the same name, both undefined.
				\end{itemize}
			\end{enumerate}
		\item {\tt endStep()} computes error indicators and performs output to the terminal. 
		\item {\tt postStep()} - defined by custom time stepper
		\end{enumerate}
	\item {\tt postExecute} - defined by custom time stepper
	\end{enumerate}
\end{enumerate}

So, as long as the routines needed from an external code can be cast into one or several of these executioner and time stepper methods shown above, then a custom executioner class, such as {\tt OpenMCExecutioner} can be created to override the needed methods. This executioner would then be used in a MOOSE input file to run an external code from within the MOOSE framework.

\subsection{OpenMC Execution}
This section describes the OpenMC executioner and time stepper created that will call the OpenMC routines that are needed to perform a Monte Carlo simulation within the MOOSE framework.

\subsubsection{{\tt OpenMCExecutioner}}
The {\tt OpenMCExecutioner} inherits from the {\tt Transient} executioner, and hence the methods that are overridden are described in detail here.

\subsubsubsection{{\tt OpenMCExecutioner::postExecute()}}
This method should perform any finalization actions to be performed at the very end of the coupled simulation (a single time). Hence, this method contains routines to deallocate allocated memory. (Do I need to separate out the \% clear statements?)

\subsubsection{{\tt OpenMCTimeStepper}}

\subsubsubsection{{\tt OpenMCTimeStepper::step()}}
This is the method that should be called to run an OpenMC simulation for each Picard iteration (any possibly multiple Picard iterations per timestep). The {\tt solveStep()} method of the executioner calls the {\tt step()} method of the time stepper for each step, and hence these actions are placed in the OpenMC time stepper, rather than in the executioner itself.  

Because data transfer from MOOSE to OpenMC will occur in a Transfer object, this section should be independent of any input information. The core action kernel of OpenMC is the {\tt openmc\_run()} subroutine, which performs all of the logic of iterating over batches, generations, and particles and simulating the transport of each particle through the geometry. Below is shown a commented version of this subroutine. 

No changes were made to the {\tt openmc\_run} subroutine. However, for this routine to run multiple times in a row, several changes were made to other parts of the OpenMC source code. The normal progression of an OpenMC simulation consists of three steps - {\tt openmc\_init}, {\tt openmc\_run}, and {\tt openmc\_finalize}. If instead the simulation flow consists of a single call to {\tt openmc\_init}, followed by an arbitrary number of calls to {\tt openmc\_run}, and followed by a single call to {\tt openmc\_finalize}, then the following changes must be made regarding the {\tt openmc\_run} portion:

\begin{itemize}
\item OpenMC creates a {\tt stl\_vector} module that simulates the actions of the vector class in C++. This class defines members such as {\tt push\_back}, which simply appends entries to vectors. These data structures must be cleared before restarting {\tt openmc\_run}, or else the data structures will continue to increase in length, and results will be incorrect (is this still true with c-api branch?).
\item In the variable declaration sections of several OpenMC modules, variables are initialized. For example, the {\tt k\_sum(2)} array, which holds the cumulative sum of all computed values of \(k_{eff}\) in order to compute an average \(k_{eff}\) at the end of a single OpenMC simulation, is initialized to zero in the variable declaration portion of the {\tt eigenvalue} module. When calling {\tt openmc\_run} several times in a row, {\tt k\_sum(2)} is only set to zero the very first time - for further calls, the magnitudes of its entries simply keep increasing. Therefore, the {\tt openmc\_reset} subroutine, which should be called before every {\tt openmc\_run} call, correctly resets such variables. 
\end{itemize}

\subsection{Nek Execution}
This section describes the Nek executioner and time stepper created that will call the Nek routines that are needed to perform a computational fluid dynamics simulation within the MOOSE framework.

\subsubsection{{\tt NekExecutioner}}
The Nek executioner has all of the same parameters as the transient executioner, and only redefines the {\tt init()}, {\tt preStep()}, and {\tt postStep()} methods. The {\tt init()} method is called before executing the time stepper. The {\tt preStep()} and {\tt postStep()} methods are called surrounding the {\tt computeDT()}, {\tt takeStep()}, and {\tt endStep()} methods, and hence essentially surround the call to the methods that perform an actual time step solution.  

\begin{lstlisting}
#include "NekExecutioner.h"
#include "NekInterface.h"

template<>
InputParameters validParams<NekExecutioner>() {
  InputParameters params = validParams<Transient>();
  return params;
}

NekExecutioner::NekExecutioner(const InputParameters & parameters) :
    Transient(parameters) { }

void NekExecutioner::init() {
  Transient::init();
  FORTRAN_CALL(Nek5000::nek_init)(_communicator.get());
}

void NekExecutioner::preStep() {
  Transient::preStep();
  FORTRAN_CALL(Nek5000::nek_init_step)();
}

void NekExecutioner::postStep() {
  Transient::postStep();
  FORTRAN_CALL(Nek5000::nek_finalize_step)();
}
\end{lstlisting}

\subsubsection{{\tt NekTimeStepper}}







\section{OpenMC Theory}
This section provides my notes on the Monte Carlo method used in OpenMC. For the purposes of code coupling, it is important to understand how the numerical algorithm is mapped into Fortran code, since this algorithm is to be split up into modular pieces that should be able to run in a particular order, but otherwise independently. For instance, the transport loop requires an initialization period to get problem parameters from the user, but then the simulation should be able to run repeatedly, in many independent instances. 

The central limit theorem guarantees that the variance of the sample mean is inversely proportional to the number of particles \(N\):

\beq
\sigma^2\propto\frac{1}{N}
\eeq

OpenMC performs a Monte Carlo simulation one particle at a time. Before any transporting can occur, initializations are required. 

\subsection{OpenMC Data Structures}
This section discusses the data structures such as timers and particles used in the simulation process. 

\subsubsection{Banks - {\tt bank\_header.F90}}
A bank is used for storing fission sites in eigenvalue calculations. All of the state information of a neutron is not needed in order to sample from the fission sites in the next generation, storing this information in a Bank datatype uses less memory. The bank sites are associated with a scalar weight (that applies to all of the bank sites), the location of a bank particle (3 coordinates), directional cosines (3 velocities), an energy/energy group, and a delayed group. 

\subsubsection{Materials - {\tt material\_header.F90}}
Each material has a unique identifier, and is associated with a number of nuclides (i.e. a material is a mixture). The nuclides and nuclide number densities are stored in arrays indexed by the constituent nuclide. 

\subsubsection{Timers - {\tt timer\_header.F90}}
There are two general ways to report differences in time - {\tt system\_clock} and {\tt cpu\_time}. The system clock reports wall time, or elapsed time, while CPU time only reports time used by the CPU. The {\tt Timer} data type defines {\tt start} and {\tt stop} methods. The argument passed to the system clock call is a variable that holds the number of CPU counts relative to some arbitrary point in time. The value held in a timer is computed using the {\tt get\_value} method, which takes the difference between the integer numbers stored in the start and stop counters, and divides it by the count rate of the computer. So, these timers do {\it not} accumulate - they can only measure the time difference between two instances in time. The {\tt reset} method turns the timer off and sets the start counts to zero and the elapsed time to zero.

\subsection{\tt OpenMCExecutioner::init()}
This method is called one time, at the very beginning of the entire simulation. In other words, this method is not called for each time step. This method should perform tasks that are required for {\it all} OpenMC runs needed later in the coupled simulation, such as memory allocation, MPI setup, reading initial data from XML files, etc. The actions taken by this method should be completely independent of any input information from MOOSE such as temperatures that are to be coupled during Picard iterations. 

The {\tt openmc\_init} subroutine is shown below, with comments indicating what each included subroutine performs. The following modifications were made to the {\tt openmc\_init} subroutine in order to separate out actions that should be performed once per entire coupled simulation, versus once per receipt of a data transfer from a coupled MOOSE simulation. 

The initialization process includes the following major steps:

\begin{itemize}
\item Read input files and build data structures for the geometry, materials, tallies, and other variables.
\item Initialize a random number seed if not provided in the user input.
\item Read cross section data. For multi-group problems, combine information for individual nuclides into material-specific cross section data. 
\item Initialize any special energy grid treatments, such as a union energy grid or lethargy bins. 
\item Sample the particle source sites and the particle direction and energy. For a fixed source simulation, these are known explicitly and are specified in an input file, but for an eigenvalue problem, a rough initial source is either specified in the input file or is assumed to be some default source. 
\end{itemize}

\begin{lstlisting}[language=fortran]
  subroutine openmc_init(intracomm)
#ifdef MPIF08
    type(MPI_Comm), intent(in) :: intracomm     ! MPI intracommunicator
#else
    integer, intent(in), optional :: intracomm  ! MPI intracommunicator
#endif

    ! this timer records the entire runtime (start to finish - time_total%stop() is called at the end of finalize_simulation() (at the end of openmc_run()). If we leave this here, then this time_total will represent the time from start of the coupled MOOSE-OpenMC simulation to wherever time_total%stop() is called. As-is, this will represent the time from the start of the coupled simulation to the end time of the first Picard iteration (not the first time step - i.e. the first complete OpenMC run). We need to adjust this timer, because otherwise the timer will be started in openmc_init, and it will be repeatedly stopped in openmc_run(). Stopping a timer that has not yet been started will simply return zero elapsed time, which will not be accurate. 
    call time_total%start() 
    
    ! represents time from start to stop of openmc_init()
    call time_initialize%start()

#ifdef MPI
    ! Determine the number of processes in the intra-communicator and the rank of each process. This does not call MPI_INIT unless MPI_INIT has not yet been called (and hence shouldn't pose a problem when coupling with MOOSE, which calls MPI_INIT). This also creates compound MPI data types (MPI_BANK type, which has five members - weight, position, velocity, energy, and delayed neutron group). In order to correctly use an MPI custom data type, we need to know where the addresses of that custom data type are in the custom data structure - MPI_GET_ADDRESS is used to adjust the displacements so that they are portable amongst different systems. 
    call initialize_mpi(intracomm)
#endif

    ! Initialize the HDF5 library (h5open_f), and create custom HDF5 data types for coordinates and velocities, banks (hdf5_bank_t) (which inserts the types for coordinates and velocities). This will be used when writing output files that show the bank information in HDF5 format.
    call hdf5_initialize()

    ! Read command line arguments. Because OpenMC is called as a library, nothing is passed on the command line - its functions and subroutines are called directly. I tried setting an OpenMC command line parameter when calling a MOOSE executable, and it wasn't recognized. When calling OpenMC as a library, it counts "-1" command line arguments (i.e. when I run the MOOSE executable). The number of OpenMP threads can be set from the command line. This allocates/deallocates memory to hold the number and values of command line arguments. 
    call read_command_line()

    ! Read XML input files (settings, geometry, materials, and tallies). 
    ! read_settings_xml(): The number of OpenMP threads can be set in the input file, after which omp_set_num_threads() is called. The settings.xml file specifies the source distribution - the default is to use an isotropic point source at the origin with a Watt spectrum. This call allocates storage for the source. Instead of using the default, the source can be specified from an input file, or as a guess in the input file. The source can be specified as only the fissile material, or a variety of other options. If not specified, the default angular distribution is isotropic. This also sets up and allocates the Shannon entropy mesh, weight/energy cutoffs, particle traces and tracks, the UFS mesh, statepoint file requirements (by default only written for the last batch), resonance scattering methods, and temperature settings (nearest or interpolation).
    ! read_geometry_xml(): This method parses the geometry settings. Memory is allocated for the cells, for which a material or a fill must be specified. This also applies any rotations and translations for filler universes. Cell temperatures are read. If the temperature is not specified, it is set to the temperature read from the material data. The temperature is converted to sqrt(kT) instead of Kelvin. The cells are stored in a dictionary. Then, the surfaces array is allocated. Boundary conditions are read and applied to the surfaces data structure. The opposite surface is determined for periodic boundaries. Then, any lattices are read, and an array allocated for the lattices. 
    ! read_materials/_xml(): Read in the cross sections and the windowed multipole library, and store paths into environment variables. Temperatures are assigned to cells that don't already have temperatures assigned using the assign_temperatures(material_temps) method. 
    ! read_tallies_xml(): This reads tallies.xml and allocates memory for a mesh and tally filters, and creates instances of certain tally objects. 
    
    call read_input_xml()

    ! Initialize random number generator -- this has to be done after the input
    ! files have been read in case the user specified a seed for the random
    ! number generator. Do we need to recreate random seeds for each timestep?
    call initialize_prng()

    ! Read plots.xml if it exists -- this has to be done separate from the other
    ! XML files because we need the PRNG to be initialized first. 
    if (run_mode == MODE_PLOTTING) call read_plots_xml()

    ! Use dictionaries to redefine index pointers. This changes the IDs used for
    ! surfaces and materials. This also assigns boundary conditions to surfaces.
    call adjust_indices()

    ! Initialize distribcell_filters. This is needed if any tallies have distribcell 
    ! filters, or if there are any distributed materials or temperatures. A material 
    ! is "distributed" if each instance of the cell gets its own material (such as 
    ! the fuel in fuel pin universes that fill a lattice universe). 
    call prepare_distribcell()

    ! After reading input and basic geometry setup is complete, build lists of
    ! neighboring cells for efficient tracking. Then, when a particle crosses a
    ! surface, you have to search a smaller number of neighboring cells to 
    ! determine where the particle will go. 
    call neighbor_lists()

    ! Check to make sure there are not too many nested coordinate levels in the
    ! geometry since the coordinate list is statically allocated for performance. 
    if (maximum_levels(universes(root_universe)) > MAX_COORD) then
      call fatal_error("Too many nested coordinate levels in the geometry. &
           &Try increasing the maximum number of coordinate levels by &
           &providing the CMake -Dmaxcoord= option.")
    end if

    if (run_mode /= MODE_PLOTTING) then
      ! Construct information needed for nuclear data
      if (run_CE) then
        ! Construct log energy grid for cross-sections
        call logarithmic_grid()
      end if

      ! Allocate and setup tally stride, filter_matches, and tally maps. This 
      ! allocates global tallies and sets up tally arrays. 
      call configure_tallies()

      ! Set up tally procedure pointers. 
      call init_tally_routines()

      ! Determine how much work each processor should do (allocate the particles amongst the processors). 
      call calculate_work()

      ! Allocate source bank, and for eigenvalue simulations also allocate the fission bank. If using OpenMP, each thread needs its own private fission bank. At the end of each generation, all of the information from the OpenMP threads is re-combined into the master_fission_bank. 
      call allocate_banks()

      ! If this is a restart run, load the state point data and binary source file. 
      ! This overwrites some parameters, while keeping the restart values of others. 
      if (restart_run) call load_state_point()
    end if

    if (master) then
      if (run_mode == MODE_PLOTTING) then
        ! Display plotting information
        if (verbosity >= 5) call print_plot()
      else
        ! Write summary information
        if (output_summary) call write_summary()
      end if
    end if

    ! Check for particle restart run
    if (particle_restart_run) run_mode = MODE_PARTICLE

    ! Warn if overlap checking is on
    if (master .and. check_overlaps .and. run_mode /= MODE_PLOTTING) then
      call warning("Cell overlap checking is ON.")
    end if

    ! Stop initialization timer
    call time_initialize%stop()

  end subroutine openmc_init
\end{lstlisting}








\subsection{Preparing for the Next Coefficient Transfer from MOOSE}
Within {\tt openmc\_reset}, the {\tt expansion\_coeffs} array is cleared before each new Picard step (why is this not done with the other vector-type variables?). Note that the {\tt clear} method for the {\tt VectorReal} type only sets the size to zero - the actual numerical values are still held in memory. 

\subsection{Freeing Memory at End of Simulation}
Finally, it is cleared at the end of the simulation within {\tt free\_memory}, which is called from the {\tt openmc\_finalize} subroutine.








\section{The Mechanics of Coupling}
This section discusses how each of the external codes are built to permit their use from within the MOOSE framework, as well as the input files required for each external code, and how those input files should be located relative to the executable of the main Moose App.

\subsection{OpenMC}

\subsubsection{Build Configuration}
This section discusses how OpenMC is built to allow its routines to be called from within the MOOSE framework.

\subsubsection{Input Files}
This section discusses the input files needed by OpenMC, and where they should be placed relative to the Master App executable.

\subsection{Nek}

\subsubsection{Build Configuration}
This section discusses how Nek is built to allow its routines to be called from within the MOOSE framework.

\subsubsection{Input Files}
This section discusses the input files needed by Nek, and where they should be placed relative to the Master App executable.

\section{Nested Parallelism}
When using the MultiApp system in MOOSE, flexible capabilities exist for nesting parallelism. For instance, if MOOSE is run with 4 MPI processes, then passing in a communicator to OpenMC will run OpenMC with 4 MPI processes as well. 

For illustration of how to achieve various nested parallelism structures, consider the example of running the Okapi executable using 10 MPI processes. By default, all 10 of these processes will be passed into OpenMC, and OpenMC will be run with the same number of processes. 

\begin{enumerate}
\item Case 1: You wanted to limit the number of parallel processes devoted to a SubApp. You can set the {\tt max\_procs\_per\_app} parameter in the MultiApp, which will limit the number of MPI processes that get passed to OpenMC. For a Monte Carlo code that will likely always require billions of particles, this is not very useful, but for SubApps that consist of very simple calculations, you don't want to assign too many MPI processes to perform a simple calculation, or else your performance will likely degrade.
\end{enumerate} 

In order to determine what changes need to be made in MOOSE in order to pass in a different communicator to OpenMC than is used in Nek, the following is a summary of the execution process taken in a Moose App. 

\begin{enumerate}
\item Initialize MPI, solvers, and MOOSE. {\tt MooseInit} is a class that inherits from the {\tt LibMeshInit} class. The default communicator passed into the constructor of {\tt MooseInit} is {\tt MPI\_COMM\_WORLD}. In the body of the constructor, the number of OpenMP threads used in MOOSE (which is only allowed if the {\tt LIBMESH\_HAVE\_OPENMP} environment variable is set) is set to the same number that is used in LibMesh. The {\tt LibMeshInit} class initializes any dependent libraries (MPI and PETSC) and parses the command line. The destructor closes those libraries appropriately. The constructor for the {\tt LibMeshInit} class sets the default {\tt COMM\_WORLD\_IN} to {\tt MPI\_COMM\_WORLD}. The {\tt LibMeshInit} function by default sets the number of threads to 1 to avoid MPI-multithreading competition. If the program calling {\tt LibMeshInit} has not already initialized MPI, then LibMesh initializes MPI by calling {\tt MPI\_Init\_thread}, where the level of requested thread support is set to {\tt MPI\_THREAD\_FUNNELED} if there is more than one thread (all MPI calls are funneled to a single thread so that each OpenMP thread does not call MPI) and to {\tt MPI\_THREAD\_SINGLE} if there is a single thread. Then, the input communicator is duplicated for internal use. {\tt MPI\_Finalize()} is called in the destructor for the {\tt LibMeshInit} class. 
\end{enumerate}

\end{document}
