\documentclass[10pt]{article}
\usepackage[letterpaper]{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage{setspace}
\usepackage{ragged2e}
\usepackage{algorithm2e,etoolbox}
\AtBeginEnvironment{algorithm}{\let\textnormal\ttfamily\DontPrintSemicolon}
\usepackage{color, colortbl}
\usepackage{titlesec}
\usepackage{graphicx}
\usepackage{float}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage[font=small,labelfont=bf,labelsep=period]{caption}
\usepackage[english]{babel}
\usepackage{indentfirst}
\usepackage{array}
\usepackage{makecell}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{arydshln}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xfrac}
\usepackage{etoolbox}
\usepackage{cite}
\usepackage{url}
\usepackage{dcolumn}
\usepackage{hyperref}
\usepackage{courier}
\usepackage{url}
\usepackage{esvect}
\usepackage{commath}
\usepackage{verbatim} % for block comments
\usepackage{enumitem}
\usepackage{hyperref} % for clickable table of contents
\usepackage{braket}
\usepackage{titlesec}
\usepackage{booktabs}
\usepackage{gensymb}
\usepackage{longtable}
\usepackage{glossaries}
\usepackage{tcolorbox} % for colored boxes
\tcbuselibrary{breakable} % to allow colored boxed to extend over multiple pages
\usepackage[mathscr]{euscript} % for script letters
\usepackage{wasysym}  % for checkboxes
\usepackage{bm} % for bolding math symbols
\usepackage{listings} % for including code in boxes
\setcounter{MaxMatrixCols}{20} % sets max matrix length as 20 columns

\lstdefinestyle{mystyle}{ 
    commentstyle=\color{gray},
    keywordstyle=\color{magenta},
    stringstyle=\color{orange},
    basicstyle=\footnotesize,
    language=C++,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,          
    basicstyle=\small\ttfamily,        
    tabsize=2
}

\lstset{style=mystyle}

% change spacing around sections
\titlespacing*{\section}
{0pt}{8ex}{1ex}
\titlespacing*{\subsection}
{0pt}{5ex}{1ex}

% new commands for shorter writing of equations
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\beqa}{\begin{equation}\begin{aligned}}
\newcommand{\eeqa}{\end{aligned}\end{equation}}

% Acronyms used
\setacronymstyle{long-short}
\newacronym{anl}{ANL}{Argonne National Laboratory}
\newacronym{cfd}{CFD}{Computational Fluid Dynamics}
\newacronym{fe}{FE}{Finite Element}
\newacronym{fem}{FEM}{Finite Element Method}
\newacronym{inl}{INL}{Idaho National Laboratory}
\newacronym{io}{I/O}{Input/Output}
\newacronym{moon}{MOON}{MOOSE and Nek}
\newacronym{moose}{MOOSE}{the Multiphysics Object-Oriented Simulation Environment}

\titleclass{\subsubsubsection}{straight}[\subsection]

% define new command for triple sub sections
\newcounter{subsubsubsection}[subsubsection]
\renewcommand\thesubsubsubsection{\thesubsubsection.\arabic{subsubsubsection}}
\renewcommand\theparagraph{\thesubsubsubsection.\arabic{paragraph}} % optional; useful if paragraphs are to be numbered

\titleformat{\subsubsubsection}
  {\normalfont\normalsize\bfseries}{\thesubsubsubsection}{1em}{}
\titlespacing*{\subsubsubsection}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{5}{\z@}%
  {3.25ex \@plus1ex \@minus.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries}}
\renewcommand\subparagraph{\@startsection{subparagraph}{6}{\parindent}%
  {3.25ex \@plus1ex \@minus .2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries}}
\def\toclevel@subsubsubsection{4}
\def\toclevel@paragraph{5}
\def\toclevel@paragraph{6}
\def\l@subsubsubsection{\@dottedtocline{4}{7em}{4em}}
\def\l@paragraph{\@dottedtocline{5}{10em}{5em}}
\def\l@subparagraph{\@dottedtocline{6}{14em}{6em}}
\makeatother

\newcommand{\volume}{\mathop{\ooalign{\hfil$V$\hfil\cr\kern0.08em--\hfil\cr}}\nolimits}

\numberwithin{equation}{section} % for equation numbering

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}
\makeglossaries
\begin{document}

\title{OpenMC, BISON, and Nek Coupling}
\author{Argonne National Laboratory}
\maketitle

\tableofcontents
%\printglossary[type=\acronymtype,title=Abbreviations]

\clearpage

\section{Introduction}

This document is intended to describe the coupling methodology between Nek, a \gls{cfd} code developed by \gls{anl}, OpenMC, a Monte Carlo code developed by \gls{anl}, and BISON, a nuclear fuel performance code developed by \gls{inl}. Previous work by Matthew Ellis and others at MIT resulted in the coupling of OpenMC to MOOSE, and work by a joint effort of \gls{anl} and \gls{inl} resulted in the coupling of Nek to MOOSE. So, the coupling of all three codes together will attempt to utilize as much as possible regarding this previous work. The purpose of this report is to document the coupling process, justify decisions made, and illustrate how to use these codes in a coupled manner.

This report is organized beginning with a description of the overall coupling methodology, followed by a discussion of the nuts-and-bolts requirements for coupling two codes in terms of the data transfer between the codes and executing non-MOOSE codes from within the MOOSE framework. Finally, the end of this document discusses the mechanics of coupling the three codes together in terms of the build configuration. This document serves as my working notes, so any lingering ``dangers'' that are not fully resolved are indicated in pink font.

\clearpage
\section{Overall Coupling Methodology}
This section discusses the overall coupling methodology that was pursued between OpenMC-BISON, BISON-Nek, and finally OpenMC-BISON-Nek. The immediate jump to the coupling of all three codes was not performed because taking smaller steps often helps reveal difficulties that would otherwise be compounded and confusing when performing the full three-way coupling. When coupling two distinct projects, there are generally three ways to perform the coupling:

\begin{enumerate}
\item Make virtually no changes to the source code either project, but write an external script that will:
	\begin{enumerate}
	\item Run the first code.
	\item Read the text output file of the first code and modify the text input file for the second code.
	\item Run the second code.
	\item Read the text output file of the second code and modify the text input file for the first code. 
	\item Repeat (a)-(d) until convergence, then move to the next time step.
	\end{enumerate}
	This approach is necessary for projects that restrict access to source code, since a more internal coupling requires modifications to the source code of one or more of the projects. The only interaction with the software with this method is through text files.
\item Write a custom driver that will call routines from each code. This may be fast to develop, but limits the reusability of the driver itself or changes made to the two codes to meld to the driver structure. For example, pseudo-code for a driver written to couple OpenMC and Nek may have the following overall structure:

	\begin{algorithm}[H]
	{\tt initialize OpenMC}\;
	{\tt initialize Nek}\;
	\;
	\While{t $<$ end time}{
	  \While{not converged}{
	    run OpenMC\;
	    transfer data to Nek\;
	    run Nek\;
	    transfer data to OpenMC\;
	    evaluate convergence\;
	  }
	  advance time state\;
	}
	\end{algorithm}
	
\item Use an existing driver with a sufficiently general structure that the codes you want to couple can be structurally changed to be composed of routines that can be called from within the existing driver (no editing of text files). This approach, by virtue of using an existing driver, allows you to immediately take advantage of the ability to couple to any other codes that have been cast in terms of that driver's methods. However, this approach requires a good understanding of the existing driver, which can be much more time consuming than developing a custom driver from scratch. In addition, using an existing coupling framework can potentially restrict control over parallelism and performance, since the driver structure may not be ideal for every application.
\end{enumerate}

Adapting OpenMC and Nek to an existing driver framework was selected for this work for the reasons given above. Work at ANL has simultaneously worked towards developing a custom driver to run OpenMC and Nek, but this will not be discussed here. The MOOSE framework was selected as the driver for the following reasons:

\begin{enumerate}
\item Many applications have been developed with the MOOSE framework, such as nuclear fuels simulations (BISON), porous media thermal-hydraulics (PRONGHORN), and many other physics. Using the MOOSE coupling framework allows OpenMC and Nek to essentially become "MOOSE" Animals in that they can not only be coupled to each other, as the scope of this project seeks, but to these other codes with relatively minor effort.
\item Significant work has already been performed with coupling OpenMC to MOOSE and Nek to MOOSE, so the coupling of OpenMC with Nek in principle can directly take advantage of this existing work.
\item The MOOSE framework is easy-to-use and has significant coupling capabilities already developed, as well as an accessible development team for assistance.
\end{enumerate}

If the parallel performance is not acceptable, then the work developed in constructing OpenMC and Nek as MOOSE applications can still be reused for a custom driver due to MOOSE's modular structure. The MOOSE coupling framework performs coupling in a tree-like structure. A Master App controls the entire coupled solve, and is the App that performs actions such as initializing MPI and transferring data amongst all of the Apps. For the Master App, there may be any number of sub-applications. The terminology "Sub-app" and "MultiApp" are essentially the same, where the use of sub app is used primarily to denote that a MultiApp is "down-the-tree" from another MultiApp. 

\begin{figure}[H]
\centering
\includegraphics[width=10cm]{figures/multiapp_hierarchy.png}
\caption{MasterApp and MultiApp structure for a MOOSE simulation.}
\end{figure}

When using the MOOSE coupling framework, decisions must be made regarding the Master App-Multi App structure, such as which App acts as the Master App, and which as the Multi Apps. When only two codes are coupled, this choice is arbitrary, but when three or more codes are coupled this choice becomes important in determining the coupling structure, as will be discussed later. The remainder of this section discusses the high-level coupling strategy for OpenMC-MOOSE, Nek-MOOSE, and finally OpenMC-Nek-BISON. 

\subsection{OpenMC and BISON Coupling}
This section discusses the OpenMC-BISON coupling illustrated through an example that can be found in {\tt okapimcs/tests/FETs/bison-openmc}. This coupling is performed here using MOOSE as the Master App and Okapi and BISON as peer MultiApps, but because BISON is already a MOOSE Animal, this coupling can also be performed using BISON as the Master App and Okapi as the MultiApp (though this would only work for one fuel pin, since there cannot be multiple instances of the Master App). As an aside, to be fully explicit, you cannot have ``MOOSE'' as the Master App, since it just the framework. A Moose App must be the Master App, so when MOOSE is referred to as the Master App, this means that one of the Moose Apps used in the simulation is also used as the Master App, but it does not perform any physics solves, but just exists to pass data. For this implementation, Okapi is both the Master App and a Multi App.

Finally, because the later three-way coupling uses Okapi as the Master App, this coupling can {\it also} be performed with BISON as the MultiApp and Okapi the Master App. The remainder of the discussion in this section assumes a MOOSE Master App, and BISON and Okapi peer MultiApps, and the details would only be slightly modified to discuss these other configurations. OpenMC is wrapped as a MOOSE App so that it can be called as a MultiApp to the BISON Master App. BISON is already a MOOSE animal, and hence does not require any wrapping.

In a coupled solve, the following steps occur. We arbitrarily choose OpenMC to execute before BISON, so we choose to execute OpenMC on {\tt timestep\_begin}, and BISON on {\tt timestep\_end}. 

\begin{enumerate}
\item Initialize the MasterApp. The MasterApp does not perform any solves, but only exists to transfer data between its MultiApps.
\item Initialize OpenMC. This calls the {\tt OpenMCExecutioner::init()} method, which performs the regular initialization steps for any MOOSE App, followed by calling the {\tt openmc\_init} subroutine found in the OpenMC source. This subroutine performs basic tasks such as reading in the XML files that control OpenMC parameters (none are set in the MOOSE input files), allocating memory for data structures, etc.
\item Initialize BISON. Because BISON is a Moose ``animal,'' this simply calls the initialization method of the MOOSE executioner that we chose to use (for most cases, this will be {\tt Transient::init()}).
\item Perform any transfers that are defined on {\tt timestep\_begin} to the MultiApps. This calls the {\tt execute()} method of the {\tt MultiAppOkapiMooseTransfer} transfer to pass the {\tt l\_0\_coeffs\_temp} scalar variable to OpenMC (because OpenMC is not a MOOSE ``animal,'' we cannot simply use any of the transfers found in the MOOSE framework). For this step, we are transferring from MOOSE to Okapi, so we call the {\tt OpenMC::openmc\_cell\_set\_temperature} method, which takes as its arguments the OpenMC cell to change temperature and a single value. Without continuous material tracking implemented yet in OpenMC, this only selects the first expansion coefficient for temperature (so this transfer is incomplete), essentially performing a zeroth-order Legendre and Zernike expansion. This is where an initial condition on temperature is applied in OpenMC.
\item Execute OpenMC by calling the {\tt OpenMCExecutioner::execute()} method, which calls the \\{\tt openmc\_reset} and {\tt openmc\_run} subroutines in the OpenMC source. This performs a full Monte Carlo solve.
\item Transfer information back from OpenMC to the MOOSE Master App. This is done by calling the {\tt MultiAppOkapiMooseTransfer::execute()} method, which calls the {\tt fet\_deconstruction} subroutine in the OpenMC source. This subroutine stores the expansion coefficients for the FETs (which we indicated in the tallies XML input) by cell index. Then, the {\tt get\_coeffs\_from\_cell} subroutine is called to store the expansion coefficients in OpenMC in a C++ array. Finally, those values are stored in the {\tt l\_0\_coeffs\_power} scalar variable in the MOOSE Master App.
\item ``Solve'' the Master App. This is purely a dummy solve, but is required for any auxkernels to evaluate, and for any further transfers to occur.
\item Transfer the kappa fission distribution coefficients from MOOSE to BISON. This uses the \\{\tt MultiAppScalarToAuxScalarTransfer} to simply copy the coefficients for power held in \\{\tt l\_0\_coeffs\_power} to the {\tt l\_0\_coeffs\_power\_bison} variable in BISON.
\item BISON uses these coefficients to expand a continuous field variable {\tt bison\_kappa\_fission}, which holds the kappa fission distribution that was calculated by OpenMC. Functions for the Legendre, Zernike, and reconstruction are included in the BISON input file.
\item BISON then applies the {\tt KappaFissionToHeatSource} auxkernel on the {\tt bison\_heat} auxvariable to convert the kappa fission source to the correct units of W/volume that is used as a heat source in the {\tt HeatSource} kernel.
\item Solve BISON to solve for the temperature, {\tt bison\_temp}.
\item BISON deconstructs this continuous temperature field into a set of expansion coefficients, stored in {\tt l\_0\_coeffs\_temp\_bison} by calling the {\tt ZLDeconstruction} user object.
\item Transfer coefficients for temperature from BISON to MOOSE by calling the \\{\tt MultiAppScalarToAuxScalarTransfer} transfer. These coefficients are stored in the {\tt l\_0\_coeffs\_temp} variable in MOOSE. 
\item Repeat steps 4-13 for each Picard step until reaching the end of the coupled simulation.
\end{enumerate}

To summarize the above detailed steps, the following graphic shows the generic data flow for a single Picard step (i.e. the first three steps of initializing the Master App and two MultiApps are assumed to have occurred). Individual steps are numbered in the order in which they occur, where the names of the methods that are called to perform each step are shown.

\begin{figure}[H]
\centering
\includegraphics[width=15cm]{figures/OpenMC-BISON-complicated.png}
\end{figure}

\subsection{Nek and BISON Coupling}
This section discusses the coupling between Nek and BISON, where Nek is wrapped as a Moose App (MOON) to facilitate this coupling. The coupling discussed in this section was performed as a joint effort between INL and ANL in the fall of 2016.

\subsection{OpenMC, Nek, and BISON Coupling}
When coupling more than two applications using the MOOSE framework, there are several options for the coupling structure. In order to motivate the structure selected, an understanding of the limitations of the MOOSE coupling framework must be understood. The general coupling framework for a MOOSE simulation consists of a Master App and MultiApps. For each time step, unless sub-cycling is turned on (in which case one of the Apps may run at finer time steps than the others), all of the Apps, including the Master App, will execute. The order in which the execution of the Apps occurs is controlled with the {\tt execute\_on} parameter, which can take several values, most commonly {\tt timestep\_begin} and {\tt timestep\_end}. The MultiApps that execute on the start of a time step and on the end of a time step bound the execution of the Master App. If there are more than two MultiApps to a Master App, then in order to ensure full exchange of information at each time step, a custom routine for transferring data {\it in time} (not a custom {\tt Transfer} class, to be discussed later) is required. This would require developing a custom {\tt MultiApp} class, which is where the execution in time is controlled (note that there are other options for the {\tt execute\_on} parameter, but they are related to the finite element solve, such as {\tt linear\_residual} - with an external code that is not running any MOOSE kernels, these options don't make sense).

To keep the coupling as simple as possible and to avoid the need to develop custom Multi Apps (which is more intensive than developing custom transfers, executioners, and time steppers), the coupling is chosen to be limited to two distinct MultiApps per Master App. Note that this does {\it not} limit a total of two MultiApps per Master App - if the MultiApps are BISON and MOON, there may be many parallel instances of BISON, so long as there are only two distinct MultiApps (BISON and MOON). When selecting the Master App, the Master App can either perform a solve (and hence be one of Okapi, BISON, or MOON) or simply exist to transfer data between the Multi Apps (and hence be one of Okapi, BISON, MOON, PRONGHORN, RATTLESNAKE, etc. - the animal itself doesn't matter because it's only using capabilities in the MOOSE framework, not any physics-specific capabilities). The complication with using a master App for this three-way coupling that does not perform any physics solve is that it becomes much more difficult to control the execution of three peer Apps. There are only two available execution instructions that apply for non-MOOSE codes - {\tt timestep\_begin} and {\tt timestep\_end}, which does not offer enough flexibility in specification to define how {\it three} peer Apps should run.

If we restrict ourselves to only using two peer MultiApps per Master App, then the Master App must perform a physical solve. For a Master App that performs a physical solve, either BISON, MOON, or Okapi can be the Master App. Because multiple fuel pins can be simulated entirely in parallel (since a boundary coupling is employed with Nek), it would be inadvisable to have BISON be the Master App. This would restrict the coupling to a single fuel pin because there can only be a single Master App. A non-trivial amount of MPI code {\it could} allow this configuration to work correctly, but it would be much easier for either Okapi or MOON to be the Master App. 

The choice between MOON and Okapi for the Master App is more arbitrary than the choice to not choose BISON as the Master App, but is still based on characteristics of the coupling. Because Okapi is the only App that will solve for the entire domain (MOON solves in the fluid, BISON solves in the fuel), it is logical to choose Okapi as the Master App. The Okapi Master App both performs its own solve and transfers data between BISON and MOON. The example in {\tt tests/transfers/okapi-bison-nek} shows a functioning three-way coupling for a single fuel pin using this coupling structure. The figures that follow make references to variables in these files for the purpose of being explicit. The figure below shows all of the necessary methods to perform a three-way coupled simulation, with numbers indicating the order in which they would occur for a single Picard step. MOON is arbitrarily chosen to run first. 

\begin{figure}[H]
\centering
\includegraphics[width=17.5cm]{figures/OpenMC-BISON-Nek-complicated.png}
\caption{Sequence of events taken in one Picard iteration with MOON arbitrarily chosen to run first.}
\end{figure}

With Okapi as the Master App, either BISON can be run before MOON, or MOON run before BISON. Section \ref{sec:ICs} will discuss how boundary conditions are set based on which of the MultiApps executes first. For either choice of App running at the beginning of a time step versus at the end of a time step, the sequence of events is as follows. For MOON executing first:

\begin{enumerate}
\item Initialize the Master App. The actions performed here are generally the same regardless of which App is the Master App (i.e. this is not the stage at which the OpenMC input XML files would be read), since this purely sets up the entire MOOSE simulation by initializing MPI, allocating storage, etc. This is when initial conditions specified in the master input file are set.
\item Initialize MOON. This sets initial conditions specified in the MOON input file and {\it then} calls the {\tt NekExecutioner::init()} method (because we call the {\tt init()} method of {\tt Transient} before that of {\tt NekExecutioner}) and reads Nek input files to perform internal actions such as setting initial values.
\item Initialize BISON. This calls the initialization method of the executioner specified in the BISON input file. This is where initial conditions set in the BISON input file are set.
%
\item Initialize the Master App. This step is not redundant with step 1, but rather this calls the {\tt init()} method of the {\tt OpenMCExecutioner}, which reads OpenMC input files and performs internal actions such as setting initial values that are defined in the OpenMC input files.
%
\item Execute all transfers on {\tt timestep\_begin} with direction {\tt to\_multiapp}.
	\begin{enumerate}
	\item Transfer heat flux boundary condition coefficients from Okapi to MOON. For the very first time step none of the Apps have been run yet, so an initial condition must be specified on the heat flux boundary condition in the master input file.
	\end{enumerate}
\item Solve Nek.
\item Execute all transfers on {\tt timestep\_begin} with direction {\tt from\_multiapp}.
	\begin{enumerate}
	\item Transfer temperature boundary condition coefficients from MOON to Okapi. For the very first Nek step, the temperature boundary coefficients are all computed to be zero (this does not impact Okapi because it doesn't use the temperature boundary conditions).
	\item Transfer axially-binned values for fluid density and temperature from Nek to Okapi (not yet implemented).
	\end{enumerate}
\item Solve Okapi.
\item Execute all transfers on {\tt timestep\_end} with direction {\tt to\_multiapp}.
	\begin{enumerate}
	\item Transfer kappa fission distribution coefficients from Okapi to BISON.
	\item Transfer temperature boundary condition coefficients from Okapi to BISON (for the very first time step, these coefficients are all zero because Nek computes them as so).
	\end{enumerate}
\item Solve BISON.
\item Execute all transfers on {\tt timestep\_end} with direction {\tt from\_multiapp}.
	\begin{enumerate}
	\item Transfer fuel temperature coefficients from BISON to Okapi.
	\item Transfer fuel density coefficients from BISON to Okapi (not yet implemented).
	\end{enumerate}
\end{enumerate}

And for BISON executing first, the sequence of events are similar, except that steps 5-7 dealing with MOON execution and transfer are swapped with steps 9-11 dealing with BISON execution and transfer. With MOON executing before BISON, the following figure shows all of the steps taken for the very first Picard step, which shows where the initial conditions come from and how variables in each of the three codes interact. For all of the subsequent time steps, the ``initial conditions'' simply become the values computed from the last time step. Note that the data transfer for fuel density and fluid density and temperature are not shown because they are not yet implemented.

\begin{figure}[H]
\centering
\includegraphics[width=17.5cm]{figures/OpenMC-BISON-Nek-step1.png}
\caption{Sequence of events and variables used to store coupling data for the three-way OpenMC, BISON, and Nek coupling. The fonts used for different variables are shown in the box in the lower right.}
\end{figure}

\clearpage
\section{Initial Conditions}
\label{sec:ICs}
This section discusses how initial conditions are applied to OpenMC, Nek, and BISON for a coupled solve. There are two generally types of initializations that are performed for a single physics code:

\begin{itemize}
\item General initializations that specify the geometry, materials, and problem parameters that will remain unchanged throughout a simulation
\item Initial values for solution variables that are needed to begin the finite difference time marching
\end{itemize}

It becomes increasingly difficult as the number of coupled codes increases to ensure consistency between all of the initial conditions and system specifications. To attempt to achieve consistency, all initial conditions should be set from MOOSE. However, if desired, these MOOSE initial conditions can be omitted, and initial conditions set in the usual manner in the code's input files (OpenMC's XML files or Nek's {\tt rea} and {\tt usr} files). 

Either MOON or Buffalo may execute first, so the required initial conditions depends on this choice. Note that the requirements to set boundary conditions also depends on the extent to which the transfer classes have been completed - if the capability to transfer fluid density has not yet been implemented in Nek, for example, then an initial condition must be specified on the fluid density for OpenMC. 

\subsection{MOON Before BISON}
When MOON runs before BISON, initial conditions must be specified for fields that are inputs to MOON and for those fields that are inputs to Okapi that would have been determined by BISON. This section discusses how to set these initial conditions. These fields that must have initial conditions are:

\begin{itemize}
\item Heat flux boundary condition (BISON does not run before Nek)
\item Fluid-specific boundary conditions that are required by Nek, such as fluid inlet temperature/density, pressure, etc. These type of boundary conditions vary depending on the problem specifics.
\item Fuel density (BISON does not run before OpenMC)
\item Fuel temperature (BISON does not run before OpenMC)
\end{itemize}

\subsubsection{Fuel Wall Heat Flux and Temperature}
Because BISON does not run before MOON for the very first time step, an initial condition must be set on the fuel wall heat flux. Because this is a MOOSE variable (represented as a scalar auxvariable), it is most sensible to define this parameter directly in the master MOOSE input file. The fuel wall heat flux is directly related to the fuel temperature, so if the user is going to specify the wall heat flux, it would be good to ensure consistency with the initial condition specified on fuel temperature. 

The initial condition on fuel temperature is required because BISON does not run before Okapi for the very first time step. An initial condition on fuel temperature can be set either in OpenMC XML input files or through an initial condition class in the MOOSE master input file. Because it is logical to specify the wall heat flux initial condition directly in the master input file, for consistency the fuel temperature initial condition is also specified in the master input file. 

For simple cases, all temperature initial conditions needed by OpenMC can be specified in the OpenMC XML files, and the heat flux BC determined by hand. 

\subsection{BISON Before MOON}
When BISON runs before MOON, initial conditions must be specified for fields that are inputs to BISON and for those fields that are inputs to Okapi that would have been determined by Nek. This section discusses how to set these initial conditions. These fields that must have initial conditions are:

\begin{itemize}
\item Kappa-fission distribution (OpenMC does not run before BISON)
\item Temperature boundary condition (Nek does not run before BISON)
\item Fluid density (Nek does not run before OpenMC)
\item Fluid temperature (Nek does not run before OpenMC)
\end{itemize}

\clearpage
\section{Data Transfer}
This section first discusses the general requirements needed to transfer information between two physics codes. Then, subsections focusing on MOOSE, OpenMC, and Nek will individually describe the data transfer capabilities introduced in order to transfer information between all three codes. When coupling two codes, there are two general ways in which the data can be transferred:

\begin{enumerate}
\item Force both codes to solve on the same mesh, in which case the data transfer can be accomplished by determining a functional form for the transfer variable (such as temperature) over a single computational element, and then passing this information to the second code. Assuming that the second code can utilize this information exactly, this is the easiest way to transfer information, because it requires little effort to map cells in one mesh to cells in another. However, this method is very restrictive, since it is often the case that the two physics do not require the same mesh, and forcing both physics to run on the finest mesh is unnecessarily expensive. For instance, boundary layers must be resolved for fluids computations with very fine elements that would be overkill for a Monte Carlo simulation. When one of the codes is a meshless code, such as Monte Carlo codes using Constructive Solid Geometry representations, this ``easiness'' is no longer present.
\item Run both codes on different meshes.

	\begin{enumerate}
	\item Use arbitrary polynomials to express a transfer variable, and then pass the expansion coefficients to the second code. For instance, suppose Nek is to transfer temperature on a boundary to MOOSE, which would use this information to set a boundary condition. Because this information is not being passed on an element-to-element basis, its spatial distribution must be described using some type of function - delta functions if an elementwise-constant value from Nek is to be transferred to an element (not necessarily of the same size) in MOOSE, or really any arbitrary polynomial. To transfer a variable \(u(r, \theta, z)\) to MOOSE, Nek would compute the expansion coefficients \(C_{jkl}\) that capture the behavior according to expansion functions \(R(r), \Theta(\theta), Z(z)\):
	
	\beq
	\label{eq:GenericExpansion}
	u(r, \theta, z)\approx\sum_{j=0}^J\sum_{k=0}^K\sum_{l=0}^L C_{jkl}R_j(r)\Theta_k(\theta)Z_l(z)
	\eeq
	
Note that an approximate equals sign \(\approx\) is used, since for an arbitrary selection of functions, the (potentially) infinite degrees of freedom of the value of \(u\) computed by Nek cannot be fully captured. Once Nek passes the coefficients \(C_{jkl}\) to MOOSE, MOOSE would reconstruct the functional form of the transfer variable using the same equation above, and then proceed with its simulation for that step. The issue with using arbitrary polynomials is that there is no one way to determine the expansion coefficients \(C_{jk}\). For instance, assume we use the following expansion functions:

	\beqa
	R_j(r)=&r^j\\
	\Theta_k(\theta)=&\cos(k\theta)\\
	Z_l(z)=&1+z^l\\
	\eeqa

To determine the expansion coefficients for this choice requires that we construct a Vandermonde matrix, which requires that we arbitrarily select points over the Nek element at which to sample the Nek transfer variable. For \(J=1\), \(K=0\), \(L=0\), this matrix system would take the form:

\beqa
u(r,\theta,z)=\begin{bmatrix}
r_0^0\cos{(0)}(1+z_0^0) & r_0^1\cos{(\theta_0)}(1+z_0^0)\\
r_1^0\cos{(0)}(1+z_0^0) & r_1^1\cos{(\theta_0)}(1+z_0^0)\\
\end{bmatrix}
\begin{bmatrix}
C_{000}\\C_{100}
\end{bmatrix}
\eeqa

where \(r_0\) and \(r_1\) are the two sampled \(r\)-coordinates in the element that must be selected in order to determine the expansion coefficients for a first-order \(r\)-expansion, and \(\theta_0\) and \(z_0\) the sampling points to determine the expansion coefficients for a zeroth-order \(\theta\)- and \(z\)-expansion. This can cause problems such as oscillations in the interpolation if the choices of functions are poor, and can completely miss parts of the transfer variable solution if we happen to not select points near important features. In addition, if we use a statistical code such as a Monte Carlo code, choosing these sampling points will make any scores to other regions of the problem completely ``useless,'' which will lead to very high numbers of particles required to reduce variance.

	\item Use orthogonal functions to express a transfer variable, and then pass the expansion coefficients to the second code. Using orthogonal functions is essentially the same formulation as that shown in Eq. \eqref{eq:GenericExpansion}, except that the expansion functions are chosen to be orthogonal over the domain of transfer. Then, orthogonality conditions lead to exact formulas for \(C_{jkl}\) that are {\it integral} in nature, which means that all features of the solution are captured, and no creation and inversion of a Vandermonde matrix is required. 
	\end{enumerate}
	
\end{enumerate}

For the reasons discussed above, the data transfer between OpenMC, Nek, and BISON is performed using orthogonal polynomial expansions. For any code to implement data transfers using polynomials, four capabilities are required:

\begin{enumerate}
\item Functions to compute the orthogonal polynomials given a coordinate point and expansion order.
\item Routines to integrate a continuous variable over an orthogonal domain to compute the expansion coefficients. This process will be referred to in this document as ``deconstruction,'' since a continuous field is decomposed into a finite set of expansion coefficients.
\item Routines to multiply a set of expansion coefficients by orthogonal polynomials to form a continuous field form a set of finite coefficients. This process will be referred to in this document as ``construction,'' since a continuous field is formed from a set of discrete expansion coefficients.
\item Use the constructed field in some internal capacity that defines the coupling, such as in the specification of a boundary condition or heat source.
\end{enumerate}

After a summary of orthogonal polynomials available, each of these four capabilities will be described in detail for MOOSE, OpenMC, and Nek. 

\subsection{Choices of Orthogonal Polynomials}
\label{sec:Polynomials}
This section describes several choices of orthogonal polynomials that can be used to construct and deconstruct continuous fields.

\subsubsection{Legendre Polynomials}

Legendre polynomials are orthogonal over \( -1 \leq z \leq 1\), and hence are suitable for expanding solutions along the axes of fuel pins, along one direction in a slab, or in any Cartesian dimension. The Legendre polynomials are given as:

\beq
\label{eq:LegendreScaled}
P_l(z)=\sqrt{\frac{2l+1}{2}}\frac{1}{2^ll!}\frac{d^l}{dx^l}\left\lbrack\left(z^2-1)^l\right)\right\rbrack
\eeq

which are orthogonal over \(-1\) to \(+1\):

\beq
\label{eq:LegendreScaledOrthogonality}
\int_{-1}^{+1}P_l(z)P_{l'}(z)dz=\delta_{ll'}
\eeq

\subsubsection{Fourier Polynomials}

Fourier polynomials are orthogonal over \(-\pi\leq\theta\leq\pi\), and hence are suitable for expanding along the \(\theta\) direction of a cylinder. The Fourier functions are given as:

\beq
\label{eq:FourierScaled}
F_k(\theta)=
\begin{cases}
\frac{1}{\sqrt{\pi}}\cos{(k\theta)} & k \neq 0\\
\frac{1}{\sqrt{2\pi}}\cos{(k\theta)} & k = 0\\
\end{cases}
\eeq

which are orthogonal over \(-\pi\) to \(+\pi\):

\beqa
\label{eq:FourierOrthogonal}
\int_{-\pi}^{+\pi}F_k(\theta)F_{k'}(\theta)d\theta=\delta_{kk'}
\eeqa

\subsubsection{Zernike Polynomials}

Zernike polynomials are orthogonal over the unit circle, and hence are suitable for expanding in the plane of a cylinder. The Zernike polynomials are given as:

\beq
\label{eq:ZernikeScaled}
Z_n^m(r,\theta)=
\begin{cases}
\sqrt{\frac{2(n+1)}{\pi\left(1+\delta_{m0}\right)}}R_n^{|m|}(r)\cos{(m\theta)} & m\geq 0\\
-\sqrt{\frac{2(n+1)}{\pi\left(1+\delta_{m0}\right)}}R_n^{|m|}(r)\sin{(m\theta)} & m < 0\\
\end{cases}
\eeq

\beq
R_n^{|m|}(r)=\sum_{s=0}^{\frac{n-|m|}{2}}\frac{(-1)^s(n-s)!}{s!\left(\frac{n+|m|}{2}-s\right)!\left(\frac{n-|m|}{2}-s\right)!}r^{n-2s}
\eeq

Note that \(m\) can only obtain values \(-n, -n+2, -n+4, \cdots, n\). The orthogonality condition is:

\beq
\label{eq:ZernikeOrthogonal}
\int_{0}^{1}rdr\int_{0}^{2\pi}d\theta Z_{n}^m(r,\theta)Z_{n'}^{m'}=\delta_{n,n'}\delta_{m,m'}
\eeq

\subsection{Deconstructing a Continuous Variable}
This section describes how a continuous field \(u(r,\theta,z)\) can be deconstructed into a set of expansion coefficients using orthogonal polynomials. The procedure is the same regardless of the orthogonal domain (i.e. if performing a boundary or volume coupling), but will be illustrated for both a boundary coupling over the surface of a cylinder and for a volume coupling over a cylindrical volume. For a boundary coupling over the surface of a cylinder, a function \(u(\theta,z)\) can be expressed as:

\beq
\label{eq:OrthogonalExpansion}
u(\theta, z)\approx\sum_{k=0}^K\sum_{l=0}^LC_{kl}F_k(\theta)P_l(z)
\eeq

where \(F_k(\theta)\) is the \(k\)-th order Fourier function and \(P_l(z)\) is the \(l\)-th order Legendre polynomial. The expansion coefficients can be determined by multiplying both sides of Eq. \eqref{eq:OrthogonalExpansion} by orthogonal polynomials of different order, and applying the orthogonality conditions given in Section \ref{sec:Polynomials}:

\beq
\label{eq:ScaledExpansionCoeff}
C_{kl}=\int_{-1}^{+1}dz\int_{-\pi}^{+\pi}d\theta u(\theta, z)F_k(\theta)P_l(z)
\eeq

Likewise, for a volume coupling over a cylinder, a function \(u(r,\theta,z)\) can be expressed as:

\beq
\label{eq:ZLReconstruction}
u(r,\theta,z)=\sum_{l=0}^{L}\sum_{n=0}^N\sum_{m=-n}^{n}C_{l}^{nm}P_l(z)Z_n^m(r,\theta)
\eeq

Again, the coefficients can be determined by multiplying by orthogonal polynomials of different order, and applying orthogonality conditions:

\beq
\label{eq:ZernikeLegendreCoefficient}
C_l^{nm}=\int_0^1rdr\int_{0}^{2\pi}d\theta\int_{-1}^{1}dzu(r,\theta,z)P_l(z)Z_n^m(r,\theta)
\eeq

Hence, all that is required to deconstruct a continuous field into a set of expansion coefficients is to perform repeated integrations over the orthogonal domain.

\subsection{Constructing a Continuous Variable}
As opposed to deconstruction, constructing a continuous variable from a set of expansion coefficients is more straightforward, since all that is required is repeated evaluation of the orthogonal polynomials, multiplied by expansion coefficients.

\subsection{MOOSE Data Transfer}
This section describes the capabilities introduced into the MOOSE source code that allows evaluation of polynomials, and construction/deconstruction of continuous fields. These functions are placed in a local branch of MOOSE so that BISON, Okapi, and MOON can all access these functions. The remote version of this special coupling version of MOOSE is located at {\tt git@github.com:aprilnovak/coupling-moose.git}.

\subsubsection{Evaluation of Polynomials}
This section discusses the generic functions added to MOOSE to compute orthogonal polynomials. MOON previously had several of these functions in its own source - to avoid duplication when other MOOSE animals require the access to the same functions, the {\tt LegendrePolynomial} and {\tt FourierPolynomial} functions were moved from MOON to MOOSE.

\color{magenta}
No checks are made that any of the normalizing parameters specified as input for these functions match those in external coupled codes.
\color{black}

\subsubsubsection{{\tt LegendrePolynomial}}
This function computes the Legendre polynomial in Eq. \eqref{eq:LegendreScaled} for a given value of \(l\) and a specific position. Because Legendre polynomials are only orthogonal on \([-1, +1]\), a scaling factor \(z\) is used to simply stretch or compress the physical domain to the orthogonal domain. The {\tt l\_geom\_norm} parameter provides the minimum and maximum distances over which the Legendre polynomials will be forced to be orthogonal using a scaling factor. 

\subsubsubsection{{\tt FourierPolynomial}}
This function computes the Fourier function given in Eq. \eqref{eq:FourierScaled} for a given value of \(k\) and a specific position. The \(\theta\) value at which to evaluate this function is determined as the angle between the two provided points. The user must provide the center of the circle for correct normalization. Because the fuel rod of interest for coupling exists on \([-\pi, +\pi]\), no additional scaling is needed.

\subsubsubsection{{\tt ZernikePolynomial}}
This function computes a Zernike polynomial given in Eq. \eqref{eq:ZernikeScaled} given orders \(n\) and \(m\). 

\subsubsection{Field Deconstruction}
This section describes how a continuous field is deconstructed into a set of expansion coefficients in MOOSE.

\subsubsubsection{{\tt FourierLegendreDeconstruction}}
This user object converts a continuous variable into the set of expansion coefficients that would be obtained by expanding the variable in Fourier and Legendre polynomials as shown in Eq. \eqref{eq:ScaledExpansionCoeff}. This user object only computes {\it one} \(C_{kl}\) coefficient at a time, so the {\tt FLDeconstruction} user object should be used instead, since it computes all the Legendre coefficients given a single Fourier order. Because the orthogonal domain likely differs from the physical domain, the actual integral performed is shown below:

\beq
C_{kl}=\frac{2}{RH}\int_{-1}^{+1}dz\int_{-\pi}^{+\pi}u(\theta, z)F_k(\theta)P_l(z)
\eeq

If both the numerator and denominator are multiplied by \(2\pi\), the denominator can be calculated using a surface area postprocessor. Note that this user object assumes that the coupled scalar auxvariable is organized by Fourier coefficient (each scalar auxvariable holds all the Fourier coefficients for a given Legendre order). 

Note that this user object {\it either} deconstructs a {\it scalar} variable \(u(\theta, z)\), {\it or} a vector-valued variable dotted with the unit normals, multiplied by a coefficient, \(-C\nabla u\cdot\hat{n}\) (since for coupling it is often desired to integrate a flux as well). The choice between these options is indicated with the {\tt flux\_integral} boolean parameter.

\color{magenta}
This user object will give inaccurate results if the integration is not performed over a cylindrical side surface, but no checks are made that the surface area matches the height and radius specified in the Fourier and Legendre polynomials.
\color{black}

\subsubsubsection{{\tt FLDeconstruction}}
This user object computes {\it all} of the Legendre coefficients given a fixed Fourier order, reducing the number of user objects required in the input file by a factor of the number of Legendre expansion coefficients. The contents of this user object are identical to that of the {\tt FourierLegendreDeconstruction} user object, except that loops are used to update an array of values instead of a singular value.

\color{magenta}
This user object will give inaccurate results if the integration is not performed over a cylindrical side surface, but no checks are made that the surface area matches the height and radius specified in the Fourier and Legendre polynomials.
\color{black}

\subsubsubsection{{\tt ZernikeLegendreDeconstruction}}
This user object converts a continuous variable into the set of expansion coefficients that would be obtained by expanding the variable in Zernike and Legendre polynomials as shown in Eq. \eqref{eq:ZernikeLegendreCoefficient}. This user object only computes {\it one} \(C_l^{nm}\) at a time, so the {\tt ZLDeconstruction} user object should be used instead, since it computes all of the Zernike coefficients given a single Legendre order. Because the orthogonal domain likely differs from the physical domain, the actual integral performed is shown below:

\beq
C_l^{nm}=\frac{1}{R^2}\frac{2}{H}\int_0^1rdr\int_{0}^{2\pi}d\theta\int_{-1}^{1}dzu(r,\theta,z)P_l(z)Z_n^m(r,\theta)
\eeq

where \(R\) is the radius and \(H\) the height in the physical domain. The factor of 2 appears because the height in the orthogonal domain is 2. By multiplying the factor above by \(\pi/\pi\), the denominator becomes the volume of the region. Hence, a postprocessor value provides the volume instead of relying on input from the user. This object can be restricted to blocks using the {\tt blocks} parameter in the input file. If the block is not specified, then if a variable parameter is present, the blocks are set to all of the blocks that are associated with that variable (i.e. for most simulations, the entire domain). If no variable is specified, then by default the entire domain is used. 

\color{magenta}
No checks are made to ensure that the integration is performed over a cylindrical domain. Results will be inaccurate if the domain is not cylindrical or if the provided functions are not given the same normalization quantities that are implicit in the provided domain. This could be improved by adding a check that the volume computed by the postprocessor matches the volume as-computed by the coupled Legendre and Zernike functions.
\color{black}

\subsubsubsection{{\tt ZLDeconstruction}}
This user object computes {\it all} of the Zernike coefficients given a fixed Legendre order, reducing the number of user objects required in the input file by a factor of the number of Zernike expansion coefficients. The contents of this user object are identical to that of the {\tt ZernikeLegendreDeconstruction} user object, except that loops are used to update an array of values instead of a singular value.

\color{magenta}
No checks are made to ensure that the integration is performed over a cylindrical domain. Results will be inaccurate if the domain is not cylindrical or if the provided functions are not given the same normalization quantities that are implicit in the provided domain. This could be improved by adding a check that the volume computed by the postprocessor matches the volume as-computed by the coupled Legendre and Zernike functions.
\color{black}

\subsubsection{Field Reconstruction}
This section discusses the functions developed in MOOSE to reconstruct a continuous field given expansion coefficients and specified orthogonal functions.

\subsubsubsection{{\tt FourierLegendreReconstruction}}
This function reconstructs a solution as a finite series of Fourier and Legendre polynomials given expansion coefficients, and evaluates it at a single point according to Eq. \eqref{eq:OrthogonalExpansion}. This function is set up so that the {\tt poly\_coeffs} coupled to this function {\it each} hold all of the expansion coefficients for a {\it fixed} Fourier order \(f\). For instance, for a Legendre expansion of 4 and a Fourier expansion of 2, these coefficients hold:

\begin{lstlisting}
f_0_coeffs = 'C00 C01 C02 C03 C04 C05'
f_1_coeffs = 'C10 C11 C12 C13 C14 C15'
f_2_coeffs = 'C20 C21 C22 C23 C24 C25'
\end{lstlisting}

where the first index represents the \(f\) order and the second the \(n\) order. Note that the choice in MOON was for each of these aux variables to hold all of the Legendre coefficients for a given Fourier order.

\subsubsubsection{{\tt ZernikeLegendreReconstruction}}
This function reconstructs a solution as a finite series of Zernike and Legendre polynomials given expansion coefficients, and evaluates it at a single point according to Eq. \eqref{eq:ZLReconstruction}. This function is very similar to the Fourier-Legendre reconstruction function developed for MOON, except that it also includes the flexibility for the Legendre expansion direction to not be along the \(z\)-direction. This function is set up so that the {\tt poly\_coeffs} coupled to this function {\it each} hold all of the expansion coefficients for a {\it fixed} Legendre order \(l\). For instance, for a Legendre expansion of 3 and a Zernike expansion of 2, these coefficients hold:

\begin{lstlisting}
l_0_coeffs = 'C000 C01n1 C011 C02n2 C020 C022'
l_1_coeffs = 'C100 C11n1 C111 C12n2 C120 C122'
l_2_coeffs = 'C200 C21n1 C211 C22n2 C2020 C222'
l_3_coeffs = 'C300 C31n1 C311 C32n2 C320 C322'
\end{lstlisting}

where the first index represents the \(l\) order, the second the \(n\) order, and the third the \(m\) order, where {\tt n} represents a negative number. Note that the choice in MOON was for each of these aux variables to hold all of the Legendre coefficients for a given Fourier order, which is the opposite of the choice here. 

\subsubsection{Using the Constructed Field}
This section discusses the capabilities added to MOOSE to use the constructed field to enact physical feedback with other physics.

\subsubsubsection{{\tt KappaFissionToHeatSource}}
OpenMC provides a set of kappa fission expansion coefficients to MOOSE, which then passes those coefficients to MOOSE. In order to use those coefficients, they must be converted to a heat source with units of W/volume (the FE mesh is units-agnostic). The {\tt ZernikeLegendreReconstruction} is used to expand these coefficients into a continuous distribution, and then this auxkernel converts the continuous tally distribution to a heat source. There are two general methods by which to do this, both of which assume the user has knowledge of the power of the expansion domain. The power of the reactor is the sum of the power produced by fission directly in the fuel rod and the power produced by gamma heating (fuel, structural materials, and coolant). The expansion coefficients record the recoverable energy produced from {\it fission}, and hence will only have nonzero values over the fuel. So, assuming the user knows the power produced in each domain over which expansion coefficients are defined (a fuel pin, for example), the heat source can be determined by: 

\begin{itemize}
\item Multiplying the kappa fission distribution \(\kappa(\bf{r})\) (eV/source particle) by the user-specified pin fission power, divided by the integrated kappa fission distribution (over the pin):

\beq
\textrm{fission power}=\kappa(\bf{r})\times\frac{\textrm{user-defined pin fission power}}{\int d\bf{r}\kappa({\bf r})}
\eeq

\item Assuming an average energy released per fission \(E_f\) and an average number of source particles born from fission \(\nu\), the fission power may be computed as:

\beq
\textrm{fission power}=\kappa(\bf{r})\times\textrm{user-defined pin fission power}\times\frac{\nu}{E_f}
\eeq
\end{itemize}

The first approach is more desirable and general because it requires less specifications than the second method (i.e. no assumptions are needed for values for \(\nu\) and \(E_f\)) and integration over a volume can easily be performed with a MOOSE postprocessor. However, for subtle reasons, the second method must be selected unless more changes are made to the MOOSE framework. After receiving expansion coefficients for the expansion coefficients from Okapi, BISON converts those just-received coefficients into a continuous distribution using the {\tt ZernikeLegendreReconstruction} function {\it simultaneously} with the application of the functional distribution to the continuous kappa-fission distribution auxiliary variable. The second method above requires that the auxiliary variable be fully constructed before integrating it, but this integrated value is required itself in the auxkernel that defines the auxvariable. This {\it could} be circumvented by setting an initial condition for the kappa-fisison coefficients in the BISON input file, which would be used to provide the postprocessor value for the first time step; all further time steps would lag one step in the value of the integrated kappa fission distribution. For early iterations, this lagging is undesirable, since there may be large changes in the kappa fission coefficients from one iteration to the next. Alternatively, if the second method is chosen, then the postprocessor is not required at all, at the expense of slight inaccuracies in the assumption of constant \(\nu\) and \(E_f\) (though better values of these could be obtained from OpenMC?).

\color{magenta}
How are changes in power accounted for?
\color{black}

\subsubsection{Performing the Data Transfer}
This section describes the MOOSE transfer classes developed to transfer data between Okapi and BISON and Okapi and MOON; because Okapi is the Master App, BISON and MOON do not communicate with each other directly. The OpenMC and Nek routines that are called from within the MOOSE transfer classes developed are discussed in Sections \ref{sec:OpenMCTransfer} and \ref{sec:NekTransfer}, respectively. The convention for naming these transfer classes is the name of the sub App followed by the name of the Master App (the {\tt MultiAppOkapiMooseTransfer} class was developed with Okapi as the sub App to a MOOSE animal, before the decision was made to use Okapi as the Master App).

\subsubsubsection{{\tt MultiAppOkapiMooseTransfer}}
This transfer object defines data transfer from MOOSE to OpenMC (to MultiApp), and from OpenMC back to MOOSE (from MultiApp). In order to be flexible such that the {\tt MultiAppOkapiMooseTransfer} transfer can be used for both transfer to OpenMC and from OpenMC, the {\tt to\_aux\_scalar} parameter will be a dummy parameter for transfer to OpenMC (because OpenMC does not perform any finite element solve in the MOOSE context), while {\tt source\_variable} will be a dummy variable for transfer from OpenMC for the same reason.
\\\\
{\it Transfer to OpenMC}\\
For transferring data to OpenMC, the associated syntax is shown below. The source variable holds a list of the scalar auxvariables that hold all of the expansion coefficients for a given Legendre order for the temperature in MOOSE (Legendre order of two shown here). These must be present in ascending Legendre order so that OpenMC interprets these coefficients correctly.  Finally, the OpenMC subroutines that are called from {\tt MultiAppOkapiMooseTransfer} methods require the OpenMC cell index in the {\tt cells(:)} array so that we know which cells to change the temperature of. 

\color{magenta}
This transfer only works for a single coupling cell, since we won't duplicate the transfer block shown below for every single fuel pin. We'll need to write some method to internally link the OpenMC cells with the MOOSE block IDs. 

A routine needs to be developed to receive the expansion coefficients in OpenMC, and store them as a function of the OpenMC cell to which they pertain. This routine will need to not only read the coefficients, but also determine the Zernike and Legendre expansion orders (since they need not be the same orders as those used to expand the kappa fission tally).

Finally, continuous material tracking needs to be implemented in OpenMC to make actual use of these coefficients.
\color{black}

\begin{lstlisting}
[Transfers]
  [./to_openmc]
      type = MultiAppOkapiMooseTransfer
      direction = to_multiapp
      source_variable = 'l_0_coeffs l_1_coeffs l_2_coeffs'
      to_aux_scalar = 'dummy'
      openmc_cell = 1
  [../]
[]
\end{lstlisting}

A check is performed to ensure that the order of each of the {\tt source\_variable}s is the same, since each should represent all of the Zernike coefficients given a fixed Legendre order. After OpenMC receives these coefficients, without continuous material tracking implemented, for simplicity and to illustrate the data flow for coupling, the {\tt openmc\_cell\_set\_temperature} subroutine is called and passed the OpenMC cell index and the first expansion coefficient (essentially performing a zero-th order expansion for the temperature). Note that a scaling factor is applied because the \(C_0^{00}\) coefficient has to be multiplied by \(P_0(z)Z_0^0(r,\theta)\). Note that to change a temperature in OpenMC requires that you have loaded cross sections at that temperature; a range of temperatures can be loaded during the initialization step by specifying the {\tt temperature\_range} parameter in the settings XML file.
\\\\
{\it Transfer from OpenMC}\\
For transferring data from OpenMC to MOOSE, the {\tt fet\_deconstruction} OpenMC subroutine is first called in order to collect all of the expansion coefficients into the {\tt coeffs(:, :)} array. This subroutine sums over all other filters except the cell filter. For instance, if the user specifies multiple material filters, the expansion coefficients are summed over all of those materials, simply returning coefficients by cell. The auxiliary scalar variables that are to be written represent the expansion coefficients in MOOSE that hold the kappa fission tally distribution. Finally, we must specify which OpenMC cell is acting as the source of the coefficients. Then, we check that all of the {\tt to\_aux\_scalar} variables are of the same length. In order to extract the expansion coefficients from OpenMC, the {\tt get\_coeffs\_from\_cell} OpenMC subroutine stores the expansion coefficients for a specified cell in a C++ array. The size of this C++ array is assumed to be the total length of the auxiliary scalar variables specified in the {\tt to\_aux\_scalar} field (which have already been checked to be the same length). In the {\tt get\_coeffs\_from\_cell} subroutine, a check is made that the C++ array allocated to hold the expansion coefficients is of the appropriate length, so between the equal-length check made in MOOSE, and the total-length check made in OpenMC, there is no possibility of incorrect sizes in this transfer.

\begin{lstlisting}
[Transfers]
  [./to_openmc]
      type = MultiAppOkapiMooseTransfer
      direction = from_multiapp
      source_variable = 'dummy'
      to_aux_scalar = 'l_0_coeffs l_1_coeffs l_2_coeffs'
      openmc_cell = 1
  [../]
[]
\end{lstlisting}

\color{magenta}
Like the transfer to OpenMC, this transfer only works for a single fuel pin, and in the future we'll need to internally specify which OpenMC cells map to MOOSE blocks.
\color{black}

\subsubsubsection{{\tt MultiAppMooseOkapiTransfer}}
This transfer class is essentially the same as the {\tt MultiAppOkapiMooseTransfer} class, except that the syntax for ``finding'' a MOOSE variable in the problem base is slightly different when Okapi is the Master App. 

\subsubsubsection{{\tt MultiAppMoonOkapiTransfer}}
This transfer object defines what routines are called to transfer data from Okapi to Nek, and from Nek back to Okapi. In order to be flexible such that the transfer can be used for both transfer to and from Nek, the {\tt to\_aux\_scalar} parameter will be a dummy parameter for transfer to Nek (because Nek does not perform any finite element solve in the MOOSE context), while {\tt source\_variable} will be a dummy variable for transfer from Nek for the same reason.
\\\\
{\it Transfer to Nek}\\
When transferring data from Okapi to Nek, the expansion coefficients are stored in the {\tt coeff\_fij} array that is the only member of the {\tt expansion\_fcoef\_} struct. {\tt expansion\_fcoef\_} (note the trailing underscore) represents the direct mapping of a Fortran common block to a C++ struct. After writing to the flux expansion coefficient array, the {\tt flux\_reconstruction} subroutine is called, which reconstructs a continuous heat flux surface boundary condition.

{\it Transfer from Nek}\\
When transferring data from Nek to Okapi, the expansion coefficients from the {\tt coeff\_tij} array that is the only member of the {\tt expansion\_tcoef} common block are read and stored in an auxvariable in Okapi. This auxvariable serves as a ``middle-man'' for intermediate storage before being sent to BISON, where the information is eventually used. In addition to transferring expansion coefficients for a surface temperature boundary condition, the {\tt axially\_binned\_integration} Nek subroutine is called to integrate the fluid temperature and store axially-binned averages in the {\tt fluid\_temp\_bins} array. No subroutines in OpenMC have been developed to utilize axially-binned averages of temperature.

\subsection{OpenMC Data Transfer}
\label{sec:OpenMCTransfer}
This section describes the capabilities introduced into the OpenMC source code that allow evaluation of polynomials, construction/deconstruction of continuous fields, and the use of this transferred data to enact physical changes, such as changing temperatures and densities. The information in this section is based upon the {\tt zernike} branch provided by Matt Ellis, and corrections and extensions that were made are indicated where applicable.

\subsubsection{Evaluation of Polynomials}
OpenMC contains several functions in the {\tt math} module to compute polynomial values given a coordinate and order and in the {\tt tally} module to compute normalized positions.

\subsubsubsection{{\tt calc\_pn(n, x)}}
\label{sec:calc_pn}
This elemental function computes a Legendre polynomial given an order \(n\) and a coordinate \(x\), where this \(x\)-coordinate must be within the valid range \(-1\leq x\leq1\). For performance, this function hard-codes the Legendre polynomials (does not use a recursive formulation) up to order 10. Above order 10, this function returns 1, which will obviously be incorrect. No checks are made to ensure that the order or coordinate are within the valid ranges. Note that this function does not compute the {\it scaled} Legendre polynomials that are needed for neat computation of expansion coefficients for an orthogonal expansion (they should be multiplied by \(\sqrt{(2l+1)/2}\)). 

\subsubsubsection{{\tt calc\_pn\_scaled(n, x)}}
This elemental function computes the scaled Legendre polynomials that are needed for correctly computing FET expansion coefficients (without needing to worry about carrying the \(\sqrt{(2l+1)/2}\) coefficient around). This function simply calls the {\tt calc\_pn} subroutine, and hence suffers the same limitations as those discussed in Section \ref{sec:calc_pn} with regards to order and position checking. 

\subsubsubsection{{\tt calc\_zn(n, rho, phi)}}
\label{sec:calc_zn}
This function computes the Zernike polynomial for an order \(n\), radial coordinate \(\rho\), and angular coordinate \(\phi\) assuming that these coordinates have already been correctly scaled to the unit circle. No checks are made to ensure that these coordinates are within the valid ranges. For a given value of \(n\), the output of this function is an array \(zn\) of length \(n+1\), i.e. the output array holds all of the \(n=\) fixed, \(m\) values for the given \(n\). Similar to {\tt calc\_pn}, this function does not correctly scale the Zernike polynomials - they should all be divided by a factor of \(\sqrt{\pi}\) to ensure that integration over the orthogonal domain directly gives the expansion coefficients, without other factors trailing around. The original {\tt zernike} branch incorrectly computed Zernike expansion coefficients using this function, where each coefficient would be off by a factor of \(\sqrt{\pi}\) (and hence the fission power as well).

\subsubsubsection{{\tt calc\_zn\_scaled(n, rho, phi)}}
This function computes the scaled Zernike polynomials that are needed for correctly computing FET expansion coefficients (without needing to worry about carrying the \(1/\sqrt{\pi}\) coefficient around). This function simply calls the {\tt calc\_zn} subroutine, and hence suffers the same limitations as those discussed in Section \ref{sec:calc_zn} with regards to order and position checking.

\subsubsubsection{{\tt get\_polynomial\_norm\_positions(...)}}
This subroutine obtains normalized coordinate positions to be used as input to the functions discussed above. This function computes normalized \(r, \theta, z\) coordinates, where the \(r, \theta\) coordinates are normalized to the unit circle and the \(z\) coordinate to the \([-1, 1]\) orthogonal domain for Legendre polynomials. The type of score passed into this routine determines which type of normalization is performed. This subroutine was extended from Matt's implementation by back-determining certain geometric information so that the user does not need to redundantly specify it in the input file. In addition, it was previously implicitly assumed that the geometry were centered on \(0, 0\) - this has been made more general. 

A particle is input to this subroutine, and the {\tt region} array for the particle's cell looped over until finding the surfaces that are \(z\)-cylinders and \(z\)-planes. From these surfaces, the cylinder center, radius, and axial distance between the \(z\)-planes is determined. Note that because only \(z\)-cylinders and surfaces are investigated, this limits the FETs to being defined in the \(z\)-direction. {\bf Additional improvements here would make this more general so that the expansion could be performed in any direction, and for any type of expansion tally (calculating normalized coordinates over a cylinder assumes we're using Zernike, Legendre, or Zernike-Legendre expansions), such as Legender-Legendre expansions in 2D.}

\subsubsection{Field Deconstruction}
OpenMC deconstructs a statistical distribution of fission events into expansion coefficients for the fission source. Tallies are used to compute these expansion coefficients. A tally \(X\), at its most fundamental level, can be described almost entirely by its score and filter(s). A filter represents the region of phase space over which to tally, and the score the ``what'' to tally. 

\beq
X=\underbrace{\int dE\int d\bf{r}\int d\hat{\Omega}\int dt}_{\textrm{filters}} \underbrace{f(E,\bf{r},\hat{\Omega},t)}_{\textrm{score}}\psi(E,\bf{r},\hat{\Omega},t)
\eeq

For coupling to an external code, a Monte Carlo code computes a fission power distribution (W/volume) in the fuel that is used by that external code to perform a simulation to determine an updated temperature. Deconstructing the fission power distribution into a set of expansion coefficients requires using tallies to compute the expansion coefficients. OpenMC already has the {\tt kappa-fission} tally, which compute the recoverable energy production due to fission. This energy includes the fission product kinetic energy, prompt and delayed neutron kinetic energies, prompt and delayed gamma ray energies, and the total energy released from \(\beta\) particles. Neutrino energies are assumed to not deposit in the system. The prompt and delayed gamma ray energy is assumed to deposit locally, so this tally would not provide any estimate of the fission energy deposition (by photons) directly in the coolant, since no fission will occur directly in the coolant (and OpenMC does not have photon transport capabilities, anyways). The units for this tally are eV per source particle, and with proper transformations, this can be converted to a fission power density. Note that the {\tt kappa-fission} tally does not use energy-dependent \(Q\)-values, but does use nuclide-dependent values (the {\tt fission-q-recoverable} tally uses energy-dependent \(Q\)-values, which requires a cross section library with this information). The {\tt kappa-fission} tally computes the score \(X\) for analog tallies as:

\beq
X=\sum_{i}w_iQ_i\frac{\sigma_{f,i}}{\sigma_{a,i}}\phi
\eeq

where if there is no survival biasing, then we need to skip scoring to this tally for all absorption events (no absorption events occur with survival biasing). For non-analog tallies:

\beq
X=\sum_{i}Q_i\sigma_{f,i}N_i\phi
\eeq

Matt Ellis implemented an analog Zernike FET under the name {\tt kappa-fission-zn} to perform 2-D functional expansion. The effort to initialize, score, and expand FETs builds upon his work. The remainder of this section discusses the changes made to the OpenMC source in order to compute FETs, how the coefficients are stored, and additional subroutines and functions developed to interrogate the expansions.

% The kappa-fission-zn tally is set to analog in the read_tallies_xml. If the user specifies a different estimator in the XML file, it will not be used, since presumably if the estimator was hard-coded to analog, then the developer knew that the tally required post-collision information, and could not be changed.

% Collision and track-length estimators cannot be used for tallies that need post-collision information.

%The {\tt MAX\_ANG\_ORDER} was increased from 10 to 18.

\begin{comment}
Before discussing how the tallies XML file is used to populate OpenMC tally data structures, it is important to understand the meaning of the fields in the tally data structures. 

\begin{itemize}
\item All user-specified tallies are stored in the {\tt tallies} array, which is of type {\tt TallyObject}.
\item All filters are specified in the {\tt filters} array, which is of type {\tt TallyFilterContainer}. Each element of the {\tt filters} array is of type {\tt TallyFilter}, which is an abstract type that is associated with an ID and number of bins. This abstract class is overridden based on the type of filter to the more specific type, such as {\tt CellFilter}. This is done in a case-select statement based on the type of filter specified in the input file, and is required because the different types of filters have different data structure requirements.
\end{itemize}

\begin{itemize}
\item The {\tt filter\_matches} array, of length equal to the number of filters defined ({\tt n\_filters}), holds all of the valid bins and weights for each filter.
\end{itemize}
\end{comment}

The tally system in OpenMC is modular, and uses separate arrays to hold the tallies and the filters, with arrays in the tally objects that store information about which filters in the filter array are used with the tally. Scores are stored for the tally as the score bins, and are a multiplication of the number of bins in all of the filters specified for a tally. For instance, if a cell filter with two cells and an energy filter with three energy bins are specified, then the tally will have six score bins. Instead of implementing nuclides as a type of filter, they are implemented as a score bin. To implement a new score, no changes need to be made to the filters. Due to the structure of the {\tt tallies(i) \% find\_filter} array, no more than one filter of each type can be specified for a tally. For FETs, the only type of filter that we require is a cell filter, since this tally is inherently tied to a volume. 

When using scores for FETs, the only changes that need to be made are in the interpretation of the number of score bins and the the specific formulas for the score and how to expand it. The type of score is indicated with a numeric constant. The addition of a FET requires modification to the OpenMC tally data structures to add arrays to hold the expansion coefficients. Because this type of tally will always be associated with expansion coefficients, the storage of these expansion coefficients is linked to the tally object itself. For a single tally, because duplicate cells of the geometry cannot be specified in the cell filter (of which only one is allowed to be specified in the input file), the coefficients can be stored by cell index in the bins array of the cell filter. The {\tt TallyObject} data structure was modified to include two new arrays - {\tt coeffs(:, :)} and {\tt received\_coeffs(:, :)}, where the first of these arrays stores expansion coefficients computed {\it by the tally}, and the second stores expansion coefficients {\it received from an external physics code} that correspond to the same cells that the tally scores (as defined in the XML input). These arrays are only allocated if the tally score is of FET type. Note that this decision to store both the computed and received expansion coefficients together assumes that the FET defined in an OpenMC input is used to define coefficients that will be sent to an external physics code, since the corresponding coefficients to {\it receive} back are also associated with this tally. The downsides of this approach are:

\begin{itemize}
\item If the user simply wants to defined an FET {\it not} for the purpose of external physics coupling, then the {\tt received\_coeffs} array will be allocated, but never used.
\item {\bf If more than one FET tally is defined for a cell, some capability must be introduced to inform the external physics code which FET tally to populate the {\tt received\_coeffs} array (i.e. which tallies are used for coupling, and which are used for calculating coefficients for other purposes).}
\item The use of a single array to hold the expansion coefficients implies that coefficients for only one physical quantity will be received by OpenMC (one physical field that is tied to {\it that particular volume} that is associated with the FET). OpenMC will use volume coupling to an external code that performs a simulation to compute an updated temperature field, so this may require modification if more than temperatures are to be exchanged between the codes, such as densities as well. There are three ways that changes in density could be realized in a coupled simulation with OpenMC:
	\begin{enumerate}
	\item An external code computes densities, and sends this information via expansion coefficients to OpenMC. This will require additional arrays to hold the received coefficients beyond the {\tt received\_coeffs} array.
	\item An external code computes densities, and sends information via quantities other than expansion tallies to OpenMC. If the density has a weak effect on the fission power distribution, it may be sufficient to send an average density, or a density that is gridded along one dimension (such as to model coolant density decreases along a fuel channel).
	\item OpenMC internally computes an updated density using other quantities received from external physics codes. For example, to compute the fuel density, correlations for fuel density could be contained within the OpenMC source, and the temperature received from an external code used to compute those quantities. This is an inferior method to the above two options because it reduces modularity and requires adding significant physical models. However, this does not require any additional transfers of data (assuming that all of the other needed information such as temperatures and pressure) to compute densities.
	\end{enumerate}
\end{itemize}

{\bf The implementation discussed in this report assumes that no more than one FET is defined for a given cell, as many of the subroutines developed simply ``match'' with the first matching tally, which will obviously give errors if there are multiple tallies for a cell.}

The expansion order for the fission power distribution produced by OpenMC does {\it not} need to match the expansion order used for transferring data into OpenMC. Hence, the second dimension of the {\tt coeffs} and {\tt received\_coeffs} arrays do not need to match. While the dimensions of the {\tt coeffs} array can be determined from the OpenMC input files, the dimensions of the {\tt received\_coeffs} array depends on information from the external physics code that must be passed to OpenMC.

\subsubsubsection{{\tt read\_tallies\_xml()}}
Currently, all FETs must be defined in the tallies XML file. This file is read and the information stored in OpenMC data structures during the initialization phase. The following high-level summary of this routine indicates changes made to implement FETs. {\bf Future improvements may include automatically specifying which cells should have FETs defined for them given information from an external coupled code, so that the user does not need to redundantly specify the tallies (for instance, in the MOOSE input file we specify which cells in OpenMC we want to transfer information {\it to} - we could imply that these same cells will also transfer information {\it out}, and hence FETs should be defined for them).}FETs are currently implemented as a custom score type, but work by INL will extend this and improve generality by instead defining a FET filter. The discussion here is based on FET scores. 

\begin{enumerate}
%\item {\bf Read any meshes specified}. %A mesh can be used to specify the grid over which tallies are to be scored, instead of using other geometrical filters such as cells. Allocate the global {\tt meshes(n\_meshes)} array to hold this mesh information. If the run does not use CMFD acceleration, then the number of meshes is simply equal to the number of meshes specified in the input file. Otherwise, the number of meshes is the sum of the CMFD meshes plus any user-specified meshes.  
\item Read the number of user filters and assign to the {\tt filters} array. %The filters are allocated by calling {\tt add\_filters(n\_user\_filters)}, which allocates the global {\tt filters} array if it does not exist, or else extends the {\tt filters} array.
\item Add user tallies to the {\tt tallies} array.% by calling the {\tt add\_tallies("user", n\_user\_tallies)} subroutine.%If the global variable {\tt n\_tallies} is zero, then allocate {\tt tallies(n)}, otherwise extend the {\tt tallies} array with the additional tallies.
%\item Loop through the number of user meshes and read in XML data to the {\tt meshes(i)} structure. 
\item Read data for user filters. Loop through the {\tt n\_user\_filters}.
	\begin{enumerate}
	\item Allocate and declare the filter type by instantiating from the abstract class {\tt TallyFilter} with the extended type (for each type of filter, there is a corresponding type similar to {\tt CellFilter} for cell-type filters that defines additional filter-specific parameters).
	\item Determine the number of bins in a select-case statement for the filter type. The number of bins here is simply the number specified in the input file. For instance, a cell filter defined over three cells has three bins. Store the number of bins that appear in the input file to the {\tt filters(i) \% obj \% n\_bins} member.
	\item Perform any other specific initialization of {\tt filters(i)} based on data from the input file and the type of filter.
	\end{enumerate}
\item Read data for tallies. Loop through the {\tt n\_user\_tallies}.
	\begin{enumerate}
	\item Set the tally type to volume by default. 
	\item Set the estimator to tracklength by default in case the user doesn't specify anything in the input file. More events will contribute to tracklength tallies than with other estimators (though we can't use this for tallies that require post-collision information).
	\item Loop over the filters specified for this tally.	
		\begin{enumerate}
		\item For each filter on the tally, populate the numeric entry of the {\tt find\_filter} array corresponding to the filter type of this filter (i.e. set the {\tt FILTER\_CELL} entry to the filter number in the {\tt filters(:)} array if the filter is a cell filter). Hence, the {\tt find\_filter} array holds all zeros except for entries that correspond to the number of filters for this tally.
		\item Any filters that cannot use a tracklength estimator (those that require you to have sampled the outgoing phase space following a collision) are set to analog.
		\end{enumerate}
	\item Read data for nuclides (filters for this tally). The {\tt tallies(i) \% nuclide\_bins} array holds the integer equivalents of all the nuclides for tally. If you specify {\tt all}, then the number of nuclide bins is equal to the total number of nuclides plus one (the last entry, with value {\tt -1}, is the sum over all nuclides). If you specify {\tt total}, then you tally irrespective of material. Finally, if you specify a list of nuclides, then only those nuclides are tallied.
	\item Loop through the scores for this tally.
		
		\begin{enumerate}
		\item Determine the number of additional scores required due to moment scores before you allocate storage for the scores. This is only required for score types present in the {\tt MOMENT\_STRS} array, which holds the names of moment tallies. These tallies are invoked in the input file by following their name directly by a number indicating an order. A new constant, {\tt MAX\_Z\_ANG\_ORDER}, was included to indicate the maximum Zernike order of 18, which was higher than the {\tt MAX\_ANG\_ORDER} used for the Legendre and spherical harmonics expansions. If the order is greater than the maximum order, a warning is thrown, and instead the order is gracefully set to the maximum angular order allowed. To implement FETs, a new tally score, {\tt kappa-fission-z} was added.

\begin{lstlisting}[language=Fortran]
		character(*), parameter :: &
          MOMENT_STRS(7)  = (/ "scatter-p      ",   &
                               "nu-scatter-p   ",   &
                               "flux-y         ",   &
                               "total-y        ",   &
                               "scatter-y      ",   &
                               "nu-scatter-y   ",   &
                               "kappa-fission-z"/), &
          MOMENT_N_STRS(2)  = (/ "scatter-    ",   &
                               "nu-scatter- "/)
\end{lstlisting}

The number of score bins is determined according to the order of the expansion requested. For an order \(n\), \(n+1\) bins are required for Legendre, \((n+1)^2\) for spherical harmonics, and \(0.5(n+1)(n+2)\) for Zernike tallies. Hence, {\tt tallies(i) \% n\_scores} contains the actual number of scores (including {\it all} expansion orders for moment expansions), while {\tt tallies(i) \% n\_words} contains the number of scores specified in the XML file.
		\item Allocate {\tt tallies(i) \% score\_bins(:)} and {\tt tallies(i) \% moment\_order(:)} of length {\tt n\_scores}. Initialize the {\tt moment\_order} array to all zeros. 
		\item Loop over the number of scores specified in the input file.
			
			\begin{enumerate}
			\item Enter a select-case statement that loops over all of the possible score types. Additional checks are made here to ensure compatibility between the types of filters you specified above, and the types of scores. 
			\item For each type of score, set {\tt tallies(i) \% score\_bins(j) = SCORE\_FLUX}, or the appropriate type of scoring. For scores that inherently contain more than one score bin (the moment scores), simultaneously apply values for all of the {\tt score\_bins} and {\tt moment\_order} (where the order of the {\it total} expansion is held). The following shows the initialization of the new {\tt kappa-fission-zn} tally. Currently, this FET is implemented as an analog tally, and a check was added to ensure that a cell filter is used (a user cannot specify more than one filter of the same type, so no checks need to be made to ensure that more than one cell filter are specified). As discussed, because the order of the expansion to be received from an external physics code isn't know by OpenMC, the {\tt received\_coeffs} array cannot be allocated during the XML input file reading phase.
					
A modification was made to Matt's work to automatically retrieve the radius from the cell geometrical information, so the geometric normalization information that was originally required in the tallies XML input is no longer needed.
		
			\item Check that no duplicate scores exist by looping through the number of scores.
			\end{enumerate}
		\end{enumerate}
	%\item Check for tally derivatives. 
	%\item If {\tt trigger\_on} is true based on the settings XML file, then create tally triggers.
	\item {\bf Set tally estimator {\it if} specified in XML file} - this checks if the user specified an estimator in the input file (previously it was set according to the type of tally as either was strictly required to be an analog estimator, or by default as a track-length estimator).  If you don't need post-collision information, then the type specified by the user in the tallies XML file will supersede any defaults set earlier. 
	\end{enumerate}
\end{enumerate}

\subsubsubsection{{\tt score\_general\_ce(...)}}

After the tallies are read from the input file, all scoring to the tallies occurs within the {\tt transport} subroutine. Within this main transport loop, after cross sections are calculated, the following sequence of events occurs.

\begin{enumerate}
\item Find distance to nearest boundary.
\item Sample a distance to collision. 
\item Advance the particle the smaller of the distance to the nearest boundary and to a collision. %Update the {\tt p \% coord(j) \% xyz} particle coordinates by looping through the number of particle coordinates and adjusting each accordingly.
\item Score track-length tallies.% by calling {\tt score\_tracklength\_tally(p, distance)}.
%\item Score flux derivative accumulators for differential tallies.
\item If the particle crosses a surface, then determine if it crossed either a lattice boundary or a surface boundary, and update the {\tt p \% event} field. Otherwise, the particle has a collision. %(to either {\tt EVENT\_LATTICE} or {\tt EVENT\_SURFACE})
	\begin{enumerate}
	\item Score to surface current tallies before the particle direction changes, since we want to use the incoming neutron direction for this tally.
	\item Perform a collision. %by calling {\tt collision(p)}. 
	\item Score collision and analog estimator tallies. %by calling {\tt score\_collision\_tally(p)}. 
	%\item Score analog tallies by calling {\tt score\_analog\_tally(p)}.
	%\item Reset banked weight during collision by setting {\tt p \% n\_bank = 0, p \% wgt\_bank = ZERO, p \% n\_delayed\_bank(:) = 0}. 
	%\item Save coordinates for tallying purposes by writing to {\tt p \% last\_xyz\_current}. 
	\item Set {\tt p \% last\_material = NONE} so that cross sections will be re-evaluated. 
	\item Check for secondary particles if this particle is dead. If any secondary particles exist, then initialize them by calling {\tt initialize\_from\_source}. 
	\end{enumerate}
	\item Repeat until particle is dead (no secondary particles and an absorption).
\end{enumerate}

All volume tallies call the {\tt score\_general\_ce} subroutine. This subroutine adds scores to the tally array for a given filter and nuclide. In a select-case statement, the actual ``formulas'' for tallying different scores are held - the end result is to determine a value for the score. Because the FETs will be volume tallies, modifications must be made to this routine. The only change made in this section to accommodate the new {\tt kappa-fission-zn} tally was to include its type in the same case statement as the {\tt kappa-fission} tally. 

\subsubsubsection{{\tt expand\_and\_score(...)}}
After determining the correct value of the score from the {\tt score\_general\_ce} routine, expand the score if necessary and add it to the tally results by calling the {\tt expand\_and\_score} subroutine. This routine takes a previously-determined score value and adjusts it if needed, such as for functional expansion weighting, and then adds the resultant value to the tally results array. For FETs, this is where the expansion itself occurs. After computing normalized coordinates by calling the {\tt get\_polynomial\_norm\_positions} subroutine, the score is expanded in Zernike polynomials and added to the tally results array. The formula for the {\tt SCORE\_KAPPA\_FISSION\_ZN} tally is implemented here, and is essentially the same as other moment scores, except that the score is weighted by {\tt calc\_zn\_scaled}. 
			
\subsubsubsection{{\tt fet\_deconstruction()}}
Normally, the computation of the means and standard deviations of tallies is performed in the {\tt write\_tallies} subroutine. But, this routine is only called from {\tt openmc\_finalize}, which in the context of a coupled simulation, would only be called at the very end of the coupled run, which would not permit transferring the expansion coefficient information from OpenMC to an external code at each Picard step. So, the {\tt fet\_deconstruction} subroutine was added to statistically sum the FET coefficients and assemble them into the {\tt tallies(i) \% coeffs} array for easy access. 

 Then, loop over the filters for the tally until the cell filter is found (which must be present based on checks placed in the {\tt read\_tallies\_xml} subroutine). Then, looping over all of the score bins, the cell index is determined given the specific bin combination (of all of the filters). Then, the tally results are written according to the cell index. 

Consider a tally defined with two filters, a cell filter and a material filter, which will be used to illustrate how the {\tt fet\_deconstruction} subroutine stores the expansion coefficients. This subroutine loops over all tallies, and skips any tallies that don't have the {\tt coeffs} array allocated (i.e. all non-FET tallies). More than one filter may be specified for a tally. Of these potentially multiple filters, we need to determine which one is the cell filter in order to then look up that filter in the {\tt filters} array. {\tt cell\_filt\_i} is the index in the {\tt <filters> </filters>} section that is the cell filter. Below, the cell filter is listed second of the filters, so {\tt cell\_filt\_i = 2}. Then, for each score bin combination, the coefficients are stored in the {\tt coeffs} array according to the index of the cell listed in the {\tt <bins> </bins>} section of the cell filter. For instance, below, coefficient results will be stored first for cell 20, and then for cell 10.

\begin{lstlisting}
<tallies>

  <filter id="25">
    <type> cell</type>
    <bins> 20 10</bins>
  </filter>

  <filter id="35">
    <type> material</type>
    <bins> 12 4 3</bins>
  </filter>

  <tally id="1">
    <filters> 35 25</filters>
    <scores>kappa-fission-z1</scores>
  </tally>

</tallies>
\end{lstlisting}

\subsubsubsection{{\tt get\_coeffs\_from\_cell(cell\_id, cell\_coeffs, n)}}
This subroutine is used to write coefficients from OpenMC into an array passed in by an external physics code given a cell, which is represented by passing in the cell ID given in the geometry XML file. Note that the coefficients are stored in {\tt tallies(i) \% coeffs} by the index of the cell in the bins for the cell filter, and {\it not} the cell index in the {\tt cells} array. Given a cell index in the {\tt cells} array, loop over all of the tallies. Skip any tallies that {\tt tallies(i) \% coeffs} is not allocated for (i.e. non-FET tallies). Then, loop over all of the filters defined for the tally until you find the filter that is the cell filter (which must be specified given constraints in the tallies XML reading stage). Then, loop over all of the cells for the cell filter. If the cell index passed in matches one of the cell indices that are covered by that cell filter, then pass the coefficient values to the array. A check is made here to ensure that the array passed in is the appropriate size. This routine also checks to make sure that an FET tally is defined for the requested cell.

{\bf Note that this routine only matches with the first FET tally that exists for the specified cell, so this hinges on the assumption that at most one FET tally is defined for a cell.}

\subsubsection{Field Reconstruction}
This section discusses the changes made in the OpenMC source code to receive a set of expansion coefficients from an external code and expand them into a continuous field. First, a discussion is given of the mechanical receipt of coefficients from an external code and how that is expanded into a continuous field. The data transferred to OpenMC are temperatures and densities in the fuel and fluid coolant. In order to understand how to use these reconstructed fields to set temperatures and densities, it is necessary to understand how temperatures and densities are treated in OpenMC. The remaining sections then discuss how a constructed field is used to change temperatures and densities.

\subsubsubsection{{\tt receive\_coeffs\_for\_cell(cell\_id, cell\_coeffs, n)}}
This subroutine was added in order to receive expansion coefficients from an external code and store them in the {\tt received\_coeffs} array. A cell ID is passed in, which is converted to the index in the {\tt cells} array. Then, in a loop over all the tallies, all non-FET tallies will be skipped over (those that do not have {\tt coeffs} allocated). Then, if {\tt tallies(i) \% received\_coeffs} has not already been allocated, such as will be the case for the very first receipt of information from an external physics code, it will be allocated. The number of cells is assumed to match the number of cells tallied for the expansion coefficient sent {\it out} from OpenMC, while the number of coefficients is assumed to be equal to the length of the array passed into this subroutine. Hence, the order of the expansion produced by OpenMC does not need to match that received from an external code. An implicit assumption exists here that each of the cells for receiving coefficients will have the same number of coefficients, similar for the reverse case of coefficients generated by OpenMC. 

For all other cases except the first receipt of coefficients from an external code, the {\tt received\_coeffs} array will have already been allocated, so the number of coefficients for the first cell is checked against the length of the array passed into this subroutine, and an error is thrown if the sizes do not match. This implicitly assumes that the order of the expansion for each tally does not change during the simulation and that the expansion order is the same for all cells belonging to the matching OpenMC {\tt kappa-fission-zn} tally. {\bf This tally only matches with the first matching FET tally in OpenMC, which will cause problems if more than one FET is defined for the same cell in OpenMC.} An error will be thrown if no matching tallies in OpenMC exist over the cell ID passed in, but this is logical because domains are to be linked between OpenMC and an external physics code by volume coupling.

When these coefficients are used internally by OpenMC, it is assumed that the SCALAR variables used as the {\tt source\_variable} in the transfer lists the expansion coefficients grouped by all the Zernike coefficients (organized by increasing \(n\), with increasing \(m\) for each \(n\)) in increasing Legendre order. For instance, the following would be the correct syntax in the input file for a transfer to OpenMC of coefficients associated with a Legendre order of 2 and a Zernike order of 1.

\begin{lstlisting}
[Transfers]
  [./to_openmc]
    type = MultiAppOkapiMooseTransfer
    direction = to_multiapp
    source_variable = 'l_0_coeffs l_1_coeffs l_2_coeffs'
    to_aux_scalar = 'foo'
    openmc_cell = 1
  [../]
[]
\end{lstlisting}

where {\tt l\_0\_coeffs} then holds the following coefficients in the following order:

\beq
\textrm{{\tt l\_0\_coeffs}}= \begin{bmatrix}C_0^{n=0,m=0} & C_0^{n=1, m=-1} & C_0^{n=1,m=1} & C_0^{n=2,m=-2} & C_0^{n=2,m=0} & C_0^{n=2,m=2}\end{bmatrix}
\eeq

\subsubsection{Changing Temperatures}
The expansion coefficients received from an external code are stored each Picard iteration in the {\tt tallies(i) \% received\_coeffs} array.

This section describes how temperatures are set in OpenMC for a normal, non-coupled simulation in order to elicit what changes are needed to modify a temperature in OpenMC and obtain the correct end result of changing the temperature at which cross sections are evaluated.

\begin{enumerate}
\item {\bf Read input data from the geometry XML file}. In the {\tt read\_geometry\_xml()} subroutine, the global array {\tt cells(n\_cells)} is allocated, where {\tt n\_cells} is the number of cells that are specified in the input file ({\it not} each distributed cell). This is the subroutine in which the cell temperatures are read in, {\it if} they have been specified in the geometry XML file. If the temperature has not been specified, then set the temperature to {\tt ERROR\_REAL} to be changed later. 

\begin{algorithm}[H]
 \While{i $<$ n\_cells}{
  cells(i) \% instances = 0\;
  cells(i) \% distribcell\_index = NONE\;
  \;
   \eIf{number of temps specified $>$ 0}{
   allocate(cells(i) \% sqrtkT(n))\;
   }{
  allocate(cells(i) \% sqrtkT(1))\;
  cells(i) \% sqrtkT(1) = ERROR\_REAL\;
  }
 }
\end{algorithm}

\item {\bf Assign temperatures to cells that did not have temperatures specified in the geometry XML file}. The {\tt assign\_temperatures(material\_temps)} subroutine deallocates and reallocates {\tt cells(i) \% sqrtkT(:)}. If any of the materials in the cell don't have temperatures defined, then use the global {\tt temperature\_default} of 293 K.

\item {\bf Determine temperatures at which to read cross sections and \(S(a,b)\) tables}. This is done in the reading of the materials XML file. From within the {\tt read\_materials} subroutine, a call to the {\tt get\_temperatures} subroutine then allocates memory for and returns a list {\tt nuc\_temps(:)} of temperatures that each nuclide appears at in the model. 

\item {\bf Read cross sections from HDF5 files}. The {\tt nuc\_temps(:)} array is passed into the {\tt read\_ce\_cross\_sections} subroutine. Hence, the {\tt nuc\_temps(:)} array contains all of the temperatures that each nuclide appears at in the input files, while the {\tt Type(Nuclide) :: nuclides(:)} type contains much more aggregate information for the nuclides in the model. The {\tt nuc\_temps} array is passed into the {\tt nuclide\_from\_hdf5} subroutine, which finds all of the nuclear data that bounds each of the specified temperatures. Finally, the {\tt kTs(:)} array of the global {\tt nuclides} data structure is populated with all of the temperatures at which the data will be read. For a coupled simulation, you likely will not know a-priori all of the temperatures that you will need to load in, so a new settings XML parameter, {\tt temperature\_range}, allows the user to specify an expected range in temperatures for which to load nuclear data to avoid loading all the data.
\end{enumerate}

This completes the loading of temperature-dependent cross sections. The use of this temperature-dependent data occurs within the {\tt transport} subroutine. At the start of the transport routine, the calculation of cross sections for the particle is performed by initializing {\tt micro\_xs \% last\_E = ZERO}. Then, from within the event loop, the following actions occur.

\begin{enumerate}
\item {\bf Determine the particle's cell if it is not known} ({\tt NONE}) by calling the {\tt find\_cell} subroutine. This subroutine is called recursively, and will only break out once reaching the lowest universe. Once reaching the lowest universe (where the cell is filled by a homogeneous material (cell type is {\tt FILL\_MATERIAL}), this is where the {\tt p \% last\_material} and {\tt p \% last\_sqrtkT} entries are saved. A distributed cell exists if {\it either} the material or temperature lists have more than one value. Then, the particle's new temperature and material are saved according to the cell the particle is in.

% An optional list of cells is passed into the {\tt find\_cell} routine, and those cells are looped over to determine which cell a particle is in (if this list is not present, then all of the cells in the universe are searched).
				
		\begin{algorithm}[H]
		\While{i $<$ size of search\_cells}{
		  index\_cell = search\_cells(i)\;
		  \;
	      \eIf{cell\_contains(cells(index\_cell, p))}{
			p \% coord(j) \% cell = index\_cell\;
			\;
		  \If{cells(index\_cell) == FILL\_MATERIAL}{
			p \% last\_material = p \% material\;
		    p \% last\_sqrtkT = p \% sqrtkT\;
			\;
		  \If{size(cells(index\_cell) \% sqrtkT) $>$ 1 or size(cells(index\_cell) \% material) $>$ 1}{
			find distributed offset\;}
				      \;
				      p \% material = cells(index\_cell) \% material(offset + 1)\;
				      p \% sqrtkT = cells(index\_cell) \% sqrtkT(offset + 1)\;
				    }
				  }{
				    repeat;
				  }
				}
				\end{algorithm}
				
\item {\bf Calculate cross sections} by calling {\tt calculate\_xs(particle)}, but only if the {\tt p \% material} differs from {\tt p \% last\_material} (new nuclides) or if {\tt p \% sqrtkT} differs from {\tt p \% last\_sqrtkT} (new temperature). Loop through all of the nuclides in the material, and calculate microscopic cross sections for each nuclide. Call the {\tt calculate\_nuclide\_xs} subroutine if the last energy or last temperature at which microscopic cross sections were looked up has changed (since these cross sections are cached). It is in this routine that the temperature method is actually implemented. 
		
% All of the material macroscopic cross sections {\tt material\_xs} are set to zero. {\tt material\_xs} is a cache of type {\tt MaterialMacroXS} that holds cross section values for the current material. The current material is indicated by the integer value of {\tt p \% material}. {\tt micro\_xs} is a cache to hold microscopic cross section data for each nuclide in a material.

\item Then, transport the particle, score tallies, and repeat until the particle is dead and there are no more secondary particles.
\end{enumerate}

%The temperature at which a cross section is evaluated can be performed in one of two ways - {\tt TEMPERATURE\_NEAREST} simply finds the nearest temperature value in the cross section data, and uses that value, while {\tt TEMPERATURE\_INTERPOLATION} will randomly sample (linearly) between the two temperatures that bound the current temperature. The default method is to use the nearest cross section. To use the interpolation method, note that the default temperature of 293 K cannot be used, since no cross sections exist below this temperature (so there is nothing to bound on the lower end).

So, in order to use a reconstructed temperature field in OpenMC simply requires changed the temperatures of the appropriate cell, since the cell temperatures are used to determine which cross section values to use. Because the cross sections are forced to be calculated for every particle at the {\it start} of the transport subroutine, there is no need to reset or clear cross sections between Picard steps. 

\subsubsection{Changing Densities}
This section discusses the changes made in OpenMC to allow fluid densities to be changed given information from Nek. 

\subsection{Nek Data Transfer}
\label{sec:NekTransfer}
This section describes the capabilities introduced into the Nek source code that allows evaluation of polynomials, and construction/deconstruction of continuous fields. This section also describes how the coupling information resides within Nek data structures.

Nek COMMON blocks map directly into C structs, so much of the coupling information that might be needed by an external code are stored in common blocks. Several new common blocks were added to facilitate coupling and hold global variables. Table \ref{table:NekCommon} shows the names of these common blocks and their members. Note that the order of the Legendre and Fourier expansions are always at least 1 (because Fortran indexing begins at 1), and hence differs from the other coupling codes.

\color{magenta}
Make sure that the order of the polynomials is interpreted the same among all codes.
\color{black}

\begin{table}[H]
\caption{Summary of Nek common blocks and variables added to facilitate coupling.}
\centering
\begin{tabular}{l l l}
\hline\hline
Common block name & Variables & \\ [0.5ex]
\hline
{\tt layer\_data} & {\tt n\_layer} & Number of integration layers\\
{\tt fluid\_bins} & {\tt fluid\_temp\_bins} & Axially-binned average fluid temperatures\\
{\tt expansion\_tdata} & {\tt n\_legendre} & Legendre order of expansion\\
& {\tt m\_fourier} & Fourier order of expansion\\
{\tt expansion\_tcoef} & {\tt coeff\_tij} & Array holding temperature BC coefficients\\
{\tt expansion\_fcoef} & {\tt coeff\_fij} & Array holding flux BC coefficients\\
{\tt expansion\_recon} & {\tt flux\_recon} & Array holding reconstructed heat flux\\
{\tt test\_passing} & {\tt flux\_moose} & \\
& {\tt temp\_nek} & \\
\hline
\end{tabular}
\label{table:NekCommon}
\end{table}

Nek exchanges both boundary data and integral data with external codes. For boundary data, the order of the Legendre and Fourier expansions are assumed to be the same for the heat flux received from Okapi and for the temperature sent to Okapi. For integral data such as the fluid density, instead of expanding in orthogonal polynomials, a layered integration is performed to find the average value in bins. For example, to pass binned values of the fluid density to an external code, the dimension of binning (for now restricted to \(z\)) is divided into layers, and an averaging process performed for each layer. The number of these layers is assumed to be the same for the fluid density and temperature for which this type of data exchange is performed. It is also assumed that none of these constants change throughout the simulation, since they are defined only once at the start of the coupled simulation.

\subsubsection{Evaluation of Polynomials}
This section describes the functions added to Nek to evaluate polynomials. These functions are included in the {\tt usr} file.

\subsubsubsection{{\tt pl\_leg(x, n)}}
This function calculates Legendre polynomials according to Eq. \eqref{eq:LegendreScaled} using a recursion relation for a given coordinate position and order. If \(n\) exceeds the maximum allowed Legendre order, this function returns 0.0.

\color{magenta}
No checks are made that \(n\) is non-negative or that the coordinate is within the valid orthogonal range, which is fine because the {\tt nek\_mode} subroutine pre-normalizes the coordinate to the correct range by reading the minimum and maximum \(z\)-coordinates from mesh 1. Note that this implies that the Legendre expansion occurs in the \(z\)-direction.
\color{black}

\subsubsubsection{{\tt fl\_four(x, n)}}
This function computes Fourier polynomials given a value of \(\theta\) and an order \(n\) according to Eq. \eqref{eq:FourierScaled}. 

\color{magenta}
No checks are made that \(n\) is non-negative or that \(\theta\) is within the valid orthogonal range.
\color{black}

\subsubsubsection{{\tt nek\_mode(fmode, im, jm)}}
This subroutine computes all of the polynomial values (the multiplication of \(P_l(z)F_k(\theta)\)) on the wall boundary for given orders {\tt im} and {\tt jm}, and stores the results in the {\tt fmode} array for use in other routines. The \(z\)-coordinate is normalized to the orthogonal domain \([-1, +1]\), but the \(\theta\)-values is computed assuming the cylinder is centered at \(0, 0\). 

\subsubsection{Field Deconstruction}
This section describes how the temperature on the boundary is deconstructed into a set of expansion coefficients and how axial integration is performed to return average binned fluid temperatures and densities that are to be sent to an external code. 

\subsubsubsection{{\tt layered\_mask()}}
This subroutine computes the mask needed to perform a layered integration in the \(z\)-direction using the {\tt glsc3} function. The {\tt glsc3} function computes a volume integral over the {\it entire} domain, and one method for integrating only over a certain axial bin is to define a mask for that bin that is 1 for all elements in the bin and 0 for all elements outside the bin. Because Nek may in general use an unstructured mesh, there is the possibility that an element may span more than one bin. To account for this case, for elements that span more than one bin, instead of using 1 or 0 for the mask, the fraction of the GLL points that appear in the bin is used. In this way, the total integral is still conserved, but an approximation performed of splitting the element amongst bins (without actually splitting the integration over bins). 

\color{magenta}
It is assumed that elements span at most two layers (this routine can be modified to be even more general). Using a mask to compute an axial integral is also inefficient due to the high memory usage, and should be modified in the future. 
\color{black}

\subsubsubsection{{\tt axially\_binned\_integration()}}
This subroutine integrates the fluid temperature according to the masks computed in the {\tt layered\_masks} subroutine and stores the axial volume averages in the {\tt fluid\_temp\_bins} array, which is organized with the lowest-\(z\) bins listed first.

\subsubsubsection{{\tt nek\_expansion()}}
This subroutine determines the expansion coefficients for the temperature on the boundary to be passed to an external code.

\subsubsection{Field Reconstruction}
Okapi transfers a heat flux to Nek to apply on the boundary between the fuel and coolant. A structure is defined with an array {\tt coeff\_fij} of doubles, named {\tt expansion\_fcoef\_}. This array receives expansion coefficients {\it from} Okapi, and hence stores expansion coefficients for a heat flux surface boundary condition. 

\subsubsubsection{{\tt flux\_reconstruction()}}
This subroutine reconstructs a continuous flux distribution on the surface. Given coefficients \(C_{kl}\) from Okapi, the flux \(u(\theta, z)\) is reconstructed as:

\beq
u(\theta, z)=-\sum_{k=0}^{K}\sum_{l=0}^{L}C_{kl}P_l(z)F_k(\theta)\frac{\textrm{surface area}}{4}
\eeq


\subsubsection{Changing a Surface Boundary Condition}
This section describes the subroutines in Nek that are used to change the heat flux boundary condition using the reconstructed flux.

\subsubsubsection{{\tt userbc(ix, iy, iz, iside, ieg)}}
This subroutine is used to define a custom boundary condition, and is where the reconstructed flux is used to specify the heat flux boundary condition in Nek.
 

\clearpage
\section{Executioners and Timesteppers}
This section describes how an external code can be run from within the MOOSE framework by developing custom MOOSE {\tt Executioner} and {\tt TimeStepper} classes that run subroutines of the external code. In order to wrap an external code as Moose App, an understanding of the Moose executioner and timestepper methods is needed to show the types of execution ``chunks'' that a once-continuous code such as OpenMC must be decomposed into. When MOOSE performs a simulation, the following major steps are performed:

\begin{enumerate}
\item Call {\tt MooseInit} to initialize MPI, the solvers, and MOOSE.
\item Register the application's MooseApp and any that it depends on, such as modules. 
\item Create an instance of the application.
\item Execute the application by calling the {\tt run()} method of the App. This will perform setup, read the input file, and then call the {\tt executeExecutioner()} method. This method calls the {\tt init()} method, followed by the {\tt execute()} method of the executioner specified in the input file.
\end{enumerate}

Hence, the brunt work of the solve is contained in the executioner, so in order to couple an external code, most of the required calls to external code routines can be placed in the executioner in the appropriate methods that are called once per coupled solve, for every Picard step, and at the end of the simulation to perform cleanup. So, in order to call an external code from within the MOOSE framework, the structure of the executioner and timestepper classes must be understood. Because all of the coupled simulations of interest to this project will be transient, emphasis will be placed on the {\tt Transient} executioner. In a MOOSE simulation, the methods of the transient executioner are called in the following order. 

\begin{enumerate}
\item {\bf Perform initializations} at the very start of the simulation (before any time stepping has occurred), by calling {\tt init()}, which calls the {\tt init()} method of the timestepper - defined by custom time stepper. 
\item Perform the time stepping within the {\tt execute()} method.
	\begin{enumerate}
	\item {\tt preExecute} - call the {\tt preExecute()} method of the time stepper%, which deletes all previous sync times
	\item {\bf Main time stepping loop}. For each time step, perform the following:
		\begin{enumerate}
		\item For all but the first time step, the problem state and MultiApps are advanced by calling the {\tt incrementStepOrReject()} method, which checks to make sure that the last solve converged (MasterApp + any MultiApps) before advancing the state.
		\item {\tt keepGoing()} - determines whether or not to break from the main time stepping loop; a break will occur if the last solve converged (MasterApp + any MultiApps) and a) you've reached the desired number of time steps or the end simulation time or b) you've reached the steady state check, and no longer need to run a transient simulation. This method will also break out of the main time stepping loop if last solve did not converge, and you can no longer halve the time step due to minimum time step constraints.
		\item {\tt preStep()}  - defined by custom time stepper
		\item {\tt computeDT()} - compute \(\Delta t\), which calls {\tt computeStep} by the time stepper (where the current \(\Delta t\) is actually selected). If you're at the first time step, then call the {\tt computeInitialDT()} method of the time stepper. Otherwise, if the solve has not converged, then the {\tt computeFailedDT()}  method of the time stepper simply cuts the time step in half, provided you're not  already at the minimum time step size.
		\item {\tt takeStep()} - {\bf Perform a single complete time step}. Initialize the number of Picard iterations to zero. The Picard loop is then controlled as a while loop. For each Picard iteration:
			\begin{enumerate}
			\item {\tt solveStep()}  - {\bf solve a single Picard step}
				\begin{itemize}
				\item Perform transfers to MultiApps on {\tt timestep\_begin}.
				\item Execute (solve) MultiApps on {\tt timestep\_begin}. If those MultiApps don't converge, then return.
				\item {\tt preSolve()} - call executioner and time stepper methods of the same name, both undefined.
				\item {\tt \_time\_stepper->step()} solves the finite element problem of the Master App. 
				\item Check that both the MasterApp and all of the {\tt timestep\_begin} MultiApps have converged.
				\item Perform transfers to MultiApps on {\tt timestep\_end}.
				\item Execute (solve) MultiApps on {\tt timestep\_end}. If those MultiApps don't converge, then return.
				\item {\tt postSolve()} - call Executioner and time stepper methods of the same name, both undefined.
				\end{itemize}
			\end{enumerate}
		\item {\tt endStep()} computes error indicators and performs output to the terminal. 
		\item {\tt postStep()} - defined by custom time stepper
		\end{enumerate}
	\item {\tt postExecute} - defined by custom time stepper
	\end{enumerate}
\end{enumerate}

So, as long as the routines needed from an external code can be cast into one or several of these executioner and time stepper methods shown above, then a custom executioner class can be created and override the needed methods. This executioner would then be used in a MOOSE input file to run an external code from within the MOOSE framework. To wrap an external code as a MOOSE executioner requires that we decompose the code project into the following basic actions:

\begin{itemize}
\item Initialization and problem setup before {\it all} solves.
\item A manner for assessing convergence to communicate to the Master App whether or not the MOOSE-wrapped App has converged for a single set of inputs. Most external codes will already have this capability built in, and can simply always ``tell'' MOOSE that they have converged once they have finished executing.
\item Initialization and problem setup to be performed before each individual solve. This might include resetting variables to zero that would otherwise stay alive in memory with repeated runs, for instance.
\item Perform a single solve.
\end{itemize}

\subsection{OpenMC Execution}
This section describes the OpenMC executioner and time stepper created that will call the OpenMC routines that are needed to perform a Monte Carlo simulation within the MOOSE framework. 

\subsubsection{{\tt OpenMCExecutioner}}
Because OpenMC will be run in a transient simulation, the executioner that wraps calls to OpenMC routines inherits from the {\tt Transient} executioner.

\subsubsubsection{{\tt init()}}
This method is called once at the very beginning of the simulation to perform tasks that are required for {\it all} OpenMC runs needed later in the coupled simulation such as memory allocation, MPI setup, reading initial data from XML files, etc. The actions taken by this method are completely independent of any input information from MOOSE such as temperatures that are to be coupled during Picard iterations. This method simply calls the {\tt init()} method of the {\tt Transient} executioner, followed by the {\tt openmc\_init} subroutine. This subroutine performs the following major actions:

\begin{itemize}
\item Read input files and build data structures for the geometry, materials, tallies, and other variables.
\item Initialize a random number seed.
\item Read cross section data.
\item Sample the particle source sites and the particle direction and energy. For a fixed source simulation, these are known explicitly and are specified in an input file, but for an eigenvalue problem, a rough initial source is either specified in the input file or is assumed to be some default source. 
\end{itemize}

The only major change made to this subroutine was to change its parameter list to accept an MPI communicator, instead of initializing MPI itself. This permits an external code to setup MPI, which would then not be duplicated by OpenMC.

\subsubsection{{\tt OpenMCTimeStepper}}
The OpenMC time stepper is where the calls to the routines needed to run and finalize an OpenMC simulation are made.

\subsubsubsection{{\tt step()}}
This method is called once every Picard step, and contains a call to {\tt openmc\_reset}, which resets tally variables and other run-particular variables, followed by {\tt openmc\_run}, which transports the particles through the geometry by iterating over over batches, generations, and particles. 

\subsubsubsection{{\tt postExecute()}}
This method is called once at the very end of the coupled simulation, and calls {\tt openmc\_finalize}, which writes OpenMC output files (only for the last time step) and performs other basic tasks such as freeing allocated memory.

\subsection{Nek Execution}
This section describes the Nek executioner and time stepper created that will call the Nek routines that are needed to perform a computational fluid dynamics simulation within the MOOSE framework.

\subsubsection{{\tt NekExecutioner}}
Because Nek will be run in a transient simulation, the executioner that wraps calls to Nek routines inherits from the {\tt Transient} executioner.

\subsubsubsection{{\tt init()}}
This method is called once at the very beginning of the simulation to perform tasks that are required for {\it all} Nek runs needed later in the coupling simulation such as memory allocation, MPI setup, reading initial data, etc. The actions taken by this method are completely independent of any input information from MOOSE such as boundary conditions that are to be coupled during Picard iterations. This method calls the {\tt init()} method of the {\tt Transient} executioner, followed by the {\tt nek\_init()} subroutine. This subroutine is defined in the core Nek source code, and performs the following actions:

\begin{enumerate}
\item If MPI has not yet been initialized, then MPI is initialized. A duplicate communicator is created to avoid name collisions. Then, the Nek communicator is initialized by calling {\tt init\_nek\_comm(intracomm)}, which simply assigns the intracommunicator to {\tt nekcomm} and determines the rank and total number of processors.
\item Read the {\tt SESSION.NAME}, {\tt SIZE}, and {\tt .rea} files and set values based on parameters in these files. The rank 0 process reads all this information and then broadcasts it to the other processes.
\item Initialize variables, setup the domain topology, compute quadrature points and weights.
%\item Call the {\tt usrdat} subroutine.
\item Generate the geometry, compute Jacobians, set initial conditions, compute material properties, and initialize timers to zero.
\end{enumerate}

Aside from calling {\tt nek\_init()}, this method also calls {\tt nek\_init\_coupling()}, which calls other Nek {\tt usr} subroutines that are not defined in the Nek core code. This is required here to avoid making changes to the Nek core code (of which {\tt nek\_init} is a part). {\tt nek\_init\_coupling} calls the following Nek subroutines:

\begin{enumerate}
\item {\tt layered\_mask}, which sets up the masks required for performing an axial integration in bins to compute volume-averaged quantities. This only needs to be performed once per coupled solve provided the mesh and the number of bins does not change.
\end{enumerate}

and performs the following initialization actions:

\begin{enumerate}
\item Set the expansion orders for the surface temperature and heat flux boundary conditions used for coupling to BISON
\item Set the number of desired axial integration layers for the fluid temperature and density
\end{enumerate}

\subsubsubsection{{\tt preStep()}}
This method is called once at the start of each time step (not once per Picard solve, but once for each value of \(t\)). This method calls the {\tt preStep} method of the {\tt Transient} executioner, followed by the {\tt nek\_init\_step} subroutine, which is defined in the {\tt usr} file.

\subsubsubsection{{\tt postStep()}}
This method is called once at the end of each time step (not once per Picard solve, but once for each value of \(t\)). This method calls the {\tt postStep} method of the {\tt Transient} executioner, followed by the {\tt nek\_finalize\_step} subroutine, which is defined in the {\tt usr} file.

\subsubsection{{\tt NekTimeStepper}}
Note that unlike the OpenMC time stepper, the {\tt postExecute} method is not overridden, primarily because there is no dynamic memory that needs to be freed.

\subsubsubsection{{\tt step()}}
This method is called once per Picard step, and calls the {\tt nek\_step} method, which is defined in the {\tt usr} file.

\clearpage
\section{The Mechanics of Coupling}
This section discusses how each of the external codes are built to permit their use from within the MOOSE framework, as well as the input files required for each external code, and how those input files should be located relative to the executable of the main Moose App.

\subsection{OpenMC}

\subsubsection{Build Configuration}
This section discusses how OpenMC is built to allow its routines to be called from within the MOOSE framework. OpenMC is built as a shared library so that its routines can be dynamically linked to a MOOSE application. OpenMC uses CMake as its build system, so all this required (for the most part) was changing the {\tt STATIC} library specification to {\tt SHARED}. Then, to call this shared library from within a MOOSE application required adding the path to the dynamic OpenMC library in the Okapi Makefile and specifying the {\tt library\_path} in the MOOSE input file. Finally, in order for MOOSE to compile correctly, the OpenMC routines to be called must be forward-declared (since they are linked dynamically, and not at compile-time) in the {\tt OpenMCInterface.h} file.

\subsubsection{Input Files}
This section discusses the input files needed by OpenMC, and where they should be placed relative to the Master App executable. When OpenMC is wrapped as a Moose App, it still requires its own XML input files to specify Monte-Carlo specific parameters (and actually all of the parameters that you would need for a Monte Carlo solve, even if those parameters might be duplicated in the MOOSE input file, such as geometry). These input files should be placed in the same location as the MOOSE input file.

\subsection{Nek}

%Nek has minimal use of external libraries, which significantly reduces compile times (but then you need to compile for each individual problem). 

%To improve performance, matrix operations are implemented in assembly code. At the beginning of each run, Nek chooses between one of three parallel implementations by determining which one works best for the current machine - in this way, Nek automatically tunes its parallelism to the machine {\it and} problem.
\begin{comment}
Nek5000 has the following solvers:

\begin{itemize}
\item Incompressible Navier-Stokes; If the viscosity is constant over the entire domain, then the divergence of the stress tensor can be replaced by \(\mu\nabla^2\vv{V}\). The use of the stress tensor is sometimes referred to as the ``stress formulation,'' though they are the same. 

\beqa
\label{eq:INS}
\nabla\cdot\vv{V}=&0\\
\rho\frac{d\vv{V}}{dt}=&-\nabla P+\nabla\cdot\left\lbrack\mu\left(\nabla\vv{V}+(\nabla\vv{V})^T\right)\right\rbrack+\rho\vv{g}\\
\eeqa

\item Non-dimensional Navier-Stokes; this uses the normal choices of scaling parameters, and offers two formulations - one for high Reynolds number flows, in which case pressure scales as \(P^{+}=P/(\rho V^2)\), and for low Reynolds number flows, in which case pressure scales as \(P^{+}=PL/\mu V\). For high Reynolds number flow, the continuity equation is used to find the scales for velocity, and then the scaled momentum equation becomes:

\beq
\frac{du^{+}}{dt^+}=-\nabla P^++\frac{1}{Re}\nabla\cdot\tau^++\frac{1}{Fr}\frac{\vv{g}}{g}
\eeq

\item Low Mach number Navier-Stokes (Nek cannot solve the fully-compressible Navier-Stokes equations); the low Mach number approximation decouples the pressure from the velocity. Nek hard-codes the equation of state here to be the ideal gas equation, which permits variable density and viscosity. The equation of state for density is substituted into the continuity equation, giving a thermal divergence term. 

\item Incompressible Stokes. These equations are the same as those shown in Eq. \eqref{eq:INS}, except that the advective term is absent. 

\item Nondimensional Stokes; this scaling is obtained by selecting the low Reynolds number scaling of pressure discussed above.

\item Steady Stokes.
 
\item The energy equation (solved over both the fluid and solid). This can also be solved in steady form by setting the total time derivative of the temperature to zero.

\beq
\rho C_p\frac{dT}{dt}=\nabla\cdot(k\nabla T)+\dot{q}
\eeq

\item Non-dimensional energy equation. 

\beq
\frac{dT^+}{dt^+}=\frac{1}{Pe}\nabla\cdot\nabla T^++\dot{q}
\eeq

\item Passive scalar equation, which can be solved in either the fluid alone or the entire domain.

\beq
\rho C_p\frac{d\phi}{dt}=\nabla\cdot(k\nabla\phi)+\dot{q}
\eeq

\item Linearized Navier-Stokes, which evolves small perturbations about a base state by solving the linearized Navier-Stokes equation:

\beq
\rho\left(\frac{\partial \vv{V}'}{\partial t}+\vv{V}\cdot\nabla\vv{V}'+\vv{V}'\cdot\nabla\vv{V}\right)=-\nabla P'+\mu\nabla^2\vv{V}'
\eeq

These solutions can be solved at the same time as the base equations, and can provide solutions to be used for computing turbulence quantities.

\item Incompressible magnetohydrodynamics equations.

\end{itemize}

solves the incompressible Stokes, incompressible Navier-Stokes, or compressible Navier-Stokes (low Mach number) equations with forced or natural convection heat transfer in both a fixed or moving geometry. Nek solves for the velocity, pressure, and temperature. In addition, Nek can solve for conjugate heat transfer problems (solid-fluid).

Data is laid out as \(u(i, j, k, e)\), where \(e\) refers to the element.  Hence, all the \(x\)-coordinate data is grouped together, etc. So, because Fortran arrays are column-order, all data arrays are contiguous, even if the number of elements allocated for each processor are excessive given the number of actual elements (because the last array dimension occurs entirely at the end). Then, instead of looping over all four indices, one can do the following (eliminates the overhead of three additional loops):

\begin{lstlisting}[language=Fortran]
do i = 1, nx1*ny1*nz1*nelv
  u(i, 1, 1, 1) = vx(i, 1, 1, 1)
enddo
\end{lstlisting}

Nek can only handle Cartesian 2-D and 3-D geometries. The boundary between the fluid and solid must be representable, or suffer little approximation error, from expression as straight line segments, splines, or circular arcs. Nek can handle time-dependent geometries for boundary conditions that allow the boundary itself to move. For deformable geometries, a mesh velocity is defined at each collocation point of the mesh to describe the deformation of the mesh. This mesh velocity is computed using the appropriate kinematic conditions such as for free surfaces. For all other boundaries, the normal mesh velocity is set to zero. A BVP is solved for the mesh velocity using an elastostatic solver. The new mesh geometry is then computed by integrating the mesh velocity explicitly in time and updating the nodal coordinates of the collocation points. The program will automatically terminate if the mesh becomes of too poor quality (negative or zero Jacobian of transformation). 

Nek can internally generate meshes, as well as provide capabilities for mesh extruding and mirroring. {\tt genmap} is a spectral graph partitioning tool, similar to METIS, which partitions graphs to obtain optimal communication in parallel algorithms. Constructing a matrix that shows how many other nodes are connected to the current node, {\tt genmap} implements a spectral bisection algorithm to decompose a mesh for domain decomposition.

Parallelism in Nek is achieved using domain decomposition. The {\tt lglel} arrays distinguish which processor owns which elements (i.e. for processor zero this array might be {\tt (1, 3, 5, 6, 4)} if this processor contains elements 1, 3, 5, 6, and 4. Then, the {\tt gllel} array assigns the local corresponding node to each global node. For instance, if processor 0 owns 1, 3, 5, 6, and 4, while processor 1 owns 2 and 7, {\tt gllel=(1, 1, 2, 5, 3, 4, 2)}. Finally, {\tt gllnid} assigns the processor number for each global element, and would for this example have the form {\tt gllnid=(0, 1, 0, 0, 0, 0, 1)}. 
\end{comment}

\subsubsection{Build Configuration}
This section discusses how Nek is built to allow its routines to be called from within the MOOSE framework.

\subsubsection{Input Files}
This section discusses the input files needed by Nek, and where they should be placed relative to the Master App executable. Each Nek simulation requires the following core input files:

\begin{itemize}
\item {\tt SESSION.NAME} file, which contains the name of the {\tt .rea} file and the full path to the needed files. Note that it does not need to refer to the same name of the user file, while allows for different {\tt .usr} files, with the same boundary and initial conditions ({\tt .rea} files). An example session file is shown below. 

\begin{lstlisting}
integration_example
/Users/aprilnovak/projects/moon/examples/integration_example
\end{lstlisting}

\item {\tt SIZE} file, which controls, {\it at compile-time}, the polynomial order of the spectral method (i.e. the problem ``size'' refers to the number of points in each element that the solution is evaluated at), the numbers of elements per processor, and the memory allocation for most of the arrays (except those required by C utilities). An example {\tt SIZE} file is shown below to illustrate the parameters. 

It may seem strange that so many parameters should be specified in the {\tt SIZE} file that could be specified at runtime, but this is related to the limitation of FORTRAN 77 in not allowing dynamic array allocation - all array sizes must be known at compile-time, so the user has to provide estimates of maximum values of many variables to ensure sufficient space. The total size of the problem {\it per process} is {\tt lx1*ly1*lz1*lelt} (i.e. this is the size that is actually allocated). Roughly 3.2 kB are required for each grid point, of which there will be \((N+1)^3\) for a 3-D simulation, with \(N\) being the polynomial order. This should be kept in mind to ensure that the total memory requirement of the simulation can be handled by the number of processors requested - if you request too few of processors, then the memory per processor will still exceed the memory available on a core, and hence you would not be able to run your simulation without using more cores. 

\begin{table}[H]
\caption{Summary of important Nek variables.}
\centering
\begin{tabular}{p{2cm}p{13cm}}
\hline\hline
Name & Interpretation \\ [0.5ex]
\hline
{\tt gllel(:)} & Array holding the global to local element mappings; entry \(i\) is the local element number for global element \(i\)\\
{\tt ldim} & Number of spatial dimensions\\
{\tt lelt, lelv} & Upper bound on the number of thermal/velocity elements per processor\\
{\tt lglel(:)} & Array holding the local to global element mappings; entry \(i\) is the global element number for local element \(i\)\\
{\tt lorder} & Maximum time integration order\\
{\tt lp} & Maximum number of processes\\
{\tt lx<n>, ly<n>, lz<n>} & Number of GLL points in the three directions for mesh n (\(N+1\) points for an \(N\)-th order approximation)\\
{\tt nelt, nelv} & Actual number of thermal/velocity elements for current processor (problem-specific)\\
{\tt voltm<n>} & thermal volume on mesh n\\
{\tt volvm<n>} & fluid volume on mesh n\\
\hline
\end{tabular}
\end{table}

The fluid elements are ordered first in the list of elements, followed by the thermal elements.

The following table gives a summary of the notation commonly used to represent other common terms, such as coordinate points.

\begin{table}[H]
\caption{Summary of important Nek syntax.}
\centering
\begin{tabular}{p{5cm}p{10cm}}
\hline\hline
Name & Interpretation \\ [0.5ex]
\hline
{\tt bm<n>} & mass matrix on mesh n\\
{\tt hc} & convection heat transfer coefficient\\
{\tt hrad} & radiation heat transfer coefficient\\
{\tt flux} & Flux\\
{\tt ieg} & Global element number (on processor?)\\
{\tt ix, iy, iz} & Collocation points (their indices)\\
{\tt pr} & pressure\\
{\tt ps1 ... ps9} & Passive scalars 1 through 9\\
{\tt sh1, sh2} & Shear component 1, 2\\
{\tt si2, si3} & Strainrate invariant 2 and 3\\
{\tt sigma} & surface tension coefficient\\
{\tt t} & temperature\\
{\tt temp} & Temperature\\
{\tt tinf} & Temperature at infinity\\
{\tt trx, try, trz} & x, y, z traction\\
{\tt ux, uy, uz} & Velocity\\
{\tt vx, vy, vz} & Velocity\\
{\tt x, y, z} & Coordinates\\
{\tt xm<n>, ym<n>, zm<n>} & x, y, z coordinates on mesh n\\
\hline
\end{tabular}
\end{table}

Nek uses a number of meshes - mesh 1 refers to the velocity and temperature mesh, mesh 2 to the pressure mesh, and mesh 3 to ?. Nek supports a discontinuous pressure formulation, in which case the number of GLL points is two lower than the velocity/temperature mesh. This discontinuous formulation is known as the \(P_N-P_{N-2}\) formulation, while the continuous uses the same number of GLL points as the velocity and temperature.

It is common to use a relatively low-order approximation such as lx1=5 and then to run this for several flow times and use it as an initial condition for a higher-order run that is used to obtain actual results. You should design the mesh to use a polynomial order of 7 or 9.

\begin{lstlisting}[language=Fortran]
! number of points for integration using ``three half rule''. This should typically be set to lxd=1.5*lx1. On some platforms, however, you should set lxd to an even number. You should try to minimize this polynomial order used for the fine grid.
      parameter (lxd=8,lyd=lxd,lzd=lxd)

! maximum number of elements per rank for global Fast Diagonalization Method solver
      parameter (lelx=1,lely=1,lelz=1)

! for 2-D, set lzl to 1, and otherwise to 3
      parameter (lzl=3 + 2*(ldim-3))

! maximum (global) number of elements, which is usually set to more than the number of elements in the mesh. This can be set to the exact number of elements for best memory usage. 
      parameter (lelg = 1000)

! number of elements in the perturbation field, and the number of perturbation fields
      parameter (lpelv=1,lpelt=1,lpert=1)
   
! number of points in the x, y, and z directions of the perturbation field within each element of mesh 1 and mesh 2   
      parameter (lpx1=1,lpy1=1,lpz1=1)
      parameter (lpx2=1,lpy2=1,lpz2=1)


c     parameter (lbx1=lx1,lby1=ly1,lbz1=lz1)     ! array sizes
c     parameter (lbx2=lx2,lby2=ly2,lbz2=lz2)


! total number of elements in the B-field (for MHD)
      parameter (lbelv=1,lbelt=1)

! number of points in the x, y, and z directions of the B-field within each element of mesh 1 and mesh 2
      parameter (lbx1=1,lby1=1,lbz1=1)
      parameter (lbx2=1,lby2=1,lbz2=1)

!     LX1M=LX1 when there are moving meshes; =1 otherwise
      parameter (lx1m=lx1,ly1m=ly1,lz1m=lz1)

! maximum number of T-array fields (temperature and additional passive scalars)
      parameter (ldimt= 4)    

      parameter (ldimt1=ldimt+1)
      parameter (ldimt3=ldimt+3)
c
c     Note:  In the new code, LELGEC should be about sqrt(LELG)
c

! deprecated parameters?
      PARAMETER (LELGEC = 1)
      PARAMETER (LXYZ2  = 1)
      PARAMETER (LXZ21  = 1)
      PARAMETER (LMAXV=LX1*LY1*LZ1*LELV)
      PARAMETER (LMAXT=LX1*LY1*LZ1*LELT)
      PARAMETER (LMAXP=LX2*LY2*LZ2*LELV)

! ?      
      PARAMETER (LXZ=LX1*LZ1)

! maximum number of objects
      PARAMETER (MAXOBJ=4)

! maximum number of members in an object
      PARAMETER (MAXMBR=LELT*6)

! maximum number of history points a single rank will read in from hpts.in. lhis*np > npoints in hpts.in.
      PARAMETER (lhis=100)

C
C     Common Block Dimensions
C
! LCTMP0 is not in use?
      PARAMETER (LCTMP0 =2*LX1*LY1*LZ1*LELT)
      
! 
      PARAMETER (LCTMP1 =4*LX1*LY1*LZ1*LELT)
C
! The parameter LVEC controls whether an additional 42 field arrays are required for Steady State Solutions.  If you are not using Steady State, it is recommended that LVEC=1.
      PARAMETER (LVEC=1)
C
C     Uzawa projection array dimensions
C
! maximum number of history entries for residual projection
      parameter (mxprev = 40)

! dimension of Krylov subspace in GMRES
      parameter (lgmres = 40)
C
C     Split projection array dimensions
C
! not in use?
      parameter(lmvec = 1)

! not in use?
      parameter(lsvec = 1)

! not in use?
      parameter(lstore=lmvec*lsvec)
c
c     NONCONFORMING STUFF
c

! 
      parameter (maxmor = lelt)
C
C     Array dimensions
C
      COMMON/DIMN/NELV,NELT,NX1,NY1,NZ1,NX2,NY2,NZ2
     $,NX3,NY3,NZ3,NDIM,NFIELD,NPERT,NID
     $,NXD,NYD,NZD

c automatically added by makenek
      parameter(lxo   = lx1) ! max output grid size (lxo>=lx1)

c automatically added by makenek
      parameter(lpart = 62000) ! max number of particles

c automatically added by makenek
      integer ax1,ay1,az1,ax2,ay2,az2
      parameter (ax1=1,ay1=1,az1=1,ax2=1,ay2=1,az2=1) ! running averages

c automatically added by makenek
      parameter (lxs=1,lys=lxs,lzs=(lxs-1)*(ldim-2)+1) !New Pressure Preconditioner

c automatically added by makenek
      parameter (lfdm=0)  ! == 1 for fast diagonalization method

c automatically added by makenek
      common/IOFLAG/nio  ! for logfile verbosity control

c automatically added by makenek
      integer toteq
      parameter(toteq = 1  ) ! Number of conserved variables
c IFCMT=FALSE  then toteq=1

c automatically added by makenek
      integer lelcmt
      parameter(lelcmt = 1  ) ! # of cmt elements
c IFCMT=FALSE  then lelcmt=1
\end{lstlisting}

\item {\tt .rea} file specifies the equations that you'd like to run, the relevant physical parameters, the solution algorithm, the geometry, boundary conditions, the time step size, and whether or not to run with adaptive time stepping. In order to illustrate the parameters defined in this file, an example {\tt .rea} file is shown below with detailed comments. 

Not shown in this {\tt .rea} file are curvature specifications, which can be used to specify circle and sphere edges that are described with quadratic edge displacement. If no curved sections are present, this section remains empty. Curved elements allow you to have as high an order for the surface description as you use for the numerical method. 

This file can also define boundary conditions, which are specified for each face of each element, for each field. 

The parameters are stored in an array {\tt param}. 

\begin{lstlisting}[language=Fortran]
! The parameters section controls runtime parameters such as material properties, number of time steps, the order of the time integration, frequency of output, iteration tolerances, flow rate, etc. This section also contains several free parameters that the user can use as handles to be passed into the user defined routines in the user file. 
 ****** PARAMETERS *****
 
! version of Nek5000 that is being used
   2.610000      NEKTON VERSION
 
! number of spatial dimensions (should match that in SIZE file)
   3 DIMENSIONAL RUN
 
! the number of parameters that are to be defined in the rest of this file. In order to read these correctly, you cannot have any blank lines, since this determines how many lines to read. For commenting purposes, this is ignored here.
   118  PARAMETERS FOLLOW
   
! density for the case of constant properties
   1.00000     p001 DENSITY

! kinematic viscosity; if less than zero, then this parameter is defined to represent the Reynolds number, and the kinematic viscosity is set to -1 divided by the Reynolds number (for the nondimensional form of the momentum equation).
   1.00000     p002 VISCOS

! if > 0, then natural convection is turned on with the Boussinesq approximation (not in use?)
   0.00000     p003 : : : BETAG
   
! model parameter for Boussinesq approximation (not in use?)
   0.00000     p004 : : : GTHETA
   
! not in use
   0.00000     p005 : : : PGRADX
   0.00000     p006
   1.00000     p007 RHOCP
   
! conductivity for the case of constant properties; if < 0, it defines the Peclet number, and the conductivity is set to -1 divided by the provided value (for the nondimensional form of the energy equation).
   1.00000     p008 CONDUCT
   
! not in use
   0.00000     p009
   
! simulation end time if > 0, otherwise use the number of steps NSTEPS
   0.00000     p010 FINTIME

! number of time steps
 10000.000     p011 NSTEPS
 
! upper bound on the time step size. If < 0, then the time step is some multiple of the number of time steps.
  -0.00100     p012 DT
  
! frequency of iteration histories
   0.00000     p013 IOCOMM
   
! time interval to dump the fld file if > 0. Otherwise, use IOSTEP. 
   0.00000     p014 IOTIME

! dump frequency, number of time steps between dumps
   100.0       p015 IOSTEP
   
! heat/passive scalar solver (1 = Helmholtz, 2 = CVODE, 3 = CVODE with user-supplied Jacobian. A negative number sets source terms to zero), and 0 is default.
   0.00000     p016 PSSOLVER

! not in use
   0.00000     p017
   0.00000     p018 GRID < 0 --> # cells on screen
   0.00000     p019 INTYPE
   0.00000     p020 NORDER

! tolerance for pressure solver
   1.00000E-06 p021 DIVERGENCE
   
! tolerance for velocity solver
   1.00000E-08 p022 HELMHOLTZ
   
! number of passive scalars
   0.00000     p023 NPSCAL
   
! relative tolerance for the passive scalar solver
   0.00000E-08 p024 TOLREL
   
! absolute tolerance for the passive scalar solver
   0.00000E-08 p025 TOLABS
   
! maximum Courant number
   2.00000     p026 COURANT/NTAU
   
! temporal discretization order
   2.00000     p027 TORDER

! order of temporal integration for mesh velocity
   0.00000     p028 TORDER: mesh velocity (0: p28=p27)
   
! MHD viscosity
   0.00000     p029 = magnetic visc if > 0, = -1/Rm if < 0
   
! if 0, then use constant properties defined in this file, if 1 then define each scalar separately, and if 2 then set all scalars at once in the USERVP subroutine. If using the uservp() routine, then the ifuservp boolean variable is set to true.
   0.00000     p030

! if not equal to zero, then this is the number of perturbation modes in linearized N-S
   0.00000     p031

! number of BCs in re2 file if > 0. If 0, then all are defined in the re2 file
   0.00000     p032
   
! not in use
   100.000     p033 : : : MXGEOM
   100.000     p034 : : : MAXNEW
   0.10000E-03 p035 : : :
   0.10000E-03 p036 : : : XMAGNET
   0.10000E-01 p037 : : : NGRIDS
   0.00000     p038 : : : NORDER2
   0.00000     p039 : : : NORDER3
   0.00000     p040
   0.00000     p041 1-->multiplicative SEMG

! linear solver for the pressure equation; 0 for GMRES, 1 for PCG
   0.00000     p042
   
! additive multilevel scheme
   0.00000     p043 0=semg/1=schwarz
   0.00000     p044 0=E-based/1=A-based prec.
   0.00000     p045 Relaxation factor for DTFS
   0.00000     p046 reserved
   0.00000     p047 vnu: mesh matieral prop.
   0.00000     p048 : : : PGEOM
   0.00000     p049 : : : MIXLFAC
   0.00000     p050 : : : MAXCGFS
   0.00000     p051
   0.00000     p052 IOHIS
   0.00000     p053
   0.00000     p054 fixed flow rate dir: |p54|=1,2,3=x,y,z
   0.00000     p055 vol.flow rate (p54>0) or Ubar (p54<0)
   0.00000     p056 : : :
   0.00000     p057 : : :
   0.00000     p058

! force Nek to recognize element deformation
   0.00000     p059 !=0 --> full Jac. eval. for each el.
   
   0.00000     p060 !=0 --> init. velocity to small nonzero
   0.00000     p061
   0.00000     p062 >0 --> force byte_swap for output
   0.00000     p063 =8 --> force 8-byte output
   0.00000     p064 =1 --> perturbation restart

! number of I/O files
   1.00000     p065 #iofiles (eg, 0 or 64); <0 --> sep. dirs
   4.00000     p066 output : <0=ascii, else binary
   4.00000     p067 restart: <0=ascii, else binary
   0.00000     p068 iastep: freq for avg_all (0=iostep)

! not in use
   0.00000     p069 : : : frequency of srf dump
   0.00000     p070 : : :
   0.00000     p071 : : :
   0.00000     p072 : : :
   0.00000     p073

! if > 0, print Helmholtz solver iterations
   0.00000     p074

! not in use
   0.00000     p075 : : :
   0.00000     p076 : : : p77 = Oscillation frequency (omega
   0.00000     p077 : : :
   0.00000     p078 : : :
   0.00000     p079 : : :
   0.00000     p080 : : :
   0.00000     p081 : : :
   0.00000     p082 : : :
   0.00000     p083

! if < 0, force initial time step to this value
   0.00000     p084 !=0 --> sets initial timestep if p12>0
   0.00000     p085 dt ratio if p84 !=0, for timesteps>0
   0.00000     p086 reserved
   0.00000     p087 : : :
   0.00000     p088 : : :
   0.00000     p089 : : :   coarse grid weighting (default=1
   0.00000     p090 : : :
   0.00000     p091 : : :
   0.00000     p092
  20.00000     p093 Number of previous pressure solns saved
   5.00000     p094 start projecting velocity after p94 step
   5.00000     p095 start projecting pressure after p95 step
   0.00000     p096 : : :   which saving algorithm 1 = disca
   0.00000     p097 : : :   0 == > no iterative refinement
   0.00000     p098
   3.00000     p099 dealiasing: <0--> off/3--> old/4--> new
   0.00000     p100
   0.00000     p101 Number of additional modes to filter
   1.00000     p102 Dump out divergence at each time step
   0.00000     p103 weight of stabilizing filter (.01)
   0.00000     p104 : : :
   0.00000     p105 : : :
   0.00000     p106
   0.00000     p107 !=0 --> add to h2 array in hlmhotz eqn
   0.00000     p108 : : :
   0.00000     p109 : : :
   0.00000     p110 : : :
   0.00000     p111 : : :
   0.00000     p112 : : :
   0.00000     p113 : : :
   0.00000     p114 : : :
   0.00000     p115
   0.00000     p116 !=0: x elements for fast tensor product
   0.00000     p117 !=0: y elements for fast tensor product
   0.00000     p118 !=0: z elements for fast tensor product

! Passive scalar data, which can also be specified in the uservp() routine in the user file, can also be specified in this file. These lines list the values of conductivity in ascending order, followed by the heat capacity coefficients. A total of 9 passive scalars can be transported.
      4  Lines of passive scalar data follows2 CONDUCT; 2RHOCP
   1.00000       1.00000       1.00000       1.00000       1.00000
   1.00000       1.00000       1.00000       1.00000
   1.00000       1.00000       1.00000       1.00000       1.00000
   1.00000       1.00000       1.00000       1.00000

! These logical switches determine whether you are computing a steady or unsteady solution and whether advection is turned on.
          13   LOGICAL SWITCHES FOLLOW
 T      IFFLOW
 T      IFHEAT
 T      IFTRAN
 T T F F F F F F F F F  IFNAV & IFADVC (convection in P.S. fields)
 F F T T T T T T T T T T  IFTMSH (IF mesh for this field is T mesh)
 F      IFAXIS
 F      IFSTRS
 F      IFSPLIT
 F      IFMGRID
 F      IFMODEL
 F      IFKEPS
 F      IFMVBD
 F      IFCHAR
 
! The origin is set to (-0.95, -0.45). This line has no impact on the mesh being read in Nek.
   2.00000       2.00000     -0.950000     -0.450000     XFAC,YFAC,XZERO,YZERO

! This header is used to specify how many elements are available in total, the dimension of the problem, and how many elements are in the flow mesh (the fluid region?). 
 **MESH DATA** 6 lines are X,Y,Z;X,Y,Z. Columns corners 1-4;5-8
        -480  3           480     NELT,NDIM,NELV
        
! specify a restart file to read data from to provide initial conditions (used commonly for these nonlinear equations). This is used to reduce error, since the rod12.run1 file likely was run with a much lower order. For a higher-order timestepping scheme, you'll need an equivalently larger number of restart files to provide the data points for method of lines time integration. This initial condition does not need to be of the same order as the current simulation. You can also specify the velocity and temperature from different files. The initial time is taken from the last specified restart file, but this can be overridden.
           1  PRESOLVE/RESTART OPTIONS  *****
rod12.run1 U

! Initial conditions can be zero (default), from a function in the user file, based on a ``pre-solve'', where a steady-state solution such as the Stokes equation is used as the initial condition for the transient solve, or from a restart file.
           7          INITIAL CONDITIONS *****
C Default
C Default
C Default
C Default
C Default
C Default
C Default
  ***** DRIVE FORCE DATA ***** BODY FORCE, FLOW, Q
           4                  Lines of Drive force data follow
C
C
C
C
  ***** Variable Property Data ***** Overrrides Parameter data.
           1  Lines follow.

! This section defined history points. The number of points indicated is the number of lines that should follow. This can be used to write velocity components and pressure to the .sch file at each output step (when mod(istep, iohis) = 0).
           0  PACKETS OF DATA FOLLOW
  ***** HISTORY AND INTEGRAL DATA *****
           0    POINTS.  Hcode, I,J,H,IEL

! specify output fields. You must set the coordinates to true if you want any mesh modifications made in the user file to be recognizable to postx. 
  ***** OUTPUT FIELD SPECIFICATION *****
           6  SPECIFICATIONS FOLLOW
 T       COORDINATES
 T       VELOCITY
 T       PRESSURE
 T       TEMPERATURE
 F       TEMPERATURE GRADIENT
           0       PASSIVE SCALARS
  ***** OBJECT SPECIFICATION *****
       0 Surface Objects
       0 Volume  Objects
       0 Edge    Objects
       0 Point   Objects
\end{lstlisting}

\item {\tt .re2} file, which is a binary version of the {\tt .re2} file that contains all input file components that would normally go in the {\tt .rea} file, but are too large, and hence are stored in binary format. Hence, all sections of the {\tt .rea} file that scale with the number of elements are moved to the {\tt .re2} binary file. To indicate that a section is held in the binary file, numbers are negative in the read file.

\item {\tt .usr} file, which contains a set of Fortran subroutines that allow direct access to all runtime variables. Here, the user can specify spatially varying properties, volumetric heating sources, body forces, boundary and initial conditions, etc. The following routines are used to perform the following actions:
	\begin{itemize}
	\item {\tt useric()}: set an initial condition
	\item {\tt userbc()}: set a boundary condition for both the fluid and all other scalars. 
	\item {\tt userchk()}: interrogate the solution at the end of each time step for diagnostic purposes. This is called once per processor after each time step. This is where the solution can be interrogated, and subsequent changes made.
	\item {\tt uservp()}: specify variable properties. This is called once per processor, and once for every discrete point.
	\item {\tt usrdat()}: this routine is called right after the geometry is loaded into Nek5000, and allows you to modify the elements as long as you preserve the topology. You can also use this routine to override the boundary conditions that are initially specified in the {\tt .rea} file. 
	\item {\tt userdat2()}: this routine is called afer the GLL points have been distributed, and only allows for affine transformations of the geometry. This can be used to move the geometry to match intended surfaces, such as with curved surfaces. Then, the {\tt fixgeom()} routine can be called to realign point distribution in the interior of the element.
	\item {\tt userf()}: Set the form term in the momentum equation (which in most cases will simply be the gravitational vector). 
	\item {\tt userq()}: Set the volumetric heat source in the energy equation or in the passive scalar equations
	\end{itemize}
\end{itemize}

Each Nek simulation also requires the following auxiliary files, which are {\it created} from the above fundamental files.

\begin{itemize}
\item {\tt .map} file, which is created from the {\tt .rea} file by running {\tt genmap}, which will determine how the elements are split amongst parallel processes. 
\end{itemize}

\clearpage
\section{Nested Parallelism}
When using the MultiApp system in MOOSE, flexible capabilities exist for nesting parallelism. For instance, if MOOSE is run with 4 MPI processes, then passing in a communicator to OpenMC will run OpenMC with 4 MPI processes as well. 

For illustration of how to achieve various nested parallelism structures, consider the example of running the Okapi executable using 10 MPI processes. By default, all 10 of these processes will be passed into OpenMC, and OpenMC will be run with the same number of processes. 

\begin{enumerate}
\item Case 1: You wanted to limit the number of parallel processes devoted to a SubApp. You can set the {\tt max\_procs\_per\_app} parameter in the MultiApp, which will limit the number of MPI processes that get passed to OpenMC. For a Monte Carlo code that will likely always require billions of particles, this is not very useful, but for SubApps that consist of very simple calculations, you don't want to assign too many MPI processes to perform a simple calculation, or else your performance will likely degrade.
\end{enumerate} 

In order to determine what changes need to be made in MOOSE in order to pass in a different communicator to OpenMC than is used in Nek, the following is a summary of the execution process taken in a Moose App. 

\begin{enumerate}
\item Initialize MPI, solvers, and MOOSE. {\tt MooseInit} is a class that inherits from the {\tt LibMeshInit} class. The default communicator passed into the constructor of {\tt MooseInit} is {\tt MPI\_COMM\_WORLD}. In the body of the constructor, the number of OpenMP threads used in MOOSE (which is only allowed if the {\tt LIBMESH\_HAVE\_OPENMP} environment variable is set) is set to the same number that is used in LibMesh. The {\tt LibMeshInit} class initializes any dependent libraries (MPI and PETSC) and parses the command line. The destructor closes those libraries appropriately. The constructor for the {\tt LibMeshInit} class sets the default {\tt COMM\_WORLD\_IN} to {\tt MPI\_COMM\_WORLD}. The {\tt LibMeshInit} function by default sets the number of threads to 1 to avoid MPI-multithreading competition. If the program calling {\tt LibMeshInit} has not already initialized MPI, then LibMesh initializes MPI by calling {\tt MPI\_Init\_thread}, where the level of requested thread support is set to {\tt MPI\_THREAD\_FUNNELED} if there is more than one thread (all MPI calls are funneled to a single thread so that each OpenMP thread does not call MPI) and to {\tt MPI\_THREAD\_SINGLE} if there is a single thread. Then, the input communicator is duplicated for internal use. {\tt MPI\_Finalize()} is called in the destructor for the {\tt LibMeshInit} class. 
\end{enumerate}

\end{document}
